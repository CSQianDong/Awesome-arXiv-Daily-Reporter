# Do LLMs Strategically Reveal, Conceal, and Infer Information? A Theoretical and Empirical Analysis in The Chameleon Game 

**Title (ZH)**: 大型语言模型（LLMs）在《变色龙博弈》中战略性地揭示、隐瞒和推断信息吗？理论与实证分析 

**Authors**: Mustafa O. Karabag, Ufuk Topcu  

**Link**: [PDF](https://arxiv.org/pdf/2501.19398)  

**Abstract**: Large language model-based (LLM-based) agents have become common in settings that include non-cooperative parties. In such settings, agents' decision-making needs to conceal information from their adversaries, reveal information to their cooperators, and infer information to identify the other agents' characteristics. To investigate whether LLMs have these information control and decision-making capabilities, we make LLM agents play the language-based hidden-identity game, The Chameleon. In the game, a group of non-chameleon agents who do not know each other aim to identify the chameleon agent without revealing a secret. The game requires the aforementioned information control capabilities both as a chameleon and a non-chameleon. The empirical results show that while non-chameleon LLM agents identify the chameleon, they fail to conceal the secret from the chameleon, and their winning probability is far from the levels of even trivial strategies. To formally explain this behavior, we give a theoretical analysis for a spectrum of strategies, from concealing to revealing, and provide bounds on the non-chameleons' winning probability. Based on the empirical results and theoretical analysis of different strategies, we deduce that LLM-based non-chameleon agents reveal excessive information to agents of unknown identities. Our results point to a weakness of contemporary LLMs, including GPT-4, GPT-4o, Gemini 1.5, and Claude 3.5 Sonnet, in strategic interactions. 

**Abstract (ZH)**: 基于大型语言模型（LLM）的代理在包含非合作方的环境中已变得常见。在这种环境中，代理的决策需要从其对手隐藏信息，向其合作者揭示信息，并推断信息以识别其他代理的特征。为了调查LLM是否具备这些信息控制和决策能力，我们让LLM代理参与基于语言的隐藏身份游戏《变色龙》（The Chameleon）。在这个游戏中，一群未知彼此身份的非变色龙代理试图识别变色龙代理，同时不泄露秘密。该游戏要求变色龙及其非变色龙代理都具备上述信息控制能力。实验结果显示，非变色龙LLM代理能够识别出变色龙代理，但无法从变色龙代理那里隐藏秘密，它们的获胜概率远低于甚至简单策略的水平。为了正式解释这种行为，我们为从隐藏到揭示不同策略谱系进行了理论分析，并提供了非变色龙代理的获胜概率边界。基于实验结果和不同策略的理论分析，我们推断出基于LLM的非变色龙代理向未知身份的代理泄露了过多的信息。我们的研究结果指出了当今包括GPT-4、GPT-4o、Gemini 1.5和Claude 3.5 Sonnet在内的LLM在战略互动中存在的一项弱点。 

---
# MINDSTORES: Memory-Informed Neural Decision Synthesis for Task-Oriented Reinforcement in Embodied Systems 

**Title (ZH)**: MINDSTORES：面向任务的记忆导向神经决策合成在实体系统中的强化学模型 

**Authors**: Anirudh Chari, Suraj Reddy, Aditya Tiwari, Richard Lian, Brian Zhou  

**Link**: [PDF](https://arxiv.org/pdf/2501.19318)  

**Abstract**: While large language models (LLMs) have shown promising capabilities as zero-shot planners for embodied agents, their inability to learn from experience and build persistent mental models limits their robustness in complex open-world environments like Minecraft. We introduce MINDSTORES, an experience-augmented planning framework that enables embodied agents to build and leverage mental models through natural interaction with their environment. Drawing inspiration from how humans construct and refine cognitive mental models, our approach extends existing zero-shot LLM planning by maintaining a database of past experiences that informs future planning iterations. The key innovation is representing accumulated experiences as natural language embeddings of (state, task, plan, outcome) tuples, which can then be efficiently retrieved and reasoned over by an LLM planner to generate insights and guide plan refinement for novel states and tasks. Through extensive experiments in the MineDojo environment, a simulation environment for agents in Minecraft that provides low-level controls for Minecraft, we find that MINDSTORES learns and applies its knowledge significantly better than existing memory-based LLM planners while maintaining the flexibility and generalization benefits of zero-shot approaches, representing an important step toward more capable embodied AI systems that can learn continuously through natural experience. 

**Abstract (ZH)**: 虽然大型语言模型（LLMs）在作为搭载代理的零样本规划者方面展现出了令人鼓舞的能力，但由于它们无法从经验中学习并构建持久的心理模型，这限制了它们在像Minecraft这样复杂的开放世界环境中的鲁棒性。我们引入了MINDSTORES，这是一种增强经验的规划框架，它使搭载代理能够通过与其环境的自然互动来构建和利用心理模型。该方法从人类如何构建和改进认知心理模型中汲取灵感，通过维护一个包含过去经验的数据库来扩展现有的零样本LLM规划，从而指导未来的规划迭代。关键创新是将积累的经验表示为（状态、任务、计划、结果）元组的自然语言嵌入，这些嵌入可以被LLM规划器高效地检索和推理，从而生成关于新型状态和任务的见解并指导计划改进。通过在MineDojo环境中进行广泛的实验，一个为Minecraft中的代理提供底层控制的模拟环境，我们发现，MINDSTORES在学习和应用其知识方面明显优于现有的基于记忆的LLM规划器，同时维护了零样本方法的灵活性和泛化优势，标志着向能够通过自然经验持续学习的更强大搭载式智能系统迈进的重要一步。 

---
# Ontological analysis of proactive life event services 

**Title (ZH)**: proactive 生活事件服务的本体分析 

**Authors**: Kuldar Taveter  

**Link**: [PDF](https://arxiv.org/pdf/2501.19308)  

**Abstract**: Life event service is a direct digital public service provided jointly by several governmental institutions so that a person can fulfill all the obligations and use all the rights that arise due to a particular event or situation in personal life. Life event service consolidates several public services related to the same life event into one service for the service consumer. This paper presents an ontological analysis of life event services, which is based on the works by Guarino, Guizzardi, Nardi, Wagner, and others. The purpose of the ontological analysis is to understand the meanings of life event, proactive public service based on life event, and other related notions. This kind of ontological analysis is crucial because for implementing the hardware and software architectures of e-government and digital public services, it is essential to agree upon the precise meanings of the underlying terms. 

**Abstract (ZH)**: 生活事件服务是由多个政府部门联合提供的直接数字公共服务，使个人能够履行因特定生活事件或情况而产生的所有义务并行使相关的权利。生活事件服务将与同一生活事件相关的多种公共服务整合为一项服务，供服务消费者使用。本文对生活事件服务进行了本体论分析，这些分析基于Guarino、Guizzardi、Nardi、Wagner等人的工作。本体论分析的目的是理解生活事件、基于生活事件的主动公共服务以及其他相关概念的含义。这种本体论分析是至关重要的，因为为了实现电子政务和数字公共服务的硬件和软件架构，必须对底层术语的精确含义达成一致。 

---
# SETS: Leveraging Self-Verification and Self-Correction for Improved Test-Time Scaling 

**Title (ZH)**: SETS：利用自我验证和自我校正提高测试时扩展性 

**Authors**: Jiefeng Chen, Jie Ren, Xinyun Chen, Chengrun Yang, Ruoxi Sun, Sercan Ö Arık  

**Link**: [PDF](https://arxiv.org/pdf/2501.19306)  

**Abstract**: Recent advancements in Large Language Models (LLMs) have created new opportunities to enhance performance on complex reasoning tasks by leveraging test-time computation. However, conventional approaches such as repeated sampling with majority voting or reward model scoring, often face diminishing returns as test-time compute scales, in addition to requiring costly task-specific reward model training. In this paper, we present Self-Enhanced Test-Time Scaling (SETS), a novel method that leverages the self-verification and self-correction capabilities of recent advanced LLMs to overcome these limitations. SETS integrates sampling, self-verification, and self-correction into a unified framework, enabling efficient and scalable test-time computation for improved capabilities at complex tasks. Through extensive experiments on challenging planning and reasoning benchmarks, compared to the alternatives, we demonstrate that SETS achieves significant performance improvements and more favorable test-time scaling laws. 

**Abstract (ZH)**: 近年来，大型语言模型（LLMs）的最新进展为通过利用测试时计算来增强复杂推理任务的性能提供了新的机会。然而，传统的做法，如重复抽样和多数投票或奖励模型评分，在测试时计算资源增加时往往会面临递减的回报，并且还需要进行代价高昂的任务特定奖励模型训练。在本文中，我们提出了一种新的方法Self-Enhanced Test-Time Scaling (SETS)，该方法利用了最近先进LLMs的自我验证和自我纠正能力，以克服这些限制。SETS 将采样、自我验证和自我纠正整合到一个统一框架中，从而实现更高效的测试时计算，以提高复杂任务的能力。通过在具有挑战性的规划和推理基准测试中的广泛实验，与替代方案相比，我们展示了SETS在性能提升和更优的测试时扩展规律方面的显著效果。 

---
# Synthetic User Behavior Sequence Generation with Large Language Models for Smart Homes 

**Title (ZH)**: 使用大型语言模型生成智能家庭中合成用户行为序列 

**Authors**: Zhiyao Xu, Dan Zhao, Qingsong Zou, Jingyu Xiao, Yong Jiang, Zhenhui Yuan, Qing Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.19298)  

**Abstract**: In recent years, as smart home systems have become more widespread, security concerns within these environments have become a growing threat. Currently, most smart home security solutions, such as anomaly detection and behavior prediction models, are trained using fixed datasets that are precollected. However, the process of dataset collection is time-consuming and lacks the flexibility needed to adapt to the constantly evolving smart home environment. Additionally, the collection of personal data raises significant privacy concerns for users. Lately, large language models (LLMs) have emerged as a powerful tool for a wide range of tasks across diverse application domains, thanks to their strong capabilities in natural language processing, reasoning, and problem-solving. In this paper, we propose an LLM-based synthetic dataset generation IoTGen framework to enhance the generalization of downstream smart home intelligent models. By generating new synthetic datasets that reflect changes in the environment, smart home intelligent models can be retrained to overcome the limitations of fixed and outdated data, allowing them to better align with the dynamic nature of real-world home environments. Specifically, we first propose a Structure Pattern Perception Compression (SPPC) method tailored for IoT behavior data, which preserves the most informative content in the data while significantly reducing token consumption. Then, we propose a systematic approach to create prompts and implement data generation to automatically generate IoT synthetic data with normative and reasonable properties, assisting task models in adaptive training to improve generalization and real-world performance. 

**Abstract (ZH)**: 近年来，随着智能家居系统的普及，这些环境中存在的安全问题已成为日益突出的威胁。目前，大多数智能家居安全解决方案，如异常检测和行为预测模型，都是通过预先收集的数据集进行训练的。然而，数据集收集的过程耗时且缺乏适应不断演变的智能家居环境所需的灵活性。此外，收集个人数据引发了用户的重要隐私担忧。最近，大型语言模型（LLMs）因其在自然语言处理、推理和问题解决方面强大的能力，在多种应用领域中被证明是强有力的工具。在这篇论文中，我们提出了一种基于大型语言模型的合成数据集生成框架IoTGen，以增强下游智能家居智能模型的一般性。通过生成反映环境变化的新合成数据集，智能家居智能模型可以重新训练以克服固定和过时数据的局限性，使其更好地适应智能家居环境的动态特性。具体来说，我们首先提出了一种针对IoT行为数据的结构模式感知压缩（SPPC）方法，该方法在显著减少标记消耗的同时，保留了数据中最具有信息性的内容。然后，我们提出了一种系统的方法来创造提示并实施数据生成，以自动生成具有规范性和合理性的IoT合成数据，帮助任务模型适应性训练，从而提高泛化能力和实际性能。 

---
# Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks 

**Title (ZH)**: 基于概念的可解释人工智能：度量标准与基准 

**Authors**: Halil Ibrahim Aysel, Xiaohao Cai, Adam Prugel-Bennett  

**Link**: [PDF](https://arxiv.org/pdf/2501.19271)  

**Abstract**: Concept-based explanation methods, such as concept bottleneck models (CBMs), aim to improve the interpretability of machine learning models by linking their decisions to human-understandable concepts, under the critical assumption that such concepts can be accurately attributed to the network's feature space. However, this foundational assumption has not been rigorously validated, mainly because the field lacks standardised metrics and benchmarks to assess the existence and spatial alignment of such concepts. To address this, we propose three metrics: the concept global importance metric, the concept existence metric, and the concept location metric, including a technique for visualising concept activations, i.e., concept activation mapping. We benchmark post-hoc CBMs to illustrate their capabilities and challenges. Through qualitative and quantitative experiments, we demonstrate that, in many cases, even the most important concepts determined by post-hoc CBMs are not present in input images; moreover, when they are present, their saliency maps fail to align with the expected regions by either activating across an entire object or misidentifying relevant concept-specific regions. We analyse the root causes of these limitations, such as the natural correlation of concepts. Our findings underscore the need for more careful application of concept-based explanation techniques especially in settings where spatial interpretability is critical. 

**Abstract (ZH)**: 基于概念的解释方法，如概念瓶颈模型（CBMs），旨在通过将机器学习模型的决策与人类可理解的概念联系起来，提高模型的可解释性。这一方法的关键假设是，这些概念能够被准确地归因到网络的特征空间中。然而，这一基础假设尚未得到严格的验证，主要原因在于该领域缺乏标准化的度量标准和基准来评估此类概念的存在及其空间对齐情况。为解决这一问题，我们提出了三种度量方法：概念全局重要性度量、概念存在度量和概念位置度量，包括一种用于可视化概念激活的方法，即概念激活映射。我们使用后验CBMs进行基准测试，以展示其能力和挑战。通过定性和定量实验，我们表明，在许多情况下，由后验CBMs确定的最重要概念并不存在于输入图像中；即使这些概念存在，它们的显著性图也无法与预期的区域对齐，可能是因为激活整个对象或错误地识别了相关概念的特定区域。我们分析了这些限制的根本原因，如概念的自然相关性。我们的研究结果强调了在空间可解释性至关重要的情况下，更谨慎地应用基于概念的解释技术的必要性。 

---
# Jackpot! Alignment as a Maximal Lottery 

**Title (ZH)**: 中奖了！对齐作为最大彩票 

**Authors**: Roberto-Rafael Maura-Rivero, Marc Lanctot, Francesco Visin, Kate Larson  

**Link**: [PDF](https://arxiv.org/pdf/2501.19266)  

**Abstract**: Reinforcement Learning from Human Feedback (RLHF), the standard for aligning Large Language Models (LLMs) with human values, is known to fail to satisfy properties that are intuitively desirable, such as respecting the preferences of the majority \cite{ge2024axioms}. To overcome these issues, we propose the use of a probabilistic Social Choice rule called \emph{maximal lotteries} as a replacement for RLHF. We show that a family of alignment techniques, namely Nash Learning from Human Feedback (NLHF) \cite{munos2023nash} and variants, approximate maximal lottery outcomes and thus inherit its beneficial properties.
We confirm experimentally that our proposed methodology handles situations that arise when working with preferences more robustly than standard RLHF, including supporting the preferences of the majority, providing principled ways of handling non-transitivities in the preference data, and robustness to irrelevant alternatives. This results in systems that better incorporate human values and respect human intentions. 

**Abstract (ZH)**: 强化学习结合人类反馈（RLHF）是使大型语言模型（LLMs）与人类价值观相匹配的标准方法，但已知在满足一些直观上期望的性质方面存在局限性，例如尊重大多数人的偏好[Ge等人, 2024]。为克服这些局限性，我们提议使用一种称为“最大彩票”（Maximal Lotteries）的概率社会选择规则，作为RLHF的替代方案。我们展示了多种对齐技术——即Nash 从人类反馈中学习（NLHF）及其变体——可以近似最大彩票结果，并因此继承其有益的性质。

通过实验验证，我们确认所提出的方法在处理偏好问题时比标准的RLHF更具鲁棒性，包括支持大多数人的偏好、提供处理偏好数据中非传递性的原则性方法、以及对无关选项的鲁棒性。这些结果导致了更为符合人类价值观并尊重人类意图的系统。 

---
# Objective Metrics for Human-Subjects Evaluation in Explainable Reinforcement Learning 

**Title (ZH)**: 可解释强化学习中人类受试者评价的客观指标 

**Authors**: Balint Gyevnar, Mark Towers  

**Link**: [PDF](https://arxiv.org/pdf/2501.19256)  

**Abstract**: Explanation is a fundamentally human process. Understanding the goal and audience of the explanation is vital, yet existing work on explainable reinforcement learning (XRL) routinely does not consult humans in their evaluations. Even when they do, they routinely resort to subjective metrics, such as confidence or understanding, that can only inform researchers of users' opinions, not their practical effectiveness for a given problem. This paper calls on researchers to use objective human metrics for explanation evaluations based on observable and actionable behaviour to build more reproducible, comparable, and epistemically grounded research. To this end, we curate, describe, and compare several objective evaluation methodologies for applying explanations to debugging agent behaviour and supporting human-agent teaming, illustrating our proposed methods using a novel grid-based environment. We discuss how subjective and objective metrics complement each other to provide holistic validation and how future work needs to utilise standardised benchmarks for testing to enable greater comparisons between research. 

**Abstract (ZH)**: 解释是人类的基本过程。理解解释的目标和受众至关重要，然而现有可解释强化学习（XRL）的工作通常在评估中未咨询人类的意见。即便有时会咨询人类，他们也倾向于使用主观指标，如信心或理解，这些指标仅能告知研究者的用户观点，而不能表明这些解释在特定问题上的实际有效性。本文呼吁研究者使用基于可观察和可操作行为的客观人类指标来进行解释评估，以构建更具可重复性、可比性和主义根基的研究。为此，我们收集、描述并比较了几种客观评估方法，这些方法适用于调试智能体行为和辅助人类-智能体团队工作，并通过一种新颖的网格环境来阐述我们提出的方法。我们讨论了主观和客观指标如何相互补充以提供整体验证，并讨论了未来工作如何利用标准化基准测试以进行更广泛的研究比较。 

---
# SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI Interaction Experiments 

**Title (ZH)**: SHARPIE：强化学习与人机交互实验的模块化框架 

**Authors**: Hüseyin Aydın, Kevin Dubois-Godin, Libio Goncalvez Braz, Floris den Hengst, Kim Baraka, Mustafa Mert Çelikok, Andreas Sauter, Shihan Wang, Frans A. Oliehoek  

**Link**: [PDF](https://arxiv.org/pdf/2501.19245)  

**Abstract**: Reinforcement learning (RL) offers a general approach for modeling and training AI agents, including human-AI interaction scenarios. In this paper, we propose SHARPIE (Shared Human-AI Reinforcement Learning Platform for Interactive Experiments) to address the need for a generic framework to support experiments with RL agents and humans. Its modular design consists of a versatile wrapper for RL environments and algorithm libraries, a participant-facing web interface, logging utilities, deployment on popular cloud and participant recruitment platforms. It empowers researchers to study a wide variety of research questions related to the interaction between humans and RL agents, including those related to interactive reward specification and learning, learning from human feedback, action delegation, preference elicitation, user-modeling, and human-AI teaming. The platform is based on a generic interface for human-RL interactions that aims to standardize the field of study on RL in human contexts. 

**Abstract (ZH)**: 强化学习（RL）提供了一种通用的方法来建模和训练AI代理，包括人类与AI的交互场景。本文中，我们提出了一种名为SHARPIE（共享人类与AI强化学习平台，用于交互实验）的方案，以应对在实验中支持强化学习代理和人类的一般框架需求。该平台的设计具有模块化特性，包含可灵活使用的RL环境包装器和算法库，面向参与者的网页界面，日志工具，以及在流行的云平台和参与者招募平台上部署的能力。它使研究者能够探讨人类与RL代理互动的各种研究问题，包括交互奖励指定与学习、从人类反馈中学习、动作委托、偏好获取、用户建模以及人类与AI的合作。该平台基于一种通用的人类与RL交互接口，旨在在人类情境下的强化学习领域实现标准化研究。 

---
# An Empirical Game-Theoretic Analysis of Autonomous Cyber-Defence Agents 

**Title (ZH)**: 基于实证博弈论分析的自主网络防御代理研究 

**Authors**: Gregory Palmer, Luke Swaby, Daniel J.B. Harrold, Matthew Stewart, Alex Hiles, Chris Willis, Ian Miles, Sara Farmer  

**Link**: [PDF](https://arxiv.org/pdf/2501.19206)  

**Abstract**: The recent rise in increasingly sophisticated cyber-attacks raises the need for robust and resilient autonomous cyber-defence (ACD) agents. Given the variety of cyber-attack tactics, techniques and procedures (TTPs) employed, learning approaches that can return generalisable policies are desirable. Meanwhile, the assurance of ACD agents remains an open challenge. We address both challenges via an empirical game-theoretic analysis of deep reinforcement learning (DRL) approaches for ACD using the principled double oracle (DO) algorithm. This algorithm relies on adversaries iteratively learning (approximate) best responses against each others' policies; a computationally expensive endeavour for autonomous cyber operations agents. In this work we introduce and evaluate a theoretically-sound, potential-based reward shaping approach to expedite this process. In addition, given the increasing number of open-source ACD-DRL approaches, we extend the DO formulation to allow for multiple response oracles (MRO), providing a framework for a holistic evaluation of ACD approaches. 

**Abstract (ZH)**: 近年来，日益复杂的网络攻击上升趋势加剧了对强大且具有适应能力的自主网络防御（ACD）代理的需求。鉴于攻击者采用各种各样的攻击战术、技术和程序（TTPs），具备泛化能力的学习方法变得尤为重要。同时，确保ACD代理的安全性仍是一个开放性挑战。我们通过实证博弈论分析的方法，利用规范化的双或然算法（DO算法）解决这两个挑战，该方法基于对手迭代学习彼此策略的近似最佳反应；在自主网络操作代理中，这一过程存在较高的计算成本。在这项工作中，我们引入并评估了一种理论上合理的基于潜力的奖励塑形方法，以加速这一过程。此外，鉴于开源ACD-DRL方法的数量日益增多，我们扩展了DO形式化方法，引入了多反应或然算法（MRO），提供了一个全面评估ACD方法的框架。 

---
# Imitation Game for Adversarial Disillusion with Multimodal Generative Chain-of-Thought Role-Play 

**Title (ZH)**: 模仿游戏：基于多模态生成式思维链的角色扮演以消除对抗性幻觉 

**Authors**: Ching-Chun Chang, Fan-Yun Chen, Shih-Hong Gu, Kai Gao, Hanrui Wang, Isao Echizen  

**Link**: [PDF](https://arxiv.org/pdf/2501.19143)  

**Abstract**: As the cornerstone of artificial intelligence, machine perception confronts a fundamental threat posed by adversarial illusions. These adversarial attacks manifest in two primary forms: deductive illusion, where specific stimuli are crafted based on the victim model's general decision logic, and inductive illusion, where the victim model's general decision logic is shaped by specific stimuli. The former exploits the model's decision boundaries to create a stimulus that, when applied, interferes with its decision-making process. The latter reinforces a conditioned reflex in the model, embedding a backdoor during its learning phase that, when triggered by a stimulus, causes aberrant behaviours. The multifaceted nature of adversarial illusions calls for a unified defence framework, addressing vulnerabilities across various forms of attack. In this study, we propose a disillusion paradigm based on the concept of an imitation game. At the heart of the imitation game lies a multimodal generative agent, steered by chain-of-thought reasoning, which observes, internalises and reconstructs the semantic essence of a sample, liberated from the classic pursuit of reversing the sample to its original state. As a proof of concept, we conduct experimental simulations using a multimodal generative dialogue agent and evaluates the methodology under a variety of attack scenarios. 

**Abstract (ZH)**: 作为人工智能的基石，机器感知面临着由对抗幻觉带来的根本性威胁。这些对抗攻击主要表现为两种形式：演绎幻觉（deductive illusion），即根据受害者模型的一般决策逻辑精心设计特定刺激；归纳幻觉（inductive illusion），即受害者模型的一般决策逻辑被特定刺激所塑造。前者通过利用模型的决策边界来创造出一种刺激，在应用时会干扰其决策过程。后者则在模型的学习阶段强化了一种条件反射，并嵌入了一个后门，在特定刺激触发时会导致异常行为。对抗幻觉的多样性促使我们需要建立一个统一的防御框架，以应对各种攻击形式。在这项研究中，我们基于模仿游戏的概念提出了一个消幻觉（disillusion）范式。模仿游戏的核心是一个基于链式推理的多模态生成代理，该代理能够观察、内化并重建样本的语义本质，而不仅仅是试图将样本还原为其原始状态。为证明这一方法的有效性，我们使用了一个多模态生成对话代理进行了实验模拟，并在多种攻击场景下评估了该方法论。 

---
# Logical Modalities within the European AI Act: An Analysis 

**Title (ZH)**: 欧洲人工智能法案中的逻辑模态性：一项分析 

**Authors**: Lara Lawniczak, Christoph Benzmüller  

**Link**: [PDF](https://arxiv.org/pdf/2501.19112)  

**Abstract**: The paper presents a comprehensive analysis of the European AI Act in terms of its logical modalities, with the aim of preparing its formal representation, for example, within the logic-pluralistic Knowledge Engineering Framework and Methodology (LogiKEy). LogiKEy develops computational tools for normative reasoning based on formal methods, employing Higher-Order Logic (HOL) as a unifying meta-logic to integrate diverse logics through shallow semantic embeddings. This integration is facilitated by Isabelle/HOL, a proof assistant tool equipped with several automated theorem provers. The modalities within the AI Act and the logics suitable for their representation are discussed. For a selection of these logics, embeddings in HOL are created, which are then used to encode sample paragraphs. Initial experiments evaluate the suitability of these embeddings for automated reasoning, and highlight key challenges on the way to more robust reasoning capabilities. 

**Abstract (ZH)**: 本文对《欧洲人工智能法案》的逻辑模态进行了全面分析，旨在为其正式表示做准备，例如在逻辑多元知识工程框架和方法论（LogiKEy）中进行表示。LogiKEy 开发了基于形式方法的计算工具，用于规范推理，并利用高阶逻辑（HOL）作为统一的元逻辑，通过浅层语义嵌入将各种逻辑整合在一起。这种整合是通过配套的证明助手工具 Isabelle/HOL 实现的，该工具配备了多个自动定理证明器。本文讨论了《人工智能法案》中的逻辑模态以及适合其表示的逻辑类型。对于所选的这些逻辑，创建了 HOL 中的嵌入表示，并使用这些嵌入表示对样本段落进行编码。初步实验评估了这些嵌入表示用于自动化推理的适用性，并指出了通往更加稳健的推理能力过程中存在的关键挑战。 

---
# PathE: Leveraging Entity-Agnostic Paths for Parameter-Efficient Knowledge Graph Embeddings 

**Title (ZH)**: PathE：利用实体无关路径进行参数高效的知识图嵌入 

**Authors**: Ioannis Reklos, Jacopo de Berardinis, Elena Simperl, Albert Meroño-Peñuela  

**Link**: [PDF](https://arxiv.org/pdf/2501.19095)  

**Abstract**: Knowledge Graphs (KGs) store human knowledge in the form of entities (nodes) and relations, and are used extensively in various applications. KG embeddings are an effective approach to addressing tasks like knowledge discovery, link prediction, and reasoning. This is often done by allocating and learning embedding tables for all or a subset of the entities. As this scales linearly with the number of entities, learning embedding models in real-world KGs with millions of nodes can be computationally intractable. To address this scalability problem, our model, PathE, only allocates embedding tables for relations (which are typically orders of magnitude fewer than the entities) and requires less than 25% of the parameters of previous parameter efficient methods. Rather than storing entity embeddings, we learn to compute them by leveraging multiple entity-relation paths to contextualise individual entities within triples. Evaluated on four benchmarks, PathE achieves state-of-the-art performance in relation prediction, and remains competitive in link prediction on path-rich KGs while training on consumer-grade hardware. We perform ablation experiments to test our design choices and analyse the sensitivity of the model to key hyper-parameters. PathE is efficient and cost-effective for relationally diverse and well-connected KGs commonly found in real-world applications. 

**Abstract (ZH)**: 知识图谱（KGs）以实体（节点）和关系的形式存储人类知识，并在各种应用中得到广泛应用。KG嵌入是一种有效的解决知识发现、链接预测和推理等任务的方法。通常，这涉及为实体（或其子集）分配和学习嵌入表。由于这种方法随实体数量呈线性增长，在具有数百万节点的真实世界KG中学习嵌入模型可能会变成计算上不可行的任务。为了解决这个可扩展性问题，我们的模型PathE仅为关系分配嵌入表（通常关系的数量比实体少几个数量级），且所需的参数少于之前参数高效方法的25%。我们不是存储实体嵌入，而是通过利用多个实体-关系路径来上下文化三元组中的单个实体，来学习计算这些嵌入。在四个基准测试上，PathE在关系预测任务上取得了最先进的性能，并在路径丰富的知识图谱上保持了竞争力，同时在消费级硬件上进行训练。我们进行了消融实验以测试我们的设计选择，并分析了模型对关键超参数的敏感性。PathE对于真实世界应用中常见的关系多样且高度连接的知识图谱而言，是高效且成本效益高的。 

---
# Language Games as the Pathway to Artificial Superhuman Intelligence 

**Title (ZH)**: 语言游戏作为通向人工超级智能的路径 

**Authors**: Ying Wen, Ziyu Wan, Shao Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18924)  

**Abstract**: The evolution of large language models (LLMs) toward artificial superhuman intelligence (ASI) hinges on data reproduction, a cyclical process in which models generate, curate and retrain on novel data to refine capabilities. Current methods, however, risk getting stuck in a data reproduction trap: optimizing outputs within fixed human-generated distributions in a closed loop leads to stagnation, as models merely recombine existing knowledge rather than explore new frontiers. In this paper, we propose language games as a pathway to expanded data reproduction, breaking this cycle through three mechanisms: (1) \textit{role fluidity}, which enhances data diversity and coverage by enabling multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward variety}, embedding multiple feedback criteria that can drive complex intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving interaction constraints to foster learnability, thereby injecting continual novelty. By scaling language games into global sociotechnical ecosystems, human-AI co-evolution generates unbounded data streams that drive open-ended exploration. This framework redefines data reproduction not as a closed loop but as an engine for superhuman intelligence. 

**Abstract (ZH)**: 大型语言模型（LLMs）向人工超级智能（ASI）的演变依赖于数据的再生产，这是一个循环过程，模型通过生成、整理和重新训练新型数据来精炼其能力。然而，当前的方法面临数据再生产陷阱的风险：在固定的人类生成的数据分布中进行循环优化会导致停滞，因为模型只是重新组合现有的知识，而不是探索新的领域。在本文中，我们提出语言游戏是一种扩展数据再生产途径，通过三种机制打破这一循环：（1）**角色灵活性**，通过多智能体系统在任务之间动态切换角色来增强数据的多样性和覆盖率；（2）**奖励多样性**，嵌入多种反馈标准以驱动复杂智能行为；以及（3）**规则可塑性**，通过迭代进化交互约束来促进可学习性，从而不断注入新奇性。将语言游戏扩展到全球社会技术生态系统中，人类与人工智能的共同进化产生无限制的数据流，推动开放式的探索。这种框架重新定义了数据再生产，不再将其视为一个封闭循环，而是成为推动超人类智能的动力引擎。 

---
# Bridging the Reasoning Gap: Small LLMs Can Plan with Generalised Strategies 

**Title (ZH)**: 弥合推理差距：小型语言模型可以通过泛化策略进行规划 

**Authors**: Andrey Borro, Patricia J Riddle, Michael W Barley, Michael J Witbrock  

**Link**: [PDF](https://arxiv.org/pdf/2501.18817)  

**Abstract**: Recent advancements in the reasoning skills of Large Language Models (LLMs) demonstrate an increase in the ability of LLMs to solve simple planning tasks. However, as long as the driving force behind improved reasoning capability is the size and complexity of the model, the financial and computational costs associated with running them will also increase. This trend raises questions about continued accessibility and whether these improvements will increase at the same pace as models continue to grow in size and expense. We propose two approaches to enhance the reasoning ability of less resource-intensive LLMs. (1) Provide them with a generalised strategy for solving tasks within a given domain, generated by a more resource-intensive LLM. (2) Exploit their cost-effectiveness by iteratively prompting these models to correct errors in their proposed solutions. Our empirical results from planning and mathematical reasoning tasks demonstrate that these methods improve the performance of less resource-intensive LLMs to levels comparable with their more resource-intensive counterparts, at a fraction of the cost. Additionally, we show that the utilisation of generalised strategies in our experiments reduced the cost of the less resource-intensive model by nearly 30 percent on average. 

**Abstract (ZH)**: 大型语言模型（LLMs）推理能力的近期进展表明，LLMs在解决简单规划任务方面的能力有所提升。然而，只要增强推理能力的动力来源于模型的规模和复杂度，那么运行这些模型所涉及的财务和计算成本也将增加。这种趋势引发了关于持续可访问性以及这些改进是否会以与模型规模和成本增长相同的速度进行的疑问。我们提出了两种提高低资源密集型LLMs推理能力的方法：（1）提供一种由更资源密集型LLM生成的通用策略，用于解决给定领域的任务；（2）利用它们的经济性，通过迭代提示这些模型纠正其提议的解决方案中的错误。从规划和数学推理任务的实证结果来看，这些方法能够使低资源密集型LLMs的性能达到与更资源密集型LLMs相媲美的水平，但成本仅为后者的几分之一。此外，我们展示了在实验中利用通用策略降低了低资源密集型模型大约30%的成本。 

---
# LLM-Generated Heuristics for AI Planning: Do We Even Need Domain-Independence Anymore? 

**Title (ZH)**: 生成的LLM启发式方法在AI规划中的应用：我们还需要领域无关性吗？ 

**Authors**: Alexander Tuisov, Yonatan Vernik, Alexander Shleyfman  

**Link**: [PDF](https://arxiv.org/pdf/2501.18784)  

**Abstract**: Domain-independent heuristics have long been a cornerstone of AI planning, offering general solutions applicable across a wide range of tasks without requiring domain-specific engineering. However, the advent of large language models (LLMs) presents an opportunity to generate heuristics tailored to specific planning problems, potentially challenging the necessity of domain independence as a strict design principle. In this paper, we explore the use of LLMs to automatically derive planning heuristics from task descriptions represented as successor generators and goal tests written in general purpose programming language. We investigate the trade-offs between domain-specific LLM-generated heuristics and traditional domain-independent methods in terms of computational efficiency and explainability. Our experiments demonstrate that LLMs can create heuristics that achieve state-of-the-art performance on some standard IPC domains, as well as their ability to solve problems that lack an adequate Planning Domain Definition Language ({\sc pddl}) representation. We discuss whether these results signify a paradigm shift and how they can complement existing approaches. 

**Abstract (ZH)**: 领域无关的经验在人工智能规划中一直是一个基石，提供了一类适用于广泛任务的通用解决方案，无需针对特定领域进行工程设计。然而，大型语言模型（LLMs）的出现为生成针对特定规划问题定制的经验提供了一个机会，可能挑战领域无关性作为严格设计原则的必要性。在本文中，我们探讨了使用LLMs从以通用编程语言编写的任务描述（表示为后续生成器和目标测试）中自动推导规划经验的方法。我们研究了领域特定的LLM生成经验与传统领域无关方法之间的权衡，尤其是在计算效率和可解释性方面的权衡。实验结果表明，LLMs能够创建在一些标准IPC领域中达到最佳性能的经验，并展示了它们解决缺乏适当规划领域定义语言（PDDL）表示的问题的能力。我们讨论了这些结果是否标志着范式转变，并探讨了它们如何补充现有的方法。 

---
# Simulation Streams: A Programming Paradigm for Controlling Large Language Models and Building Complex Systems with Generative AI 

**Title (ZH)**: 仿真流：控制大规模语言模型和构建基于生成式AI复杂系统的编程范式 

**Authors**: Peter Sunehag, Joel Z. Leibo  

**Link**: [PDF](https://arxiv.org/pdf/2501.18668)  

**Abstract**: We introduce Simulation Streams, a programming paradigm designed to efficiently control and leverage Large Language Models (LLMs) for complex, dynamic simulations and agentic workflows. Our primary goal is to create a minimally interfering framework that harnesses the agentic abilities of LLMs while addressing their limitations in maintaining consistency, selectively ignoring/including information, and enforcing strict world rules. Simulation Streams achieves this through a state-based approach where variables are modified in sequential steps by "operators," producing output on a recurring format and adhering to consistent rules for state variables. This approach focus the LLMs on defined tasks, while aiming to have the context stream remain "in-distribution". The approach incorporates an Entity-Component-System (ECS) architecture to write programs in a more intuitive manner, facilitating reuse of workflows across different components and entities. This ECS approach enhances the modularity of the output stream, allowing for complex, multi-entity simulations while maintaining format consistency, information control, and rule enforcement. It is supported by a custom editor that aids in creating, running, and analyzing simulations. We demonstrate the versatility of simulation streams through an illustrative example of an ongoing market economy simulation, a social simulation of three characters playing a game of catch in a park and a suite of classical reinforcement learning benchmark tasks. These examples showcase Simulation Streams' ability to handle complex, evolving scenarios over 100s-1000s of iterations, facilitate comparisons between different agent workflows and models, and maintain consistency and continued interesting developments in LLM-driven simulations. 

**Abstract (ZH)**: 我们引入了Simulation Streams编程范式，该范式旨在高效控制和利用大型语言模型（LLMs）进行复杂的动态模拟和有代理性的工作流程。我们的主要目标是创建一个干扰最小的框架，充分利用LLMs的代理能力，同时解决它们在保持一致性、选择性忽略或包含信息以及强制执行严格的世界规则方面的限制。Simulation Streams 通过基于状态的方法实现这一目标，这种方法通过“操作符”在顺序步骤中修改变量，以一致的格式输出，并遵循状态变量的一致规则。这种方法将重点放在定义的任务上，同时让上下文流保持“在分布”状态。该方法采用实体-组件-系统（ECS）架构，使编程更加直观，有利于在不同组件和实体之间重用工作流程。这种ECS方法增强了输出流的模块性，可以在保持格式一致性、信息控制和规则执行的前提下，进行复杂、多实体的模拟。该方法还借助自定义编辑器来辅助创建、运行和分析模拟。我们通过以下示例展示了Simulation Streams的多功能性：一个持续的市场经济模拟、三个角色在公园里玩接球游戏的社会模拟，以及一系列经典的强化学习基准任务。这些示例展示了Simulation Streams在数百到数千次迭代中处理复杂、演变场景的能力，支持不同代理工作流程和模型之间的比较，并在LLM驱动的模拟中保持一致性及持续的有趣发展。 

---
# Enhancing Large Language Model Efficiencyvia Symbolic Compression: A Formal Approach Towards Interpretability 

**Title (ZH)**: 通过符号压缩提升大型语言模型效率：通向可解释性的形式化方法 

**Authors**: Lumen AI, Tengzhou No. 1 Middle School, Shihao Ji, Zihui Song, Fucheng Zhong, Jisen Jia, Zhaobo Wu, Zheyi Cao, Tianhao Xu  

**Link**: [PDF](https://arxiv.org/pdf/2501.18657)  

**Abstract**: Large language models (LLMs) face significant token efficiency bottlenecks in code generation and logical reasoning tasks, a challenge that directly impacts inference cost and model interpretability. This paper proposes a formal framework based on symbolic compression,integrating combinatory logic, information-theoretic optimal encoding, and context-aware inference techniques to achieve a step-change improvement in token efficiency while preserving semantic integrity. We establish a mathematical framework within a functional programming paradigm, derive the quantitative relationship between symbolic density and model interpretability, and propose a differentiable compression factor metric to evaluate encoding efficiency. Furthermore, we leverage parameter-efficient fine-tuning (PEFT) techniques to achieve a low-cost application of the GAEL language. Experimental results show that this method achieves a 78.3% token compression rate in code generation tasks while improving logical traceability by 62% through structural explicitness. This research provides new theoretical tools for efficient inference in LLMs and opens a symbolic path for modelinterpretability research. 

**Abstract (ZH)**: 大语言模型（LLMs）在代码生成和逻辑推理等任务中面临显著的标记效率瓶颈，这直接影响推理成本和模型可解释性。本文提出了一种基于符号压缩的形式化框架，该框架结合了组合逻辑、信息论最优编码和上下文感知推理技术，以在保持语义完整性的同时实现标记效率的大幅改进。我们在此功能式编程范式下建立了一个数学框架，推导了符号密度与模型可解释性之间的定量关系，并提出了一种可微分压缩因子度量来评估编码效率。此外，我们利用参数效率微调（PEFT）技术实现GAEL语言的低成本应用。实验结果表明，该方法在代码生成任务中的标记压缩率达到78.3%，并通过结构明确性提高了逻辑追溯性62%。这项研究为LLMs的高效推理提供了新的理论工具，并为模型可解释性研究开辟了一条符号途径。 

---
# AI Biases Towards Rich and Powerful Surnames 

**Title (ZH)**: AI 对富裕和有影响力的姓氏的偏见 

**Authors**: Pat Pataranutaporn, Nattavudh Powdthavee, Pattie Maes  

**Link**: [PDF](https://arxiv.org/pdf/2501.19407)  

**Abstract**: Surnames often convey implicit markers of social status, wealth, and lineage, shaping perceptions in ways that can perpetuate systemic biases. This study investigates whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals. Drawing on 600 surnames from the United States and Thailand, countries with differing sociohistorical dynamics and surname conventions, we categorize names into Rich, Legacy, Normal, and phonetically similar Variant groups. Our findings reveal that elite surnames consistently predict AI-generated perceptions of power, intelligence, and wealth, leading to significant consequences for decisions in high-stakes situations. Mediation analysis highlights perceived intelligence as a crucial pathway through which surname biases operate. Providing objective qualifications alongside the surnames reduces, but does not eliminate, these biases, especially in contexts with uniformly low credentials. These results call for fairness-aware algorithms and robust policy interventions to mitigate the reinforcement of inherited inequalities by AI systems. Our work also urges a reexamination of algorithmic accountability and its societal impact, particularly in systems designed for meritocratic outcomes. 

**Abstract (ZH)**: 姓氏通常会传递出隐含的社会地位、财富和血统的信息，这些信息塑造着人们的认知，并可能延续系统的偏见。本研究调查了姓氏如何影响基于人工智能的决策制定过程，重点关注其对招聘推荐、领导职位任命和贷款审批等关键领域的潜在影响。本研究基于来自美国和泰国的600个姓氏（这两个国家在社会历史文化背景和姓氏习俗方面存在差异），将这些姓氏划分为显赫、世袭、普通和音似变体四类。研究结果显示，精英姓氏始终能够预测人工智能生成的权力、智力和财富感知，这在高风险决策中具有显著影响。中介分析显示，感知到的智力是姓氏偏见发挥作用的关键路径之一。提供客观资格信息可以减少但不能消除这些偏见，特别是在所有候选人资格普遍较低的背景下。研究结果呼吁采用公平意识算法并制定有力的政策干预措施，以减轻人工智能系统强化继承性不平等的影响。本研究还呼吁重新审视算法问责制及其社会影响，尤其是在旨在实现公正结果的系统中。 

---
# Redefining Machine Unlearning: A Conformal Prediction-Motivated Approach 

**Title (ZH)**: 重新定义机器卸载：一种基于区间预测的方法 

**Authors**: Yingdan Shi, Ren Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19403)  

**Abstract**: Machine unlearning seeks to systematically remove specified data from a trained model, effectively achieving a state as though the data had never been encountered during training. While metrics such as Unlearning Accuracy (UA) and Membership Inference Attack (MIA) provide a baseline for assessing unlearning performance, they fall short of evaluating the completeness and reliability of forgetting. This is because the ground truth labels remain potential candidates within the scope of uncertainty quantification, leaving gaps in the evaluation of true forgetting. In this paper, we identify critical limitations in existing unlearning metrics and propose enhanced evaluation metrics inspired by conformal prediction. Our metrics can effectively capture the extent to which ground truth labels are excluded from the prediction set. Furthermore, we observe that many existing machine unlearning methods do not achieve satisfactory forgetting performance when evaluated with our new metrics. To address this, we propose an unlearning framework that integrates conformal prediction insights into Carlini & Wagner adversarial attack loss. Extensive experiments on the image classification task demonstrate that our enhanced metrics offer deeper insights into unlearning effectiveness, and that our unlearning framework significantly improves the forgetting quality of unlearning methods. 

**Abstract (ZH)**: 机器遗忘旨在系统地从训练好的模型中移除指定的数据，从而实现一种仿佛这些数据从未被训练过的效果。虽然未学习准确度（UA）和成员推断攻击（MIA）等度量标准可以为评估遗忘性能提供基础，但它们在评估遗忘的完整性和可靠性方面存在不足。这是因为真实标签在不确定性量化范围内仍然可能成为候选者，导致在评估真正遗忘时留下空白。在本文中，我们识别了现有遗忘度量的关键局限性，并提出了一种基于一致预测的增强评估度量。我们的度量可以有效地捕捉真实标签被排除出预测集的程度。此外，我们观察到，在使用我们的新度量评估时，许多现有的机器遗忘方法未能达到满意的遗忘性能。为了解决这一问题，我们提出了一种集成一致预测见解的对抗攻击损失的遗忘框架。在图像分类任务上的广泛实验证明，我们的增强度量提供了对遗忘效果更深入的洞察，并且我们的遗忘框架显著提高了遗忘方法的遗忘质量。 

---
# Vintix: Action Model via In-Context Reinforcement Learning 

**Title (ZH)**: 维尼亚克斯：基于上下文强化学习的行动模型 

**Authors**: Andrey Polubarov, Nikita Lyubaykin, Alexander Derevyagin, Ilya Zisman, Denis Tarasov, Alexander Nikulin, Vladislav Kurenkov  

**Link**: [PDF](https://arxiv.org/pdf/2501.19400)  

**Abstract**: In-Context Reinforcement Learning (ICRL) represents a promising paradigm for developing generalist agents that learn at inference time through trial-and-error interactions, analogous to how large language models adapt contextually, but with a focus on reward maximization. However, the scalability of ICRL beyond toy tasks and single-domain settings remains an open challenge. In this work, we present the first steps toward scaling ICRL by introducing a fixed, cross-domain model capable of learning behaviors through in-context reinforcement learning. Our results demonstrate that Algorithm Distillation, a framework designed to facilitate ICRL, offers a compelling and competitive alternative to expert distillation to construct versatile action models. These findings highlight the potential of ICRL as a scalable approach for generalist decision-making systems. Code to be released at this https URL 

**Abstract (ZH)**: 上下文中的强化学习（ICRL）代表了一种有潜力的范式，通过在推理时的试错交互来开发能够在多种场景下学习的一般性代理，类似于大型语言模型通过上下文调整的方式，但更侧重于奖励最大化。然而，ICRL 在扩展到玩具任务和单一领域设置之外依然面临着一个开放的挑战。在这项工作中，我们提出了通过引入一个固定跨领域的模型，来首次尝试扩展 ICRL 的步骤。该模型能够通过上下文中的强化学习来学习行为。我们的实验结果表明，算法蒸馏（Algorithm Distillation），一种旨在促进 ICRL 的框架，提供了与专家蒸馏相比具有吸引力且竞争性的替代方案，用于构建多功能的行为模型。这些发现强调了 ICRL 作为一种可用于拓展的一般性决策系统方法的潜力。代码将在以下链接发布：该 <https://your_link_here> URL 

---
# Scalable-Softmax Is Superior for Attention 

**Title (ZH)**: 可扩展的Softmax在注意力机制中更为优越 

**Authors**: Ken M. Nakanishi  

**Link**: [PDF](https://arxiv.org/pdf/2501.19399)  

**Abstract**: The maximum element of the vector output by the Softmax function approaches zero as the input vector size increases. Transformer-based language models rely on Softmax to compute attention scores, causing the attention distribution to flatten as the context size grows. This reduces the model's ability to prioritize key information effectively and potentially limits its length generalization. To address this problem, we propose Scalable-Softmax (SSMax), which replaces Softmax in scenarios where the input vector size varies. SSMax can be seamlessly integrated into existing Transformer-based architectures. Experimental results in language modeling show that models using SSMax not only achieve faster loss reduction during pretraining but also significantly improve performance in long contexts and key information retrieval. Furthermore, an analysis of attention scores reveals that SSMax enables the model to focus attention on key information even in long contexts. Additionally, although models that use SSMax from the beginning of pretraining achieve better length generalization, those that have already started pretraining can still gain some of this ability by replacing Softmax in the attention layers with SSMax, either during or after pretraining. 

**Abstract (ZH)**: 随着输入向量尺寸的增加，Softmax函数输出向量的最大元素趋于零。基于Transformer的语言模型依赖于Softmax来计算注意力分数，导致随着背景尺寸的增长，注意力分布变平。这降低了模型有效优先处理关键信息的能力，并可能限制其长度泛化能力。为解决这一问题，我们提出了可扩展Softmax（SSMax），它可以在输入向量尺寸变化的场景中替代Softmax。SSMax可以无缝集成到现有的基于Transformer的架构中。在语言建模实验中，使用SSMax的模型不仅在预训练期间更快地减少了损失，还在长上下文和关键信息检索方面显著提高了性能。此外，对注意力分数的分析表明，SSMax使模型能够在长上下文中聚焦于关键信息。此外，虽然从预训练一开始就使用SSMax的模型在长度泛化方面表现更好，但对于已经开始预训练的模型，通过在注意力层中用SSMax替换Softmax，这两种模型都可以在预训练期间或预训练后获得一定程度的长度泛化能力。 

---
# s1: Simple test-time scaling 

**Title (ZH)**: 当然，以下是翻译后的标题或内容，符合学术规范：

S1: 测试时简单的缩放方法 

**Authors**: Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto  

**Link**: [PDF](https://arxiv.org/pdf/2501.19393)  

**Abstract**: Test-time scaling is a promising new approach to language modeling that uses extra test-time compute to improve performance. Recently, OpenAI's o1 model showed this capability but did not publicly share its methodology, leading to many replication efforts. We seek the simplest approach to achieve test-time scaling and strong reasoning performance. First, we curate a small dataset s1K of 1,000 questions paired with reasoning traces relying on three criteria we validate through ablations: difficulty, diversity, and quality. Second, we develop budget forcing to control test-time compute by forcefully terminating the model's thinking process or lengthening it by appending "Wait" multiple times to the model's generation when it tries to end. This can lead the model to double-check its answer, often fixing incorrect reasoning steps. After supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and equipping it with budget forcing, our model s1 exceeds o1-preview on competition math questions by up to 27% (MATH and AIME24). Further, scaling s1 with budget forcing allows extrapolating beyond its performance without test-time intervention: from 50% to 57% on AIME24. Our model, data, and code are open-source at this https URL. 

**Abstract (ZH)**: 测试时扩展是一种有前景的新语言模型方法，它利用额外的测试时计算资源来提高性能。最近，OpenAI的o1模型展示了这种能力，但没有公开分享其方法论，导致了众多的复制努力。我们寻求实现测试时扩展和强大推理性能的最简单方法。首先，我们精心筛选了一个包含1000个问题及其推理轨迹的小数据集s1K，这些问题依赖于我们通过消融实验验证的三个标准：难度、多样性和质量。其次，我们开发了预算强制策略，以控制测试时计算资源，该策略通过强制终止模型的思考过程或在模型试图结束时多次添加“等待”指令来延长模型生成的内容。这可能会促使模型重新检查其答案，通常会纠正错误的推理步骤。在对Qwen2.5-32B-Instruct语言模型进行监督微调并在其上使用预算强制策略后，我们的模型s1在竞赛数学问题上的表现超越了o1-preview高达27%（MATH和AIME24）。进一步地，通过预算强制策略对s1进行扩展，可以在不进行测试时干预的情况下超出其性能表现：AIME24上的得分从50%提高到57%。我们的模型、数据和代码在此处对外开放：[该链接]。 

---
# Decoding-based Regression 

**Title (ZH)**: 基于解码的回归 

**Authors**: Xingyou Song, Dara Bahri  

**Link**: [PDF](https://arxiv.org/pdf/2501.19383)  

**Abstract**: Language models have recently been shown capable of performing regression tasks wherein numeric predictions are represented as decoded strings. In this work, we provide theoretical grounds for this capability and furthermore investigate the utility of causal auto-regressive sequence models when they are applied to any feature representation. We find that, despite being trained in the usual way - for next-token prediction via cross-entropy loss - decoding-based regression is as performant as traditional approaches for tabular regression tasks, while being flexible enough to capture arbitrary distributions, such as in the task of density estimation. 

**Abstract (ZH)**: 近年来，语言模型已被证明具备执行回归任务的能力，其中数值预测通过解码字符串进行表示。本项研究为这种能力提供了理论基础，并进一步探讨了因果自回归序列模型在应用于任何特征表示时的应用价值。我们发现，尽管这些模型是通过交叉熵损失进行传统的下一步-token 预测训练的，但基于解码的回归与传统的表格型回归方法在性能上相当，同时还具有足够的灵活性，能够捕捉到任意分布，例如密度估计任务。 

---
# CoSTI: Consistency Models for (a faster) Spatio-Temporal Imputation 

**Title (ZH)**: CoSTI：时空插值的致性模型（用于更快的插值） 

**Authors**: Javier Solís-García, Belén Vega-Márquez, Juan A. Nepomuceno, Isabel A. Nepomuceno-Chamorro  

**Link**: [PDF](https://arxiv.org/pdf/2501.19364)  

**Abstract**: Multivariate Time Series Imputation (MTSI) is crucial for many applications, such as healthcare monitoring and traffic management, where incomplete data can compromise decision-making. Existing state-of-the-art methods, like Denoising Diffusion Probabilistic Models (DDPMs), achieve high imputation accuracy; however, they suffer from significant computational costs and are notably time-consuming due to their iterative nature. In this work, we propose CoSTI, an innovative adaptation of Consistency Models (CMs) for the MTSI domain. CoSTI employs Consistency Training to achieve comparable imputation quality to DDPMs while drastically reducing inference times, making it more suitable for real-time applications. We evaluate CoSTI across multiple datasets and missing data scenarios, demonstrating up to a 98% reduction in imputation time with performance on par with diffusion-based models. This work bridges the gap between efficiency and accuracy in generative imputation tasks, providing a scalable solution for handling missing data in critical spatio-temporal systems. 

**Abstract (ZH)**: 多变量时间序列插补（MTSI）对于许多应用至关重要，如健康监护和交通管理，其中不完整数据可能会损害决策过程。现有的前沿方法，例如去噪扩散概率模型（DDPMs），能够在插补准确性方面取得高成效；然而，这些方法由于其迭代性质而遭受显著的计算成本和极度耗时的问题。在本研究中，我们提出了CoSTI，一种创新的Consistency Models（CMs）在MTSI领域的适应方法。CoSTI利用一致性训练来实现与DDPMs相当的插补质量，同时大幅减少推断时间，使其更适合实时应用。我们对多个数据集和缺失数据情况进行评估，结果显示CoSTI在插补时间上可降低高达98%，且性能与基于扩散的方法相当。本研究在生成性插补任务中弥合了效率与准确性的差距，提供了一种用于处理关键空间-时间系统中缺失数据的可扩展解决方案。 

---
# We're Different, We're the Same: Creative Homogeneity Across LLMs 

**Title (ZH)**: 我们独特，我们相同：大型语言模型中的创意思维一致性 

**Authors**: Emily Wenger, Yoed Kenett  

**Link**: [PDF](https://arxiv.org/pdf/2501.19361)  

**Abstract**: Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond. Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs. However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM -- which arguably has a limited range of outputs -- or from using LLMs in general as creative assistants. To study this question, we elicit creative responses from humans and a broad set of LLMs using standardized creativity tests and compare the population-level diversity of responses. We find that LLM responses are much more similar to other LLM responses than human responses are to each other, even after controlling for response structure and other key variables. This finding of significant homogeneity in creative outputs across the LLMs we evaluate adds a new dimension to the ongoing conversation about creativity and LLMs. If today's LLMs behave similarly, using them as a creative partners -- regardless of the model used -- may drive all users towards a limited set of "creative" outputs. 

**Abstract (ZH)**: 现在有许多强大的大型语言模型（LLMs）可用作写作辅助工具、创意生成器等。虽然这些LLMs被宣传为有帮助的创意伙伴，但已有研究表明，将LLMs用作创意伙伴会导致创意输出范围较窄。然而，这些研究只考虑了一个单独的LLM的影响，未回答这种创意范围的缩小是否源于使用特定的LLM（其输出范围有限）还是源于整体上将LLM用于创意辅助。为了研究这个问题，我们使用标准化的创造力测试从人类和广泛的不同LLM中诱发出创造力回应，并比较了群体层面的多样性和回应结构等关键变量。我们发现，尽管控制了回应结构和其他关键变量，LLM的回应与其他LLM的回应更为相似，而人类回应之间的多样性更大。这一关于评价范围内LLMs输出显著同质性的发现为正在进行的关于创造力和LLMs的讨论增添了新维度。如果今天的LLMs表现相似，无论使用哪种模型作为创意伙伴，都可能将所有用户引导向有限的“创意”输出。 

---
# Do Large Multimodal Models Solve Caption Generation for Scientific Figures? Lessons Learned from SCICAP Challenge 2023 

**Title (ZH)**: 大型多模态模型是否解决了科学图表的 caption 生成问题？来自 SCICAP 挑战 2023 的经验教训 

**Authors**: Ting-Yao E. Hsu, Yi-Li Hsu, Shaurya Rohatgi, Chieh-Yang Huang, Ho Yin Sam Ng, Ryan Rossi, Sungchul Kim, Tong Yu, Lun-Wei Ku, C. Lee Giles, Ting-Hao K. Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19353)  

**Abstract**: Since the SCICAP datasets launch in 2021, the research community has made significant progress in generating captions for scientific figures in scholarly articles. In 2023, the first SCICAP Challenge took place, inviting global teams to use an expanded SCICAP dataset to develop models for captioning diverse figure types across various academic fields. At the same time, text generation models advanced quickly, with many powerful pre-trained large multimodal models (LMMs) emerging that showed impressive capabilities in various vision-and-language tasks. This paper presents an overview of the first SCICAP Challenge and details the performance of various models on its data, capturing a snapshot of the fields state. We found that professional editors overwhelmingly preferred figure captions generated by GPT-4V over those from all other models and even the original captions written by authors. Following this key finding, we conducted detailed analyses to answer this question: Have advanced LMMs solved the task of generating captions for scientific figures? 

**Abstract (ZH)**: 自2021年SCICAP数据集发布以来，科研界在生成学术文章中科学图形的标题方面取得了显著进展。2023年，首次SCICAP挑战赛举行，邀请全球团队利用扩展的SCICAP数据集开发能够跨多种学术领域生成多种图形标题的模型。与此同时，文本生成模型快速发展，出现了许多强大的预训练多模态大模型（LMMs），这些模型在多种视觉和语言任务中展现了令人印象深刻的性能。本文概述了首次SCICAP挑战赛，并详细介绍了各种模型在数据上的性能表现，捕捉当前领域的状态。我们发现，专业编辑普遍更偏好GPT-4V生成的图形标题，甚至优于所有其他模型以及作者原写的标题。基于这一关键发现，我们进行了详细分析，以回答这个问题：高度先进的多模态大模型是否已经解决了生成科学图形标题的任务？ 

---
# Pathological MRI Segmentation by Synthetic Pathological Data Generation in Fetuses and Neonates 

**Title (ZH)**: 胎儿和新生儿病理MRI分割中的合成病理数据生成方法 

**Authors**: Misha P.T Kaandorp, Damola Agbelese, Hosna Asma-ull, Hyun-Gi Kim, Kelly Payette, Patrice Grehten, Gennari Antonio Giulio, Levente István Lánczi, Andras Jakab  

**Link**: [PDF](https://arxiv.org/pdf/2501.19338)  

**Abstract**: Developing new methods for the automated analysis of clinical fetal and neonatal MRI data is limited by the scarcity of annotated pathological datasets and privacy concerns that often restrict data sharing, hindering the effectiveness of deep learning models. We address this in two ways. First, we introduce Fetal&Neonatal-DDPM, a novel diffusion model framework designed to generate high-quality synthetic pathological fetal and neonatal MRIs from semantic label images. Second, we enhance training data by modifying healthy label images through morphological alterations to simulate conditions such as ventriculomegaly, cerebellar and pontocerebellar hypoplasia, and microcephaly. By leveraging Fetal&Neonatal-DDPM, we synthesize realistic pathological MRIs from these modified pathological label images. Radiologists rated the synthetic MRIs as significantly (p < 0.05) superior in quality and diagnostic value compared to real MRIs, demonstrating features such as blood vessels and choroid plexus, and improved alignment with label annotations. Synthetic pathological data enhanced state-of-the-art nnUNet segmentation performance, particularly for severe ventriculomegaly cases, with the greatest improvements achieved in ventricle segmentation (Dice scores: 0.9253 vs. 0.7317). This study underscores the potential of generative AI as transformative tool for data augmentation, offering improved segmentation performance in pathological cases. This development represents a significant step towards improving analysis and segmentation accuracy in prenatal imaging, and also offers new ways for data anonymization through the generation of pathologic image data. 

**Abstract (ZH)**: 开发用于自动分析临床胎儿和新生儿MRI数据的新方法受限于标注病理数据的稀缺性和隐私问题，这些问题常限制数据共享，妨碍深度学习模型的效果。我们通过两种方式应对这一挑战。首先，我们引入了一种名为Fetal&Neonatal-DDPM的新型扩散模型框架，该框架能够从语义标签图像中生成高质量的合成病理性胎儿和新生儿MRI图像。其次，我们通过形态学变化修改健康标签图像，以模拟如脑室扩大、小脑和小脑桥脑萎缩、小头畸形等状况，从而增强训练数据。利用Fetal&Neonatal-DDPM，我们能够从这些修改后的病理标签图像中合成逼真的病理MRI图像。放射科医生评估合成的MRI图像在质量（p < 0.05）和诊断价值方面明显优于真实MRI图像，能够更清晰地显示血管和脉络膜，并与标签注释更加一致。合成的病理数据显著提高了最先进的nnUNet分割 performance，特别是在严重脑室扩大的案例中，脑室分割的交集分割分数（Dice分数）显著提高（0.9253 vs. 0.7317）。这一研究表明，生成式AI可能成为数据增强的变革性工具，特别是在病理案例中提供更好的分割性能。这一发展代表了在产前成像分析和分割准确性改进方面的重要进步，并为数据匿名化提供了新的途径，通过生成病理图像数据。 

---
# What is causal about causal models and representations? 

**Title (ZH)**: 因果模型和表示究竟何以具有因果性？ 

**Authors**: Frederik Hytting Jørgensen, Luigi Gresele, Sebastian Weichwald  

**Link**: [PDF](https://arxiv.org/pdf/2501.19335)  

**Abstract**: Causal Bayesian networks are 'causal' models since they make predictions about interventional distributions. To connect such causal model predictions to real-world outcomes, we must determine which actions in the world correspond to which interventions in the model. For example, to interpret an action as an intervention on a treatment variable, the action will presumably have to a) change the distribution of treatment in a way that corresponds to the intervention, and b) not change other aspects, such as how the outcome depends on the treatment; while the marginal distributions of some variables may change as an effect. We introduce a formal framework to make such requirements for different interpretations of actions as interventions precise. We prove that the seemingly natural interpretation of actions as interventions is circular: Under this interpretation, every causal Bayesian network that correctly models the observational distribution is trivially also interventionally valid, and no action yields empirical data that could possibly falsify such a model. We prove an impossibility result: No interpretation exists that is non-circular and simultaneously satisfies a set of natural desiderata. Instead, we examine non-circular interpretations that may violate some desiderata and show how this may in turn enable the falsification of causal models. By rigorously examining how a causal Bayesian network could be a 'causal' model of the world instead of merely a mathematical object, our formal framework contributes to the conceptual foundations of causal representation learning, causal discovery, and causal abstraction, while also highlighting some limitations of existing approaches. 

**Abstract (ZH)**: 因果贝叶斯网络因其能够预测干预分布而被称为“因果”模型。为了将这种因果模型的预测与现实世界的结果联系起来，我们必须确定世界中的哪些行为对应于模型中的哪些干预。例如，若要将某种行为解释为对治疗变量的干预，这种行为需要a) 以与干预对应的方式改变治疗的分布，并且b) 不改变其他方面，例如结果如何依赖于治疗；尽管一些边缘分布可能会由于干预而发生变化。我们引入了一个正式框架，以使不同行为解释为干预的要求更加精确。我们证明，将行为解释为干预的直观解释是循环的：在这一解释下，每个正确描述观察分布的因果贝叶斯网络都会被自动认为是干预上有效，且任何行为都无法实际证伪这样的模型。我们证明了一个不可能的结果：不存在一种既非循环又同时满足一组自然要求的解释。相反，我们探讨了非循环的解释，这些解释可能会违反某些要求，并展示这种解释如何反过来使因果模型能够被证伪。通过严谨地探讨因果贝叶斯网络如何能够成为一个描述世界的“因果”模型而非仅仅是数学对象，我们的正式框架为因果表示学习、因果发现和因果抽象的概念基础做出了贡献，同时也指出了现有方法的一些限制。 

---
# Capturing Temporal Dynamics in Large-Scale Canopy Tree Height Estimation 

**Title (ZH)**: 在大规模林冠树木高度估计中捕捉时间动态 

**Authors**: Jan Pauls, Max Zimmer, Berkant Turan, Sassan Saatchi, Philippe Ciais, Sebastian Pokutta, Fabian Gieseke  

**Link**: [PDF](https://arxiv.org/pdf/2501.19328)  

**Abstract**: With the rise in global greenhouse gas emissions, accurate large-scale tree canopy height maps are essential for understanding forest structure, estimating above-ground biomass, and monitoring ecological disruptions. To this end, we present a novel approach to generate large-scale, high-resolution canopy height maps over time. Our model accurately predicts canopy height over multiple years given Sentinel-2 time series satellite data. Using GEDI LiDAR data as the ground truth for training the model, we present the first 10m resolution temporal canopy height map of the European continent for the period 2019-2022. As part of this product, we also offer a detailed canopy height map for 2020, providing more precise estimates than previous studies. Our pipeline and the resulting temporal height map are publicly available, enabling comprehensive large-scale monitoring of forests and, hence, facilitating future research and ecological analyses. For an interactive viewer, see this https URL. 

**Abstract (ZH)**: 随着全球温室气体排放量的增加，准确的大规模树冠高度图对于了解森林结构、估计地上生物量以及监测生态扰动至关重要。为此，我们提出了一种生成大规模高分辨率树冠高度图的新方法。我们的模型能够在给定Sentinel-2时间序列卫星数据的情况下，准确预测多个年份的树冠高度。利用GEDI激光雷达数据作为训练模型的真实数据，我们首次发布了2019-2022年期间欧洲大陆10米分辨率的树冠高度时间序列图。作为该产品的组成部分，我们还提供了2020年的详细树冠高度图，提供了比之前研究更精确的估计。我们的管道和生成的时间序列高度图均已公开，从而能够实现对森林的大规模监测，并促进未来的科研和生态分析。如需使用交互式查看器，请访问 <this URL>。 

---
# Reward-Guided Speculative Decoding for Efficient LLM Reasoning 

**Title (ZH)**: 基于奖励引导的推测性解码以实现高效的大模型推理 

**Authors**: Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, Caiming Xiong  

**Link**: [PDF](https://arxiv.org/pdf/2501.19324)  

**Abstract**: We introduce Reward-Guided Speculative Decoding (RSD), a novel framework aimed at improving the efficiency of inference in large language models (LLMs). RSD synergistically combines a lightweight draft model with a more powerful target model, incorporating a controlled bias to prioritize high-reward outputs, in contrast to existing speculative decoding methods that enforce strict unbiasedness. RSD employs a process reward model to evaluate intermediate decoding steps and dynamically decide whether to invoke the target model, optimizing the trade-off between computational cost and output quality. We theoretically demonstrate that a threshold-based mixture strategy achieves an optimal balance between resource utilization and performance. Extensive evaluations on challenging reasoning benchmarks, including Olympiad-level tasks, show that RSD delivers significant efficiency gains against decoding with the target model only (up to 4.4x fewer FLOPs), while achieving significant better accuracy than parallel decoding method on average (up to +3.5). These results highlight RSD as a robust and cost-effective approach for deploying LLMs in resource-intensive scenarios. 

**Abstract (ZH)**: 我们介绍了奖励引导推测解码（RSD，Reward-Guided Speculative Decoding）框架，该框架旨在提高大型语言模型（LLMs）推断效率。RSD 精心结合了一个轻量级草稿模型与一个更强大的目标模型，并且引入了一种可控的偏差机制，以优先输出高奖励的内容，这与现有的推测解码方法（这些方法强制实行严格的无偏性）形成了互补。RSD 使用过程奖励模型来评估中间解码步骤，并动态决定是否调用目标模型，从而优化计算成本与输出质量之间的权衡。我们从理论上证明了基于阈值的混合策略能够实现资源利用与性能之间的最优平衡。在包括奥林匹克级别任务在内的具有挑战性的推理基准测试中的广泛评估显示，RSD 在计算量方面能带来显著的效率增益（最多可以减少 4.4 倍的 FLOPs），同时在平均性能上显著优于并行解码方法（最多可提高 3.5 个百分点）。这些结果凸显了 RSD 在资源密集型场景中部署 LLM 的稳健性和成本效益。 

---
# Language Bias in Self-Supervised Learning For Automatic Speech Recognition 

**Title (ZH)**: 自动语音识别中自我监督学习的语言偏见 

**Authors**: Edward Storey, Naomi Harte, Peter Bell  

**Link**: [PDF](https://arxiv.org/pdf/2501.19321)  

**Abstract**: Self-supervised learning (SSL) is used in deep learning to train on large datasets without the need for expensive labelling of the data. Recently, large Automatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to train on over one hundred different languages simultaneously. However, deeper investigation shows that the bulk of the training data for XLS-R comes from a small number of languages. Biases learned through SSL have been shown to exist in multiple domains, but language bias in multilingual SSL ASR has not been thoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis (LTH) to identify language-specific subnetworks within XLS-R and test the performance of these subnetworks on a variety of different languages. We are able to show that when fine-tuning, XLS-R bypasses traditional linguistic knowledge and builds only on weights learned from the languages with the largest data contribution to the pretraining data. 

**Abstract (ZH)**: 自监督学习（SSL）在深度学习中被用于在无需昂贵数据标注的情况下对大量数据进行训练。近年来，诸如XLS-R等大型自动语音识别（ASR）模型利用SSL同时训练超过一百种不同的语言。然而，进一步的研究表明，XLS-R的主要训练数据来自少数几种语言。已有研究显示，通过SSL学到的偏差在多个领域中存在，但在多语言SSL ASR中的语言偏差尚未得到充分研究。本文中，我们利用彩票票假说（LTH）识别XLS-R中的语言特定子网络，并测试这些子网络在不同语言上的表现。我们发现，当进行微调时，XLS-R跳过了传统的语言知识，仅基于训练数据中数据贡献最大的语言学到的权重进行构建。 

---
# Beyond checkmate: exploring the creative chokepoints in AI text 

**Title (ZH)**: 超越将棋终局：探索AI文本生成中的创造性瓶颈 

**Authors**: Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee  

**Link**: [PDF](https://arxiv.org/pdf/2501.19301)  

**Abstract**: Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) and Artificial Intelligence (AI), unlocking unprecedented capabilities. This rapid advancement has spurred research into various aspects of LLMs, their text generation & reasoning capability, and potential misuse, fueling the necessity for robust detection methods. While numerous prior research has focused on detecting LLM-generated text (AI text) and thus checkmating them, our study investigates a relatively unexplored territory: portraying the nuanced distinctions between human and AI texts across text segments. Whether LLMs struggle with or excel at incorporating linguistic ingenuity across different text segments carries substantial implications for determining their potential as effective creative assistants to humans. Through an analogy with the structure of chess games-comprising opening, middle, and end games-we analyze text segments (introduction, body, and conclusion) to determine where the most significant distinctions between human and AI texts exist. While AI texts can approximate the body segment better due to its increased length, a closer examination reveals a pronounced disparity, highlighting the importance of this segment in AI text detection. Additionally, human texts exhibit higher cross-segment differences compared to AI texts. Overall, our research can shed light on the intricacies of human-AI text distinctions, offering novel insights for text detection and understanding. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经改变了自然语言处理（NLP）和人工智能（AI），解锁了前所未有的能力。这种快速的发展推动了对LLM们的各个方面，包括文本生成与推理能力，以及潜在滥用的研究，促生了对稳健检测方法的需求。虽然大量的先前研究集中在检测LLM生成的文本（AI文本）并对其进行遏制，但我们的研究旨在探索一个相对未被充分研究的领域：在不同文本段落中，描绘人类文本与AI文本之间的细微差别。LLM们在不同文本段落中处理或融合语言创新的能力差异，对确定它们作为有效的创意助手的潜力具有重要意义。通过将文本段落与国际象棋比赛的结构（开局、中局和残局）进行类比，我们对文本段落（引言、正文和结论）进行分析，以确定人类文本与AI文本之间差异最为显著的地方。尽管AI文本由于篇幅较长，可以更好地接近正文段，但深入研究揭示了这种差距尤为显著，突显了该段落对AI文本检测的重要性。此外，与AI文本相比，人类文本在不同段落之间的差异更大。总体而言，我们的研究可以揭示人类文本与AI文本的复杂差异，为文本检测和理解提供新颖的见解。 

---
# Analysis of LLMs vs Human Experts in Requirements Engineering 

**Title (ZH)**: 分析大型语言模型与人类专家在需求工程中的表现 

**Authors**: Cory Hymel, Hiroe Johnson  

**Link**: [PDF](https://arxiv.org/pdf/2501.19297)  

**Abstract**: The majority of research around Large Language Models (LLM) application to software development has been on the subject of code generation. There is little literature on LLMs' impact on requirements engineering (RE), which deals with the process of developing and verifying the system requirements. Within RE, there is a subdiscipline of requirements elicitation, which is the practice of discovering and documenting requirements for a system from users, customers, and other stakeholders. In this analysis, we compare LLM's ability to elicit requirements of a software system, as compared to that of a human expert in a time-boxed and prompt-boxed study. We found LLM-generated requirements were evaluated as more aligned (+1.12) than human-generated requirements with a trend of being more complete (+10.2%). Conversely, we found users tended to believe that solutions they perceived as more aligned had been generated by human experts. Furthermore, while LLM-generated documents scored higher and performed at 720x the speed, their cost was, on average, only 0.06% that of a human expert. Overall, these findings indicate that LLMs will play an increasingly important role in requirements engineering by improving requirements definitions, enabling more efficient resource allocation, and reducing overall project timelines. 

**Abstract (ZH)**: 关于大型语言模型（LLM）在软件开发中的应用，大多数研究集中在代码生成方面。对于需求工程（RE），即涉及系统需求开发和验证的过程，有关LLM影响的研究较少。需求工程中有一分支是需求获取，即从用户、客户和其他利益相关方中发现并记录系统需求的过程。在本分析中，我们比较了LLM与人类专家在限定时间和提示箱条件下获取软件系统需求的能力。我们发现，LLM生成的需求与人类生成的需求相比，更符合预期（平均评价得分为+1.12），且呈现更为完整（平均增幅+10.2%）的趋势。然而，用户倾向于认为那些看起来更符合预期的需求是由人类专家生成的。此外，尽管LLM生成的文档评分更高，执行速度是人类专家的720倍，但其平均成本仅为人类专家的0.06%。总体而言，这些发现表明，LLM将在需求工程中发挥越来越重要的作用，通过改进需求定义、提高资源分配效率并缩短项目时间线来助力需求工程的发展。 

---
# Linear $Q$-Learning Does Not Diverge: Convergence Rates to a Bounded Set 

**Title (ZH)**: 线性 $Q$-学习不会发散：收敛到有界集的速度 

**Authors**: Xinyu Liu, Zixuan Xie, Shangtong Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19254)  

**Abstract**: $Q$-learning is one of the most fundamental reinforcement learning algorithms. Previously, it is widely believed that $Q$-learning with linear function approximation (i.e., linear $Q$-learning) suffers from possible divergence. This paper instead establishes the first $L^2$ convergence rate of linear $Q$-learning to a bounded set. Notably, we do not make any modification to the original linear $Q$-learning algorithm, do not make any Bellman completeness assumption, and do not make any near-optimality assumption on the behavior policy. All we need is an $\epsilon$-softmax behavior policy with an adaptive temperature. The key to our analysis is the general result of stochastic approximations under Markovian noise with fast-changing transition functions. As a side product, we also use this general result to establish the $L^2$ convergence rate of tabular $Q$-learning with an $\epsilon$-softmax behavior policy, for which we rely on a novel pseudo-contraction property of the weighted Bellman optimality operator. 

**Abstract (ZH)**: $Q$-学习是强化学习中最基本的算法之一。以往普遍认为，带有线性函数近似的$Q$-学习（即线性$Q$-学习）可能会导致发散问题。本文首次确立了线性$Q$-学习向有界集收敛的$L^2$收敛速率。值得注意的是，我们并没有对原始的线性$Q$-学习算法进行任何修改，没有做贝尔曼完备性假设，也没有做行为策略接近最优性的假设。我们所要求的只是具有自适应温度的$\epsilon$-softmax行为策略。我们分析的关键在于在马尔可夫噪声下快速变化的转移函数情况下的随机近似的一般结果。作为副产品，我们还利用这一般结果，确立了具有$\epsilon$-softmax行为策略的表格$Q$-学习的$L^2$收敛速率，我们为此依赖于加权贝尔曼最优性算子的一个新奇的半收敛性质。 

---
# A Zero-Shot Generalization Framework for LLM-Driven Cross-Domain Sequential Recommendation 

**Title (ZH)**: 基于大语言模型驱动的跨域序列推荐的零样本泛化框架 

**Authors**: Yunzhe Li, Junting Wang, Hari Sundaram, Zhining Liu  

**Link**: [PDF](https://arxiv.org/pdf/2501.19232)  

**Abstract**: Zero-shot cross-domain sequential recommendation (ZCDSR) enables predictions in unseen domains without the need for additional training or fine-tuning, making it particularly valuable in data-sparse environments where traditional models struggle. Recent advancements in large language models (LLMs) have greatly improved ZCDSR by leveraging rich pretrained representations to facilitate cross-domain knowledge transfer. However, a key challenge persists: domain semantic bias, which arises from variations in vocabulary and content focus across domains. This misalignment leads to inconsistencies in item embeddings and hinders generalization.
To address this issue, we propose a novel framework designed to enhance LLM-based ZCDSR by improving cross-domain alignment at both the item and sequential levels. At the item level, we introduce a generalization loss that promotes inter-domain compactness by aligning embeddings of similar items across domains while maintaining intra-domain diversity to preserve unique item characteristics. This prevents embeddings from becoming overly generic while ensuring effective transferability. At the sequential level, we develop a method for transferring user behavioral patterns by clustering user sequences in the source domain and applying attention-based aggregation for target domain inference. This dynamic adaptation of user embeddings allows effective zero-shot recommendations without requiring target-domain interactions.
Comprehensive experiments across multiple datasets and domains demonstrate that our framework significantly improves sequential recommendation performance in the ZCDSR setting. By mitigating domain bias and enhancing the transferability of sequential patterns, our method provides a scalable and robust approach for achieving more effective zero-shot recommendations across domains. 

**Abstract (ZH)**: 无监督跨域序列推荐（Zero-shot Cross-Domain Sequential Recommendation, ZCDSR）能够在未见过的领域进行预测，无需额外的训练或微调，尤其适用于传统模型难以应对的数据稀疏环境。最近在大型语言模型（Large Language Models, LLMs）方面的进展大幅提升了ZCDSR，通过利用丰富的预训练表示来促进跨域知识迁移。然而，一个关键挑战依然存在：领域语义偏差，这源于不同领域词汇和内容关注点的差异导致的不一致。这种不一致导致项目嵌入的不一致性，从而阻碍了一般化能力。

为解决这一问题，我们提出了一种新的框架，旨在通过改进LLM基础的ZCDSR来提高跨域对齐。在项目层面，我们引入了一种泛化损失，通过跨领域对类似项目的嵌入进行对齐，以增加它们的紧凑性，同时保持 intra-domain 的多样性，以便保留项目的独特特征。这防止嵌入变得过于通用，同时确保有效迁移能力。在序列层面，我们开发了一种方法，通过在源领域聚类用户序列表征，并应用注意力聚合来推断目标领域的用户行为模式。这种动态的用户嵌入适应能力使得在无需目标领域交互的情况下，能够实现有效的无监督推荐。

我们通过多个数据集和领域的全面实验表明，我们的框架显著提高了ZCDSR设置中的序列推荐性能。通过缓解领域偏差并增强序列模式的迁移性，我们的方法提供了一种可扩展且稳健的方法，能够在不同领域中实现更有效的无监督推荐。 

---
# Integrating Semi-Supervised and Active Learning for Semantic Segmentation 

**Title (ZH)**: 将半监督学习和主动学习集成应用于语义分割 

**Authors**: Wanli Ma, Oktay Karakus, Paul L. Rosin  

**Link**: [PDF](https://arxiv.org/pdf/2501.19227)  

**Abstract**: In this paper, we propose a novel active learning approach integrated with an improved semi-supervised learning framework to reduce the cost of manual annotation and enhance model performance. Our proposed approach effectively leverages both the labelled data selected through active learning and the unlabelled data excluded from the selection process. The proposed active learning approach pinpoints areas where the pseudo-labels are likely to be inaccurate. Then, an automatic and efficient pseudo-label auto-refinement (PLAR) module is proposed to correct pixels with potentially erroneous pseudo-labels by comparing their feature representations with those of labelled regions. This approach operates without increasing the labelling budget and is based on the cluster assumption, which states that pixels belonging to the same class should exhibit similar representations in feature space. Furthermore, manual labelling is only applied to the most difficult and uncertain areas in unlabelled data, where insufficient information prevents the PLAR module from making a decision. We evaluated the proposed hybrid semi-supervised active learning framework on two benchmark datasets, one from natural and the other from remote sensing imagery domains. In both cases, it outperformed state-of-the-art methods in the semantic segmentation task. 

**Abstract (ZH)**: 在本文中，我们提出了一种结合改进的半监督学习框架的新颖主动学习方法，以降低人工标注的成本并提升模型性能。所提出的方法有效地利用了主动学习中选中的标记数据以及被排除在选点过程之外的未标记数据。提出的新颖主动学习方法指出了伪标签可能出现不准确的区域。随后，我们提出了一种自动且高效的伪标签自动精炼（PLAR）模块，通过比较潜在错误伪标签像素的特征表示与其已经标记区域的特征表示来进行像素修正。该方法未增加标注预算，并基于聚类假设，即属于同一类别的像素在特征空间中应具有相似的表示。此外，仅对未标记数据中最难且最不确定的区域进行人工标注，这些区域缺乏足够的信息使得PLAR模块无法做出决策。我们分别在自然场景和遥感影像数据集上评估了所提出的混合半监督主动学习框架，在语义分割任务中，该方法均优于现有最先进的方法。 

---
# Strassen Attention: Unlocking Compositional Abilities in Transformers Based on a New Lower Bound Method 

**Title (ZH)**: 斯特拉斯宾格注意力：基于新型下界方法解锁Transformer的组合能力 

**Authors**: Alexander Kozachinskiy, Felipe Urrutia, Hector Jimenez, Tomasz Steifer, Germán Pizarro, Matías Fuentes, Francisco Meza, Cristian Buc, Cristóbal Rojas  

**Link**: [PDF](https://arxiv.org/pdf/2501.19215)  

**Abstract**: We propose a novel method to evaluate the theoretical limits of Transformers, allowing us to prove the first lower bounds against one-layer softmax Transformers with infinite precision. We establish those bounds for three tasks that require advanced reasoning. The first task, Match3 (Sanford et al., 2023), requires looking at all triples of positions. The second and third tasks address compositionality-based reasoning: one is composition of functions (Peng et al., 2024) and the other is composition of binary relations. We formally prove the inability of one-layer softmax Transformers to solve any of these tasks. In an attempt to overcome these limitations, we introduce Strassen attention and prove that with this mechanism a one-layer Transformer can in principle solve all these tasks. We also show that it enjoys sub-cubic running-time complexity, making it more scalable than similar previously proposed mechanisms, such as higher-order attention (Sanford et al., 2023). To complement our theoretical findings, we experimentally studied Strassen attention and compared it against standard (Vaswani et al, 2017), higher-order attention (Sanford et al., 2023) and triangular attention (Bergen et al. 2021). Our results help to disentangle all these attention mechanisms, highlighting their strengths and limitations. In particular, Strassen attention outperforms standard attention significantly on all the tasks. Altogether, understanding the theoretical limitations can guide research towards scalable attention mechanisms that improve the reasoning abilities of Transformers. 

**Abstract (ZH)**: 我们提出了一种新的方法来评估Transformer的理论极限，从而使我们能够证明具有无限精度的一层softmax Transformer的第一个下界。我们为三个要求高级推理的任务建立了这些界限。第一个任务，Match3（Sanford et al., 2023），要求检查所有三元组的位置。第二个和第三个任务涉及基于组合性的推理：一个是函数的组合（Peng et al., 2024），另一个是二元关系的组合。我们正式证明了一层softmax Transformer无法解决这些任务中的任何一项。为了克服这些局限，我们引入了Strassen注意力，并证明通过这一机制，一层Transformer原则上可以解决所有这些任务。我们还展示了其亚三次运行时间复杂性，使其比其他先前提出的机制（如高阶注意力，Sanford et al., 2023）更具可扩展性。

为了补充我们的理论发现，我们实验研究了Strassen注意力，并将其与标准注意力（Vaswani et al., 2017）、高阶注意力（Sanford et al., 2023）和三角注意力（Bergen et al., 2021）进行了比较。我们的实验结果有助于区分这些注意力机制，突出它们的优势和局限性。特别是，Strassen注意力在所有任务上显著优于标准注意力。总体而言，理解这些理论极限可以指导研究，以开发更具扩展性的注意力机制，从而提高Transformer的推理能力。 

---
# Single cell resolution 3D imaging and segmentation within intact live tissues 

**Title (ZH)**: 整块活组织内的单细胞分辨率三维成像与分割 

**Authors**: G. Paci, P. Vicente-Munuera, I. Fernandez-Mosquera, A. Miranda, K. Lau, Q. Zhang, R. Barrientos, Y. Mao  

**Link**: [PDF](https://arxiv.org/pdf/2501.19203)  

**Abstract**: Epithelial cells form diverse structures from squamous spherical organoids to densely packed pseudostratified tissues. Quantification of cellular properties in these contexts requires high-resolution deep imaging and computational techniques to achieve truthful three-dimensional (3D) structural features. Here, we describe a detailed step-by-step protocol for sample preparation, imaging and deep-learning-assisted cell segmentation to achieve accurate quantification of fluorescently labelled individual cells in 3D within live tissues. We share the lessons learned through troubleshooting 3D imaging of Drosophila wing discs, including considerations on the choice of microscopy modality and settings (objective, sample mounting) and available segmentation methods. In addition, we include a computational pipeline alongside custom code to assist replication of the protocol. While we focus on the segmentation of cell outlines from membrane labelling, this protocol applies to a wide variety of samples, and we believe it be valuable for studying other tissues that demand complex analysis in 3D. 

**Abstract (ZH)**: 上皮细胞可以形成多种结构，从扁平的球状器官球到紧密排列的假复层组织。在这些不同结构中对细胞属性进行量化需要采用高分辨率显微成像技术和计算方法以获得真实的三维（3D）结构特征。本文描述了一种详细的样本制备、成像和深度学习辅助细胞分割的步骤协议，以实现对活组织中荧光标记单个细胞的准确3D量化。我们分享了通过解决果蝇翅膀盘的3D成像问题所获得的经验教训，涵盖了显微镜成像模式与参数（物镜、样品安装）以及可用分割方法的选择考虑。此外，我们还提供了一个计算管道和自定义代码以辅助该协议的重复实施。虽然本协议主要针对膜标记细胞轮廓的分割，但其所适用的样本范围广泛，我们相信该协议对于研究其他需要复杂3D分析的组织也是非常有价值的。 

---
# Efficient Reasoning with Hidden Thinking 

**Title (ZH)**: 高效的隐式思维推理 

**Authors**: Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, Jiuxiang Gu  

**Link**: [PDF](https://arxiv.org/pdf/2501.19201)  

**Abstract**: Chain-of-Thought (CoT) reasoning has become a powerful framework for improving complex problem-solving capabilities in Multimodal Large Language Models (MLLMs). However, the verbose nature of textual reasoning introduces significant inefficiencies. In this work, we propose $\textbf{Heima}$ (as hidden llama), an efficient reasoning framework that leverages reasoning CoTs at hidden latent space. We design the Heima Encoder to condense each intermediate CoT into a compact, higher-level hidden representation using a single thinking token, effectively minimizing verbosity and reducing the overall number of tokens required during the reasoning process. Meanwhile, we design corresponding Heima Decoder with traditional Large Language Models (LLMs) to adaptively interpret the hidden representations into variable-length textual sequence, reconstructing reasoning processes that closely resemble the original CoTs. Experimental results across diverse reasoning MLLM benchmarks demonstrate that Heima model achieves higher generation efficiency while maintaining or even better zero-shot task accuracy. Moreover, the effective reconstruction of multimodal reasoning processes with Heima Decoder validates both the robustness and interpretability of our approach. 

**Abstract (ZH)**: 链式推理（Chain-of-Thought, CoT）已成为增强多模态大型语言模型（Multimodal Large Language Models, MLLMs）复杂问题解决能力的强大框架。然而，文本推理的冗长性引入了显著的效率问题。在本文中，我们提出了一种名为**heima**（隐藏的 llama）的有效推理框架，该框架在隐藏的潜在空间中利用推理链式结构。我们设计了heima编码器，通过单个思考令牌将每个中间链式推理压缩成紧凑的高层次隐藏表示，从而有效减少了冗余并减少了推理过程中所需的令牌数量。同时，我们设计了对应的heima解码器，通过传统的大型语言模型（Large Language Models, LLMs），自适应地将隐藏表示解码为变长的文本序列，从而重构出与原始链式推理过程类似的推理过程。在多种多模态推理MLLM基准测试中的实验结果表明，heima模型在保持或甚至提高零样本任务准确性的同时，实现了更高的生成效率。此外，heima解码器对多模态推理过程的有效重构验证了我们方法的鲁棒性和可解释性。 

---
# Rethinking Early Stopping: Refine, Then Calibrate 

**Title (ZH)**: 重新思考早期停止：先细化，后校准 

**Authors**: Eugène Berta, David Holzmüller, Michael I. Jordan, Francis Bach  

**Link**: [PDF](https://arxiv.org/pdf/2501.19195)  

**Abstract**: Machine learning classifiers often produce probabilistic predictions that are critical for accurate and interpretable decision-making in various domains. The quality of these predictions is generally evaluated with proper losses like cross-entropy, which decompose into two components: calibration error assesses general under/overconfidence, while refinement error measures the ability to distinguish different classes. In this paper, we provide theoretical and empirical evidence that these two errors are not minimized simultaneously during training. Selecting the best training epoch based on validation loss thus leads to a compromise point that is suboptimal for both calibration error and, most importantly, refinement error. To address this, we introduce a new metric for early stopping and hyperparameter tuning that makes it possible to minimize refinement error during training. The calibration error is minimized after training, using standard techniques. Our method integrates seamlessly with any architecture and consistently improves performance across diverse classification tasks. 

**Abstract (ZH)**: 机器学习分类器经常生成概率预测，这对于各个领域的准确且可解释的决策制定至关重要。这些预测的质量通常通过适当的损失函数如交叉熵来评估，这些损失函数可以分解为两个部分：校准误差评估一般性下的不足或过度自信，而细化误差衡量区分不同类别的能力。在本文中，我们提供了理论和实验证据表明，在训练过程中这两个误差并不会同时被最小化。基于验证损失选择最优训练周期因此会导致一个对于校准误差和最重要的是细化误差都为次优的妥协点。为了解决这一问题，我们引入了一个新的早期停止和超参数调整的评价指标，使得在训练过程中能够最小化细化误差。校准误差则在训练完成后通过标准技术进行最小化。我们的方法可以无缝集成到任何架构中，并且在各种分类任务中都能一致性地提高性能。 

---
# Secured Communication Schemes for UAVs in 5G: CRYSTALS-Kyber and IDS 

**Title (ZH)**: 5G环境中的无人机安全通信方案：CRYSTALS-Kyber和IDS 

**Authors**: Taneya Sharma, Seyed Ahmad Soleymani, Mohammad Shojafar, Rahim Tafazolli  

**Link**: [PDF](https://arxiv.org/pdf/2501.19191)  

**Abstract**: This paper introduces a secure communication architecture for Unmanned Aerial Vehicles (UAVs) and ground stations in 5G networks, addressing critical challenges in network security. The proposed solution integrates the Advanced Encryption Standard (AES) with Elliptic Curve Cryptography (ECC) and CRYSTALS-Kyber for key encapsulation, offering a hybrid cryptographic approach. By incorporating CRYSTALS-Kyber, the framework mitigates vulnerabilities in ECC against quantum attacks, positioning it as a quantum-resistant alternative. The architecture is based on a server-client model, with UAVs functioning as clients and the ground station acting as the server. The system was rigorously evaluated in both VPN and 5G environments. Experimental results confirm that CRYSTALS-Kyber delivers strong protection against quantum threats with minimal performance overhead, making it highly suitable for UAVs with resource constraints. Moreover, the proposed architecture integrates an Artificial Intelligence (AI)-based Intrusion Detection System (IDS) to further enhance security. In performance evaluations, the IDS demonstrated strong results across multiple models with XGBoost, particularly in more demanding scenarios, outperforming other models with an accuracy of 97.33% and an AUC of 0.94. These findings underscore the potential of combining quantum-resistant encryption mechanisms with AI-driven IDS to create a robust, scalable, and secure communication framework for UAV networks, particularly within the high-performance requirements of 5G environments. 

**Abstract (ZH)**: 本文介绍了在5G网络中用于无人机（UAV）和地面站之间安全通信的架构，解决了网络安全性中的关键挑战。提出的解决方案将高级加密标准（AES）与椭圆曲线密码学（ECC）以及CRYSTALS-Kyber集成到密钥封装中，采用了一种混合加密方法。通过引入CRYSTALS-Kyber，该框架能够缓解ECC在遭受量子攻击时的漏洞，使它成为一种抗量子攻击的替代方案。该架构基于服务器-客户端模型，无人机作为客户端，地面站作为服务器。系统在虚拟专用网络（VPN）和5G环境中进行了严格的评估。实验结果表明，CRYSTALS-Kyber能够以最小的性能开销提供强大的量子威胁防护，使其非常适合资源受限的无人机领域。此外，提出的架构整合了基于人工智能（AI）的入侵检测系统（IDS），进一步增强了安全性。在性能评估中，IDS在多种模型（特别是使用XGBoost模型）中表现出色，在更具挑战性的场景中尤其表现优异，其准确率高达97.33%，AUC值为0.94。这些发现强调了结合抗量子加密机制与AI驱动的IDS构建抗量子、可扩展且安全的无人机网络通信架构的潜力，尤其是在5G环境的高性能要求下。 

---
# Enhancing Model Defense Against Jailbreaks with Proactive Safety Reasoning 

**Title (ZH)**: 增强模型防御以对抗 Jailbreak 攻击：基于前瞻性安全性推理的方法 

**Authors**: Xianglin Yang, Gelei Deng, Jieming Shi, Tianwei Zhang, Jin Song Dong  

**Link**: [PDF](https://arxiv.org/pdf/2501.19180)  

**Abstract**: Large language models (LLMs) are vital for a wide range of applications yet remain susceptible to jailbreak threats, which could lead to the generation of inappropriate responses. Conventional defenses, such as refusal and adversarial training, often fail to cover corner cases or rare domains, leaving LLMs still vulnerable to more sophisticated attacks. We propose a novel defense strategy, Safety Chain-of-Thought (SCoT), which harnesses the enhanced \textit{reasoning capabilities} of LLMs for proactive assessment of harmful inputs, rather than simply blocking them. SCoT augments any refusal training datasets to critically analyze the intent behind each request before generating answers. By employing proactive reasoning, SCoT enhances the generalization of LLMs across varied harmful queries and scenarios not covered in the safety alignment corpus. Additionally, it generates detailed refusals specifying the rules violated. Comparative evaluations show that SCoT significantly surpasses existing defenses, reducing vulnerability to out-of-distribution issues and adversarial manipulations while maintaining strong general capabilities. 

**Abstract (ZH)**: 大规模语言模型（LLMs）在广泛的应用中发挥着关键作用，但仍然容易受到脱管攻击的威胁，这可能导致生成不适当的回答。传统防御措施，如拒绝和对抗训练，往往难以涵盖边缘案例或稀有领域，导致LLMs仍然容易受到更复杂的攻击。我们提出了一种新颖的防御策略，安全链式思考（SCoT），它利用了LLMs增强的推理能力进行主动评估有害输入，而不是简单地拒绝它们。SCoT 增强了任何拒绝训练数据集，在生成答案之前，针对每个请求认真分析其背后的目的。通过采用主动推理，SCoT 提高了LLMs在各种未涵盖在安全性对齐语料中的有害查询和场景中的泛化能力。此外，它还生成详细的拒绝理由，明确说明违反了哪些规则。比较评估表明，SCoT 显著超越了现有防御措施，减少了对分布外问题和对抗操纵的脆弱性，同时保持了强大的通用能力。 

---
# Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer Using Generative Artificial Intelligence 

**Title (ZH)**: 使用生成式人工智能实现乳腺癌多模态虚拟活检的增强智能 

**Authors**: Aurora Rofena, Claudia Lucia Piccolo, Bruno Beomonte Zobel, Paolo Soda, Valerio Guarrasi  

**Link**: [PDF](https://arxiv.org/pdf/2501.19176)  

**Abstract**: Full-Field Digital Mammography (FFDM) is the primary imaging modality for routine breast cancer screening; however, its effectiveness is limited in patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced Spectral Mammography (CESM), a second-level imaging technique, offers enhanced accuracy in tumor detection. Nonetheless, its application is restricted due to higher radiation exposure, the use of contrast agents, and limited accessibility. As a result, CESM is typically reserved for select cases, leaving many patients to rely solely on FFDM despite the superior diagnostic performance of CESM. While biopsy remains the gold standard for definitive diagnosis, it is an invasive procedure that can cause discomfort for patients. We introduce a multimodal, multi-view deep learning approach for virtual biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral oblique views to classify lesions as malignant or benign. To address the challenge of missing CESM data, we leverage generative artificial intelligence to impute CESM images from FFDM scans. Experimental results demonstrate that incorporating the CESM modality is crucial to enhance the performance of virtual biopsy. When real CESM data is missing, synthetic CESM images proved effective, outperforming the use of FFDM alone, particularly in multimodal configurations that combine FFDM and CESM modalities. The proposed approach has the potential to improve diagnostic workflows, providing clinicians with augmented intelligence tools to improve diagnostic accuracy and patient care. Additionally, as a contribution to the research community, we publicly release the dataset used in our experiments, facilitating further advancements in this field. 

**Abstract (ZH)**: 数字全视野乳腺X线摄影（FFDM）是常规乳腺癌筛查的主要成像技术，但在乳腺组织致密或伴有纤维囊性病变的患者中其效果受到限制。对比增强光谱乳腺X线摄影（CESM）是一种高级成像技术，能够提高肿瘤检测的准确性，但其应用受到较高的辐射暴露、对比剂使用和较低的可及性限制，在许多情况下只能用于特定病例，导致许多患者只能依赖FFDM成像，尽管CESM在诊断性能上优于FFDM。尽管活检是最终确诊的金标准，但它仍然是一个侵入性程序，可能会给患者带来不适。我们提出了一种多模态、多视角深度学习方法，用于虚拟活检，通过整合在头足位和腋中线倾斜位上的FFDM和CESM成像模态，将病变分类为恶性或良性。为了解决CESM数据缺失的挑战，我们利用生成人工智能从FFDM扫描中补全CESM图像。实验结果表明，在虚拟活检中集成CESM成像模态是提高诊断性能的关键。在缺少真实CESM数据的情况下，合成CESM图像表现出色，特别是在结合FFDM和CESM成像模态的多模态配置中优于单独使用FFDM。该方法有望改进诊断工作流程，为临床医生提供增强的人工智能工具，提高诊断准确性和患者护理水平。此外，作为对研究社区的贡献，我们将实验中使用的数据集公开展示，促进该领域的进一步发展。 

---
# SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation 

**Title (ZH)**: SWAT：滑动窗口对抗训练在逐步领域适应中的应用 

**Authors**: Zixi Wang, Yubo Huang, Wenwei Luo, Tonglan Xie, Mengmeng Jing, Lin Zuo  

**Link**: [PDF](https://arxiv.org/pdf/2501.19155)  

**Abstract**: Domain shifts are critical issues that harm the performance of machine learning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers when the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA) alleviates this problem in a mild way by gradually adapting from the source to the target domain using multiple intermediate domains. In this paper, we propose Sliding Window Adversarial Training (SWAT) for Gradual Domain Adaptation. SWAT uses the construction of adversarial streams to connect the feature spaces of the source and target domains. In order to gradually narrow the small gap between adjacent intermediate domains, a sliding window paradigm is designed that moves along the adversarial stream. When the window moves to the end of the stream, i.e., the target domain, the domain shift is drastically reduced. Extensive experiments are conducted on public GDA benchmarks, and the results demonstrate that the proposed SWAT significantly outperforms the state-of-the-art approaches. The implementation is available at: this https URL. 

**Abstract (ZH)**: 领域变换是严重损害机器学习性能的关键问题。无监督领域适应（UDA）可以缓解这一问题，但在领域变换陡峭且剧烈时效果不佳。渐进领域适应（GDA）通过使用多个中间领域逐渐从源域适应到目标域，以温和的方式缓解这一问题。本文提出了一种滑动窗口对抗训练（SWAT）方法，用于渐进领域适应。SWAT 通过构建对手流来连接源域和目标域的特征空间。为了逐步缩小相邻中间领域之间的细微差距，设计了滑动窗口范式，该范式沿着对手流移动。当窗口移动到对手流的末端即目标域时，领域变换显著降低。在公共 GDA 基准上进行了广泛的实验，并且结果表明所提出的 SWAT 显著优于现有方法。该实现可从以下链接获得：this https URL。 

---
# On the inductive bias of infinite-depth ResNets and the bottleneck rank 

**Title (ZH)**: 无限深度ResNet的归纳偏置及其瓶颈秩分析 

**Authors**: Enric Boix-Adsera  

**Link**: [PDF](https://arxiv.org/pdf/2501.19149)  

**Abstract**: We compute the minimum-norm weights of a deep linear ResNet, and find that the inductive bias of this architecture lies between minimizing nuclear norm and rank. This implies that, with appropriate hyperparameters, deep nonlinear ResNets have an inductive bias towards minimizing bottleneck rank. 

**Abstract (ZH)**: 我们计算了深层线性ResNet的最小范数权重，并发现该架构的归纳偏置位于最小核范数和秩之间。这表明，通过合适的超参数设置，深层非线性ResNet具有趋向于最小化瓶颈秩的归纳偏置。 

---
# Improving Multi-Label Contrastive Learning by Leveraging Label Distribution 

**Title (ZH)**: 通过利用标签分布改善多标签对比学习 

**Authors**: Ning Chen, Shen-Huan Lyu, Tian-Shuang Wu, Yanyan Wang, Bin Tang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19145)  

**Abstract**: In multi-label learning, leveraging contrastive learning to learn better representations faces a key challenge: selecting positive and negative samples and effectively utilizing label information. Previous studies selected positive and negative samples based on the overlap between labels and used them for label-wise loss balancing. However, these methods suffer from a complex selection process and fail to account for the varying importance of different labels. To address these problems, we propose a novel method that improves multi-label contrastive learning through label distribution. Specifically, when selecting positive and negative samples, we only need to consider whether there is an intersection between labels. To model the relationships between labels, we introduce two methods to recover label distributions from logical labels, based on Radial Basis Function (RBF) and contrastive loss, respectively. We evaluate our method on nine widely used multi-label datasets, including image and vector datasets. The results demonstrate that our method outperforms state-of-the-art methods in six evaluation metrics. 

**Abstract (ZH)**: 在多标签学习中，利用对比学习来学到更好的表示面临着一个关键挑战：选择正负样本并在标签层面平衡损失函数。之前的研究根据标签之间的重叠来选择正负样本，并基于此进行标签层面的损失平衡。然而，这些方法面临着复杂的正负样本选择过程，并不能考虑到不同标签的重要性差异。为了解决这些问题，我们提出了一种新的方法，通过标签分布来改进多标签对比学习。具体来说，在选择正负样本时，我们只需要考虑标签之间是否存在交集。为了建模标签之间的关系，我们提出了两种方法从逻辑标签中恢复标签分布，分别是基于径向基函数（RBF）和对比损失的方法。我们在九个广泛使用的多标签数据集上评估了我们的方法，包括图像和向量数据集。结果表明，我们的方法在六个评估指标上优于现有的最新方法。 

---
# A Metric for the Balance of Information in Graph Learning 

**Title (ZH)**: 图学习中信息均衡的度量标准 

**Authors**: Alex O. Davies, Nirav S. Ajmeri, Telmo de Menezes e Silva Filho  

**Link**: [PDF](https://arxiv.org/pdf/2501.19137)  

**Abstract**: Graph learning on molecules makes use of information from both the molecular structure and the features attached to that structure. Much work has been conducted on biasing either towards structure or features, with the aim that bias bolsters performance. Identifying which information source a dataset favours, and therefore how to approach learning that dataset, is an open issue. Here we propose Noise-Noise Ratio Difference (NNRD), a quantitative metric for whether there is more useful information in structure or features. By employing iterative noising on features and structure independently, leaving the other intact, NNRD measures the degradation of information in each. We employ NNRD over a range of molecular tasks, and show that it corresponds well to a loss of information, with intuitive results that are more expressive than simple performance aggregates. Our future work will focus on expanding data domains, tasks and types, as well as refining our choice of baseline model. 

**Abstract (ZH)**: 分子结构学习综合了分子结构及其特征上的信息。在这一领域，大量的工作集中在偏向结构或特征上，目标是通过偏差增强模型性能。识别数据集更倾向于哪种信息来源，以及因此如何有效地进行学习，仍是一个开放的问题。本文提出了噪声-噪声比率差异（Noise-Noise Ratio Difference，NNRD），这是一种定量衡量结构信息和特征信息中更有用信息的比例的变化的指标。通过独立迭代对特征和结构进行去噪处理，而保留另一种信息源不变，NNRD衡量了信息流失的程度。我们利用NNRD在一系列分子任务中进行了测试，并展示了其与信息流失程度之间的良好对应关系，结果直观且比简单的性能汇总指标更具表达力。我们的未来研究将集中在扩大数据领域、任务类型以及优化基线模型选择上。 

---
# Decorrelated Soft Actor-Critic for Efficient Deep Reinforcement Learning 

**Title (ZH)**: 独立相关软 Actor-Critic 以实现高效的深度强化学习 

**Authors**: Burcu Küçükoğlu, Sander Dalm, Marcel van Gerven  

**Link**: [PDF](https://arxiv.org/pdf/2501.19133)  

**Abstract**: The effectiveness of credit assignment in reinforcement learning (RL) when dealing with high-dimensional data is influenced by the success of representation learning via deep neural networks, and has implications for the sample efficiency of deep RL algorithms. Input decorrelation has been previously introduced as a method to speed up optimization in neural networks, and has proven impactful in both efficient deep learning and as a method for effective representation learning for deep RL algorithms. We propose a novel approach to online decorrelation in deep RL based on the decorrelated backpropagation algorithm that seamlessly integrates the decorrelation process into the RL training pipeline. Decorrelation matrices are added to each layer, which are updated using a separate decorrelation learning rule that minimizes the total decorrelation loss across all layers, in parallel to minimizing the usual RL loss. We used our approach in combination with the soft actor-critic (SAC) method, which we refer to as decorrelated soft actor-critic (DSAC). Experiments on the Atari 100k benchmark with DSAC shows, compared to the regular SAC baseline, faster training in five out of the seven games tested and improved reward performance in two games with around 50% reduction in wall-clock time, while maintaining performance levels on the other games. These results demonstrate the positive impact of network-wide decorrelation in deep RL for speeding up its sample efficiency through more effective credit assignment. 

**Abstract (ZH)**: 高维数据处理中强化学习（RL）中的信用分配效果受深度神经网络表示学习成功的影響，并对深度RL算法的样本效率具有重要意义。输入去相关曾被提出作为一种加速神经网络优化的方法，并在高效深度学习和有效的深度RL表示学习方面证明具有影响力。我们提出了一种基于去相关反向传播算法的新型在线去相关方法，该方法无缝地将去相关过程集成到RL训练管道中。在每一层中添加去相关矩阵，并使用单独的去相关学习规则更新这些矩阵，该规则旨在最小化所有层的总去相关损失，同时继续最小化通常的RL损失。我们使用该方法与软演员-评论家（SAC）方法结合，我们称之为去相关软演员-评论家（DSAC）。实验结果表明，在对Atari 100k基准测试的七款游戏中，DSAC相比于常规SAC基准，在五款游戏中具有更快的训练速度，并在两款游戏中通过大约50%的墙钟时间减少，实现了更好的奖励性能，而在其他游戏中保持了相同的性能水平。这些结果表明，网络范围内的全局去相关对于通过更有效的信用分配提高深度RL的样本效率具有积极影响。 

---
# Shaping Sparse Rewards in Reinforcement Learning: A Semi-supervised Approach 

**Title (ZH)**: 强化学习中稀疏奖励的塑造：一种半监督方法 

**Authors**: Wenyun Li, Wenjie Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19128)  

**Abstract**: In many real-world scenarios, reward signal for agents are exceedingly sparse, making it challenging to learn an effective reward function for reward shaping. To address this issue, our approach performs reward shaping not only by utilizing non-zero-reward transitions but also by employing the Semi-Supervised Learning (SSL) technique combined with a novel data augmentation to learn trajectory space representations from the majority of transitions, zero-reward transitions, thereby improving the efficacy of reward shaping. Experimental results in Atari and robotic manipulation demonstrate that our method effectively generalizes reward shaping to sparse reward scenarios, achieving up to four times better performance in reaching higher best scores compared to curiosity-driven methods. The proposed double entropy data augmentation enhances performance, showcasing a 15.8\% increase in best score over other augmentation methods. 

**Abstract (ZH)**: 在许多现实场景中，代理的奖励信号极为稀疏，这使得学习有效的奖励函数以进行奖励塑造变得非常具有挑战性。为了解决这一问题，我们的方法不仅通过利用非零奖励过渡来进行奖励塑造，还结合半监督学习（SSL）技术和一种新颖的数据增强方法，从大多数过渡（包括零奖励过渡）中学习轨迹空间表示，从而提高奖励塑造的有效性。我们在Atari游戏和机器人操作中的实验结果表明，我们的方法能够有效地将奖励塑造推广到稀疏奖励场景，相较于好奇心驱动的方法，在达到更高最佳得分方面可获得高达四倍的性能提升。所提出的双重熵数据增强方法能够进一步提升性能，相较于其他数据增强方法，最佳得分提高了15.8%。 

---
# FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling 

**Title (ZH)**: FedRTS：基于组合 theta 猜测的联邦鲁棒剪枝 

**Authors**: Hong Huang, Hai Yang, Yuan Chen, Jiaxun Ye, Dapeng Wu  

**Link**: [PDF](https://arxiv.org/pdf/2501.19122)  

**Abstract**: Federated Learning (FL) enables collaborative model training across distributed clients without data sharing, but its high computational and communication demands strain resource-constrained devices. While existing methods use dynamic pruning to improve efficiency by periodically adjusting sparse model topologies while maintaining sparsity, these approaches suffer from issues such as greedy adjustments, unstable topologies, and communication inefficiency, resulting in less robust models and suboptimal performance under data heterogeneity and partial client availability. To address these challenges, we propose Federated Robust pruning via combinatorial Thompson Sampling (FedRTS), a novel framework designed to develop robust sparse models. FedRTS enhances robustness and performance through its Thompson Sampling-based Adjustment (TSAdj) mechanism, which uses probabilistic decisions informed by stable, farsighted information instead of deterministic decisions reliant on unstable and myopic information in previous methods. Extensive experiments demonstrate that FedRTS achieves state-of-the-art performance in computer vision and natural language processing tasks while reducing communication costs, particularly excelling in scenarios with heterogeneous data distributions and partial client participation. Our codes are available at: this https URL 

**Abstract (ZH)**: 联邦学习（FL）能够在不共享数据的情况下实现分布式客户端间的协作模型训练，但其对计算和通信的高要求给资源受限的设备带来了压力。现有方法通过周期性调整稀疏模型拓扑结构以保持稀疏性来提高效率，但这些方法存在贪婪调整、拓扑结构不稳定和通信低效等问题，导致在数据异质性和部分客户端可用的情况下模型不够稳健且性能不佳。为了解决这些问题，我们提出了基于组合式泰勒斯采样的联邦稳健剪枝（FedRTS）方法，这是一种新型框架，旨在开发稳健的稀疏模型。FedRTS 通过其基于泰勒斯采样的调整（TSAdj）机制提高了模型的稳健性和性能，该机制使用了由稳定且远视信息指导的概率决策，而不是依赖于不稳定且近视信息的确定性决策。广泛的实验表明，FedRTS 在计算机视觉和自然语言处理任务中达到了最先进的性能，同时减少了通信成本，特别是在数据分布异质和部分客户端参与的场景中表现尤为出色。我们的代码可在以下链接获取：this https URL 

---
# Principal Components for Neural Network Initialization 

**Title (ZH)**: 用于神经网络初始化的主成分分析 

**Authors**: Nhan Phan, Thu Nguyen, Pål Halvorsen, Michael A. Riegler  

**Link**: [PDF](https://arxiv.org/pdf/2501.19114)  

**Abstract**: Principal Component Analysis (PCA) is a commonly used tool for dimension reduction and denoising. Therefore, it is also widely used on the data prior to training a neural network. However, this approach can complicate the explanation of explainable AI (XAI) methods for the decision of the model. In this work, we analyze the potential issues with this approach and propose Principal Components-based Initialization (PCsInit), a strategy to incorporate PCA into the first layer of a neural network via initialization of the first layer in the network with the principal components, and its two variants PCsInit-Act and PCsInit-Sub. Explanations using these strategies are as direct and straightforward as for neural networks and are simpler than using PCA prior to training a neural network on the principal components. Moreover, as will be illustrated in the experiments, such training strategies can also allow further improvement of training via backpropagation. 

**Abstract (ZH)**: 主成分分析（PCA）是一种常用的降维和去噪工具，因此在训练神经网络之前常被应用于数据预处理。然而，这种方法会使可解释人工智能（XAI）方法的解释变得复杂，因为解释模型决策时会涉及到PCA。在本文中，我们分析了这种方法潜在的问题，并提出了一种名为基于主成分的初始化（PCsInit）的策略，即通过在网络的第一层使用主成分进行初始化，将PCA融入神经网络的第一个层，以及其两种变体PCsInit-Act和PCsInit-Sub。使用这些策略进行解释可以直接且直观，比在训练前对主成分进行PCA预处理更加简便。此外，实验结果还将表明，此类训练策略还可以通过反向传播进一步提高训练效果。 

---
# A Benchmark for Incremental Micro-expression Recognition 

**Title (ZH)**: 增量微表情识别的基准标准 

**Authors**: Zhengqin Lai, Xiaopeng Hong, Yabin Wang, Xiaobai Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.19111)  

**Abstract**: Micro-expression recognition plays a pivotal role in understanding hidden emotions and has applications across various fields. Traditional recognition methods assume access to all training data at once, but real-world scenarios involve continuously evolving data streams. To respond to the requirement of adapting to new data while retaining previously learned knowledge, we introduce the first benchmark specifically designed for incremental micro-expression recognition. Our contributions include: Firstly, we formulate the incremental learning setting tailored for micro-expression recognition. Secondly, we organize sequential datasets with carefully curated learning orders to reflect real-world scenarios. Thirdly, we define two cross-evaluation-based testing protocols, each targeting distinct evaluation objectives. Finally, we provide six baseline methods and their corresponding evaluation results. This benchmark lays the groundwork for advancing incremental micro-expression recognition research. All code used in this study will be made publicly available. 

**Abstract (ZH)**: 微表达识别在理解隐藏情感方面发挥着关键作用，并且在多个领域都有应用。传统识别方法假设可以一次性获得所有训练数据，但在现实世界中，数据流是不断演变的。为应对此需求，即在适应新数据的同时保留先前学习的知识，我们首次引入了一个专门针对增量微表达识别设计的基准。我们的贡献包括：首先，我们为微表达识别制定了增量学习设置。其次，我们组织了顺序数据集，并精心策划了学习顺序以反映现实世界场景。第三，我们定义了两种交叉评估为基础的测试协议，每种协议针对不同的评估目标。最后，我们提供了六种基线方法及其相应的评估结果。该基准为推进增量微表达识别研究奠定了基础。研究中所使用的所有代码将公开提供。 

---
# Fairness Analysis of CLIP-Based Foundation Models for X-Ray Image Classification 

**Title (ZH)**: 基于CLIP的foundation模型在X射线图像分类中的公平性分析 

**Authors**: Xiangyu Sun, Xiaoguang Zou, Yuanquan Wu, Guotai Wang, Shaoting Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19086)  

**Abstract**: X-ray imaging is pivotal in medical diagnostics, offering non-invasive insights into a range of health conditions. Recently, vision-language models, such as the Contrastive Language-Image Pretraining (CLIP) model, have demonstrated potential in improving diagnostic accuracy by leveraging large-scale image-text datasets. However, since CLIP was not initially designed for medical images, several CLIP-like models trained specifically on medical images have been developed. Despite their enhanced performance, issues of fairness - particularly regarding demographic attributes - remain largely unaddressed. In this study, we perform a comprehensive fairness analysis of CLIP-like models applied to X-ray image classification. We assess their performance and fairness across diverse patient demographics and disease categories using zero-shot inference and various fine-tuning techniques, including Linear Probing, Multilayer Perceptron (MLP), Low-Rank Adaptation (LoRA), and full fine-tuning. Our results indicate that while fine-tuning improves model accuracy, fairness concerns persist, highlighting the need for further fairness interventions in these foundational models. 

**Abstract (ZH)**: X射线成像是医学诊断中的关键工具，能够提供对多种健康状况的无创洞察。近期，视觉-语言模型，如对比语言-图像预训练（CLIP）模型，已显示出通过利用大规模图像-文本数据集提高诊断准确性的潜力。然而，由于CLIP最初并不是为医学图像设计的，因此专门针对医学图像训练的类似CLIP的模型已经得到了开发。尽管这些模型的性能有所提升，但关于公平性的问题，尤其是针对人口统计属性的问题，仍然未得到充分解决。在本研究中，我们对应用于X射线图像分类的CLIP-like模型的公平性进行了全面分析。我们使用零样本推断和多种微调技术（包括线性探针、多层感知机（MLP）、低秩适应（LoRA）和全量微调）评估了这些模型在不同患者人口统计学特征和疾病类别中的性能及公平性。研究结果表明，虽然微调能够提高模型的准确性，但公平性问题仍然存在，这强调了对这些基础模型进行进一步公平性干预的必要性。 

---
# Improving vision-language alignment with graph spiking hybrid Networks 

**Title (ZH)**: 使用图刺激混合网络提高视觉语言对齐 

**Authors**: Siyu Zhang, Heming Zheng, Yiming Wu, Yeming Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.19069)  

**Abstract**: To bridge the semantic gap between vision and language (VL), it is necessary to develop a good alignment strategy, which includes handling semantic diversity, abstract representation of visual information, and generalization ability of models. Recent works use detector-based bounding boxes or patches with regular partitions to represent visual semantics. While current paradigms have made strides, they are still insufficient for fully capturing the nuanced contextual relations among various objects. This paper proposes a comprehensive visual semantic representation module, necessitating the utilization of panoptic segmentation to generate coherent fine-grained semantic features. Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that integrates the complementary advantages of Spiking Neural Networks (SNNs) and Graph Attention Networks (GATs) to encode visual semantic information. Intriguingly, the model not only encodes the discrete and continuous latent variables of instances but also adeptly captures both local and global contextual features, thereby significantly enhancing the richness and diversity of semantic representations. Leveraging the spatiotemporal properties inherent in SNNs, we employ contrastive learning (CL) to enhance the similarity-based representation of embeddings. This strategy alleviates the computational overhead of the model and enriches meaningful visual representations by constructing positive and negative sample pairs. We design an innovative pre-training method, Spiked Text Learning (STL), which uses text features to improve the encoding ability of discrete semantics. Experiments show that the proposed GSHN exhibits promising results on multiple VL downstream tasks. 

**Abstract (ZH)**: 为了弥合视觉（Vision）与语言（Language）之间的语义差距（VL），需要开发一种良好的对齐策略，该策略包括处理语义多样性、视觉信息的抽象表示以及模型的泛化能力。最近的研究使用基于检测器的边界框或具有规则分割的补丁来表示视觉语义。尽管当前框架已经取得了一定的进展，但它们仍然不足以充分捕捉到不同物体之间细腻的上下文关系。本文提出了一种综合性的视觉语义表示模块，需要利用全景分割生成连贯的细粒度语义特征。此外，我们提出了一个新的图放电混合网络（Graph Spiking Hybrid Network，GSHN），该网络结合了放电神经网络（Spiking Neural Networks，SNNs）和图注意力网络（Graph Attention Networks，GATs）的优点来编码视觉语义信息。令人感兴趣的是，该模型不仅编码实例的离散和连续潜在变量，还能有效地捕捉局部和全局上下文特征，从而显著增强了语义表示的丰富性和多样性。利用SNNs固有的时空特性，我们采用了对比学习（Contrastive Learning，CL）来增强基于相似性的表示能力。该策略减轻了模型的计算负担，并通过构建正样本和负样本对来丰富有意义的视觉表示。我们设计了一种创新的预训练方法——放电文本学习（Spiked Text Learning，STL），该方法使用文本特征来提高离散语义的编码能力。实验结果表明，提出的方法在多种VL下游任务中表现出色。 

---
# BEAT: Balanced Frequency Adaptive Tuning for Long-Term Time-Series Forecasting 

**Title (ZH)**: BEAT：平衡频率自适应调整方法用于长期时间序列预测 

**Authors**: Zhixuan Li, Naipeng Chen, Seonghwa Choi, Sanghoon Lee, Weisi Lin  

**Link**: [PDF](https://arxiv.org/pdf/2501.19065)  

**Abstract**: Time-series forecasting is crucial for numerous real-world applications including weather prediction and financial market modeling. While temporal-domain methods remain prevalent, frequency-domain approaches can effectively capture multi-scale periodic patterns, reduce sequence dependencies, and naturally denoise signals. However, existing approaches typically train model components for all frequencies under a unified training objective, often leading to mismatched learning speeds: high-frequency components converge faster and risk overfitting, while low-frequency components underfit due to insufficient training time. To deal with this challenge, we propose BEAT (Balanced frEquency Adaptive Tuning), a novel framework that dynamically monitors the training status for each frequency and adaptively adjusts their gradient updates. By recognizing convergence, overfitting, or underfitting for each frequency, BEAT dynamically reallocates learning priorities, moderating gradients for rapid learners and increasing those for slower ones, alleviating the tension between competing objectives across frequencies and synchronizing the overall learning process. Extensive experiments on seven real-world datasets demonstrate that BEAT consistently outperforms state-of-the-art approaches. 

**Abstract (ZH)**: 时间序列预测对于许多实际应用至关重要，包括天气预报和金融市场建模。尽管时域方法仍然占主导地位，但频域方法可以有效捕捉多尺度周期模式、减少序列依赖性，并自然去除信号噪声。然而，现有方法通常在统一的目标函数下对所有频率进行模型组件训练，这往往会导致学习速率不匹配的问题：高频分量更快收敛，存在过拟合的风险，而低频分量由于训练时间不足而容易欠拟合。为应对这一挑战，我们提出了一种名为BEAT（Balanced frEquency Adaptive Tuning）的新框架，该框架能够动态监测每个频率的训练状态，并自适应地调整其梯度更新。BEAT通过识别每个频率的收敛、过拟合或欠拟合状态，动态重新分配学习优先级，对快速学习者适度调整梯度，对慢速学习者增加梯度，从而缓解不同频率之间竞争目标之间的张力，并同步整个训练过程。在七个实际数据集上的广泛实验表明，BEAT在性能上始终优于现有最先进的方法。 

---
# Enabling Autonomic Microservice Management through Self-Learning Agents 

**Title (ZH)**: 通过自学习代理实现自主微服务管理 

**Authors**: Fenglin Yu, Fangkai Yang, Xiaoting Qin, Zhiyang Zhang, Jue Zhang, Qingwei Lin, Hongyu Zhang, Yingnong Dang, Saravan Rajmohan, Dongmei Zhang, Qi Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.19056)  

**Abstract**: The increasing complexity of modern software systems necessitates robust autonomic self-management capabilities. While Large Language Models (LLMs) demonstrate potential in this domain, they often face challenges in adapting their general knowledge to specific service contexts. To address this limitation, we propose ServiceOdyssey, a self-learning agent system that autonomously manages microservices without requiring prior knowledge of service-specific configurations. By leveraging curriculum learning principles and iterative exploration, ServiceOdyssey progressively develops a deep understanding of operational environments, reducing dependence on human input or static documentation. A prototype built with the Sock Shop microservice demonstrates the potential of this approach for autonomic microservice management. 

**Abstract (ZH)**: 现代软件系统的日益复杂性需要强大的自管理能力。尽管大型语言模型（LLMs）在这一领域展现出潜力，但在将其一般知识适应特定服务环境时，它们通常会面临挑战。为解决这一限制，我们提出了一种自学习代理系统——ServiceOdyssey，该系统能够自主管理微服务，而无需了解特定服务配置的知识。通过利用课程学习原理和迭代探索，ServiceOdyssey逐步加深对运营环境的理解，从而减少对人类输入或静态文档的依赖。基于Sock Shop微服务构建的原型演示了这种方法在自主微服务管理和配置方面的潜力。 

---
# Towards Physiologically Sensible Predictions via the Rule-based Reinforcement Learning Layer 

**Title (ZH)**: 基于规则的强化学习层实现生理上合理的预测 

**Authors**: Lingwei Zhu, Zheng Chen, Yukie Nagai, Jimeng Sun  

**Link**: [PDF](https://arxiv.org/pdf/2501.19055)  

**Abstract**: This paper adds to the growing literature of reinforcement learning (RL) for healthcare by proposing a novel paradigm: augmenting any predictor with Rule-based RL Layer (RRLL) that corrects the model's physiologically impossible predictions. Specifically, RRLL takes as input states predicted labels and outputs corrected labels as actions. The reward of the state-action pair is evaluated by a set of general rules. RRLL is efficient, general and lightweight: it does not require heavy expert knowledge like prior work but only a set of impossible transitions. This set is much smaller than all possible transitions; yet it can effectively reduce physiologically impossible mistakes made by the state-of-the-art predictor models. We verify the utility of RRLL on a variety of important healthcare classification problems and observe significant improvements using the same setup, with only the domain-specific set of impossibility changed. In-depth analysis shows that RRLL indeed improves accuracy by effectively reducing the presence of physiologically impossible predictions. 

**Abstract (ZH)**: 本文通过对医疗保健领域强化学习（RL）的研究贡献，提出了一种新颖的范式：将基于规则的强化学习层（RRLL）添加到任何预测器中，以纠正模型的生理上不可能的预测结果。具体而言，RRLL 将预测状态标签作为输入，并输出修正后的标签作为动作。状态-动作对的奖励由一套通用规则进行评估。RRLL 具有效率高、通用性强且轻量化的特性：它不需要以往工作所需的大量专家知识，而只需要一套不可能发生的转换状态集。虽然这一集合并未包含所有可能的转换状态，但仍然能够有效减少最先进的预测模型所犯的生理上不可能的错误。我们通过在多种关键的医疗保健分类问题上验证 RRLL 的效用，并在保持相同实验设置的情况下观察到显著改进，仅改变特定领域的不可能性状态集。深入分析表明，RRLL 通过有效减少生理上不可能的预测结果的存在而真正提高了准确性。 

---
# Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE) 

**Title (ZH)**: 模型校准的理解 -- 一种温和的介绍与校准的可视化探究，以及预期校准误差（ECE）的探讨 

**Authors**: Maja Pavlovic  

**Link**: [PDF](https://arxiv.org/pdf/2501.19047)  

**Abstract**: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration. 

**Abstract (ZH)**: 为确保模型可靠，必须对其进行校准，使其在每个决策上的信心程度接近其实际结果。在这篇博文中，我们将探讨最常用的校准定义，然后深入介绍一个常用的模型校准评估指标。随后，我们将讨论这种指标的一些局限性，这些局限性揭示了对现有校准概念的额外需求，而这些新的校准概念需要新的评估指标。本文并不旨在全面剖析所有关于校准的研究工作，也不专注于如何校准模型。相反，本文旨在为不同的校准概念及其评估指标提供一个温和的介绍，并重新强调一种仍然广泛用于评估校准的标准指标的一些问题。 

---
# Swarm-Gen: Fast Generation of Diverse Feasible Swarm Behaviors 

**Title (ZH)**: Swarm-Gen：快速生成多样可行的 swarm 行为 

**Authors**: Simon Idoko, B.Bhanu Teja, K.Madhava Krishna, Arun Kumar Singh  

**Link**: [PDF](https://arxiv.org/pdf/2501.19042)  

**Abstract**: Coordination behavior in robot swarms is inherently multi-modal in nature. That is, there are numerous ways in which a swarm of robots can avoid inter-agent collisions and reach their respective goals. However, the problem of generating diverse and feasible swarm behaviors in a scalable manner remains largely unaddressed. In this paper, we fill this gap by combining generative models with a safety-filter (SF). Specifically, we sample diverse trajectories from a learned generative model which is subsequently projected onto the feasible set using the SF. We experiment with two choices for generative models, namely: Conditional Variational Autoencoder (CVAE) and Vector-Quantized Variational Autoencoder (VQ-VAE). We highlight the trade-offs these two models provide in terms of computation time and trajectory diversity. We develop a custom solver for our SF and equip it with a neural network that predicts context-specific initialization. Thecinitialization network is trained in a self-supervised manner, taking advantage of the differentiability of the SF solver. We provide two sets of empirical results. First, we demonstrate that we can generate a large set of multi-modal, feasible trajectories, simulating diverse swarm behaviors, within a few tens of milliseconds. Second, we show that our initialization network provides faster convergence of our SF solver vis-a-vis other alternative heuristics. 

**Abstract (ZH)**: 机器人群体的协调行为本质上是多模态的。也就是说，一群机器人可以通过多种方式避免相互碰撞并达到各自的目标。然而，以可扩展的方式生成多样化且可行的群体行为问题仍然 largely未得到解决。在本论文中，我们通过结合生成模型和安全过滤器（SF）来填补这一空白。具体来说，我们从学习到的生成模型中采样多样化的轨迹，随后通过SF将其投影到可行集上。我们尝试了两种生成模型的选择，分别是条件变分自编码器（CVAE）和向量量化变分自编码器（VQ-VAE）。我们讨论了这两种模型在计算时间和轨迹多样性方面的权衡。我们为我们的SF开发了一个自定义求解器，并为此配备了可以预测上下文特定初始化的神经网络。初始化网络是自监督训练的，利用了SF求解器的可微性优势。我们提供了一组实验证据。首先，我们证明了我们可以在几毫秒内生成大量多模态、可行的轨迹，模拟多样化的群体行为。其次，我们展示了我们的初始化网络能够相较于其他替代启发法更快地加速我们的SF求解器的收敛。 

---
# Virtual airways heatmaps to optimize point of entry location in lung biopsy planning systems 

**Title (ZH)**: 虚拟气道热图以优化肺活检规划系统中的入口位置 

**Authors**: Debora Gil, Pere Lloret, Marta Diez-Ferrer, Carles Sanchez  

**Link**: [PDF](https://arxiv.org/pdf/2501.19003)  

**Abstract**: Purpose: We present a virtual model to optimize point of entry (POE) in lung biopsy planning systems. Our model allows to compute the quality of a biopsy sample taken from potential POE, taking into account the margin of error that arises from discrepancies between the orientation in the planning simulation and the actual orientation during the operation. Additionally, the study examines the impact of the characteristics of the lesion. Methods: The quality of the biopsy is given by a heatmap projected onto the skeleton of a patient-specific model of airways. The skeleton provides a 3D representation of airways structure, while the heatmap intensity represents the potential amount of tissue that it could be extracted from each POE. This amount of tissue is determined by the intersection of the lesion with a cone that represents the uncertainty area in the introduction of biopsy instruments. The cone, lesion, and skeleton are modelled as graphical objects that define a 3D scene of the intervention. Results: We have simulated different settings of the intervention scene from a single anatomy extracted from a CT scan and two lesions with regular and irregular shapes. The different scenarios are simulated by systematic rotation of each lesion placed at different distances from airways. Analysis of the heatmaps for the different settings show a strong impact of lesion orientation for irregular shape and the distance for both shapes. Conclusion: The proposed heatmaps help to visually assess the optimal POE and identify whether multiple optimal POEs exist in different zones of the bronchi. They also allow us to model the maximum allowable error in navigation systems and study which variables have the greatest influence on the success of the operation. Additionally, they help determine at what point this influence could potentially jeopardize the operation. 

**Abstract (ZH)**: 目的：我们提出了一种虚拟模型，用于优化肺活检计划系统中的穿刺点（Point of Entry, POE）。该模型允许计算从潜在的POE获取的活检样本的质量，同时考虑了规划仿真中方向与实际操作中方向之间的偏差所引起的误差范围。此外，本研究还探讨了病灶特征的影响。方法：活检的质量由投影在患者特异性气道模型骨架上的热图表示。该骨架提供了气道结构的3D表示，而热图强度则代表从每个POE可以提取的组织量。组织量通过病变与代表穿刺器械引入不确定区域的圆锥体的交集来确定。圆锥体、病变和骨架作为图形对象定义了介入场景的3D场景。结果：我们从一次CT扫描提取的单一解剖结构中模拟了不同场景设置，并针对两种具有规则和不规则形状的病变进行建模。通过系统旋转放置在不同气道距离处的病变，模拟了各种情况。不同设置的热图分析显示，不规则形状的病变方向对热图有强烈影响，两种形状的病变距离都对热图有影响。结论：所提出的热图有助于直观评估最优穿刺点并确定是否在不同支气管区域存在多个最优穿刺点。它们还允许我们建模导航系统的最大允许误差，并研究哪些变量对操作的成功影响最大。此外，它们还帮助确定在这个影响可能危及操作之前，这种情况在哪个点上会发生。 

---
# Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings 

**Title (ZH)**: 基于嵌入表示和Token概率的方法对AI生成文本检测模型的对抗攻击：一种策略分析 

**Authors**: Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo  

**Link**: [PDF](https://arxiv.org/pdf/2501.18998)  

**Abstract**: In recent years, text generation tools utilizing Artificial Intelligence (AI) have occasionally been misused across various domains, such as generating student reports or creative writings. This issue prompts plagiarism detection services to enhance their capabilities in identifying AI-generated content. Adversarial attacks are often used to test the robustness of AI-text generated detectors. This work proposes a novel textual adversarial attack on the detection models such as Fast-DetectGPT. The method employs embedding models for data perturbation, aiming at reconstructing the AI generated texts to reduce the likelihood of detection of the true origin of the texts. Specifically, we employ different embedding techniques, including the Tsetlin Machine (TM), an interpretable approach in machine learning for this purpose. By combining synonyms and embedding similarity vectors, we demonstrates the state-of-the-art reduction in detection scores against Fast-DetectGPT. Particularly, in the XSum dataset, the detection score decreased from 0.4431 to 0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC. 

**Abstract (ZH)**: 近年来，利用人工智能（AI）进行文本生成的工具偶尔被误用于各个领域，如生成学生报告或创意写作等。这一问题促使查重检测服务提高识别AI生成内容的能力。对抗攻击通常被用来测试AI生成文本检测器的鲁棒性。本文提出了一种针对检测模型（如Fast-DetectGPT）的新颖文本对抗攻击方法。该方法利用嵌入模型对数据进行扰动，旨在通过重构AI生成的文本以降低其被检测到真实来源的可能性。具体来说，我们采用不同的嵌入技术，包括Tsetlin机（TM），这是一种可解释的机器学习方法，用于此目的。通过结合同义词和嵌入相似向量，我们证明了该方法在对抗Fast-DetectGPT方面的检测分数表现出最先进的降低效果。特别地，在XSum数据集中，检测分数从0.4431降至0.2744的AUROC；在SQuAD数据集中，检测分数则从0.5068降至0.3532的AUROC。 

---
# VKFPos: A Learning-Based Monocular Positioning with Variational Bayesian Extended Kalman Filter Integration 

**Title (ZH)**: VKFPos：基于学习的变分贝叶斯扩展卡尔曼滤波集成的单目定位方法 

**Authors**: Jian-Yu Chen, Yi-Ru Chen, Yin-Qiao Chang, Che-Ming Li, Jann-Long Chern, Chih-Wei Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18994)  

**Abstract**: This paper addresses the challenges in learning-based monocular positioning by proposing VKFPos, a novel approach that integrates Absolute Pose Regression (APR) and Relative Pose Regression (RPR) via an Extended Kalman Filter (EKF) within a variational Bayesian inference framework. Our method shows that the essential posterior probability of the monocular positioning problem can be decomposed into APR and RPR components. This decomposition is embedded in the deep learning model by predicting covariances in both APR and RPR branches, allowing them to account for associated uncertainties. These covariances enhance the loss functions and facilitate EKF integration. Experimental evaluations on both indoor and outdoor datasets show that the single-shot APR branch achieves accuracy on par with state-of-the-art methods. Furthermore, for temporal positioning, where consecutive images allow for RPR and EKF integration, VKFPos outperforms temporal APR and model-based integration methods, achieving superior accuracy. 

**Abstract (ZH)**: 本文通过提出一种新型方法VKFPos，解决了基于学习的单目定位中的挑战。VKFPos将绝对位姿回归（APR）和相对位姿回归（RPR）通过扩展卡尔曼滤波器（EKF）整合在变分贝叶斯推断框架内。我们的方法表明，单目定位问题的基本后验概率可以分解为APR和RPR组件。这种分解通过预测APR和RPR分支中的协方差在深度学习模型中嵌入，使它们能够考虑到相应的不确定性。这些协方差增强了损失函数并促进了EKF的整合。在室内和室外数据集上的实验评估表明，单次APR分支的精度与最新方法相当。此外，在需要连续图像以进行RPR和EKF整合的时序定位中，VKFPos优于时序APR和基于模型的整合方法，实现了更高的精度。 

---
# Symmetric Pruning of Large Language Models 

**Title (ZH)**: 大型语言模型的对称剪枝 

**Authors**: Kai Yi, Peter Richtárik  

**Link**: [PDF](https://arxiv.org/pdf/2501.18980)  

**Abstract**: Popular post-training pruning methods such as Wanda and RIA are known for their simple, yet effective, designs that have shown exceptional empirical performance. Wanda optimizes performance through calibrated activations during pruning, while RIA emphasizes the relative, rather than absolute, importance of weight elements. Despite their practical success, a thorough theoretical foundation explaining these outcomes has been lacking. This paper introduces new theoretical insights that redefine the standard minimization objective for pruning, offering a deeper understanding of the factors contributing to their success. Our study extends beyond these insights by proposing complementary strategies that consider both input activations and weight significance. We validate these approaches through rigorous experiments, demonstrating substantial enhancements over existing methods. Furthermore, we introduce a novel training-free fine-tuning approach $R^2$-DSnoT that incorporates relative weight importance and a regularized decision boundary within a dynamic pruning-and-growing framework, significantly outperforming strong baselines and establishing a new state of the art. 

**Abstract (ZH)**: 流行的数据后处理剪枝方法（如Wanda和RIA）以其简洁且有效的设计而闻名，这些方法在实践中表现出色。Wanda通过裁剪过程中的校准激活优化性能，而RIA则强调权重元素的相对重要性，而非绝对重要性。尽管它们在实际应用中取得了成功，但缺乏一个能够充分解释这些结果的理论基础。本文引入了新的理论见解，重新定义了剪枝的标准最小化目标，深入了解其成功背后的因素。我们的研究不仅限于这些见解，还提出了结合输入激活和权重重要性的互补策略。我们通过严格的实验验证了这些方法，显示出相对于现有方法的显著改进。此外，我们提出了一种新的无训练微调方法$R^2$-DSnoT，它将相对权重重要性和正则化决策边界纳入动态剪枝和生长框架中，显著超越了强大的基线方法，并建立了新的性能标准。 

---
# GPO-VAE: Modeling Explainable Gene Perturbation Responses utilizing GRN-Aligned Parameter Optimization 

**Title (ZH)**: GPO-VAE：利用GRN对齐参数优化建模可解释的基因扰动响应 

**Authors**: Seungheun Baek, Soyon Park, Yan Ting Chok, Mogan Gim, Jaewoo Kang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18973)  

**Abstract**: Motivation: Predicting cellular responses to genetic perturbations is essential for understanding biological systems and developing targeted therapeutic strategies. While variational autoencoders (VAEs) have shown promise in modeling perturbation responses, their limited explainability poses a significant challenge, as the learned features often lack clear biological meaning. Nevertheless, model explainability is one of the most important aspects in the realm of biological AI. One of the most effective ways to achieve explainability is incorporating the concept of gene regulatory networks (GRNs) in designing deep learning models such as VAEs. GRNs elicit the underlying causal relationships between genes and are capable of explaining the transcriptional responses caused by genetic perturbation treatments. Results: We propose GPO-VAE, an explainable VAE enhanced by GRN-aligned Parameter Optimization that explicitly models gene regulatory networks in the latent space. Our key approach is to optimize the learnable parameters related to latent perturbation effects towards GRN-aligned explainability. Experimental results on perturbation prediction show our model achieves state-of-the-art performance in predicting transcriptional responses across multiple benchmark datasets. Furthermore, additional results on evaluating the GRN inference task reveal our model's ability to generate meaningful GRNs compared to other methods. According to qualitative analysis, GPO-VAE posseses the ability to construct biologically explainable GRNs that align with experimentally validated regulatory pathways. GPO-VAE is available at this https URL 

**Abstract (ZH)**: 动机：预测遗传扰动的细胞反应对于理解生物系统和开发靶向治疗策略至关重要。虽然变分自编码器（VAEs）在建模扰动反应方面显示出 promise，但它们的可解释性有限，因为学习到的特征往往缺乏明确的生物学意义。然而，在生物人工智能领域，模型的可解释性是最重要的方面之一。实现可解释性的最有效方法之一是在设计如VAE等深度学习模型时引入基因调控网络（GRNs）的概念。GRNs揭示了基因之间的内在因果关系，能够解释由遗传扰动处理引起的转录响应。

结果：我们提出了一种名为GPO-VAE的解释性VAE，该模型通过GRN对齐参数优化增强了解释性，明确地在潜在空间中建模基因调控网络。我们的主要方法是优化与潜在扰动效应相关的可学习参数，使其倾向于GRN对齐的可解释性。在扰动预测实验中，我们的模型在多个基准数据集上实现了最先进的转录响应预测性能。此外，在评估GRN推理任务时，我们模型的能力生成有意义的GRN优于其他方法得到了验证。根据定性分析，GPO-VAE具备构建符合实验验证调控途径的生物可解释的GRNs的能力。请参阅以下链接了解GPO-VAE：[这里](https://your-link-url.com) 

---
# Enhancing Neural Function Approximation: The XNet Outperforming KAN 

**Title (ZH)**: 提升神经网络函数逼近性能：XNet超越KAN 

**Authors**: Xin Li, Xiaotao Zheng, Zhihong Xia  

**Link**: [PDF](https://arxiv.org/pdf/2501.18959)  

**Abstract**: XNet is a single-layer neural network architecture that leverages Cauchy integral-based activation functions for high-order function approximation. Through theoretical analysis, we show that the Cauchy activation functions used in XNet can achieve arbitrary-order polynomial convergence, fundamentally outperforming traditional MLPs and Kolmogorov-Arnold Networks (KANs) that rely on increased depth or B-spline activations. Our extensive experiments on function approximation, PDE solving, and reinforcement learning demonstrate XNet's superior performance - reducing approximation error by up to 50000 times and accelerating training by up to 10 times compared to existing approaches. These results establish XNet as a highly efficient architecture for both scientific computing and AI applications. 

**Abstract (ZH)**: XNet 是一种单层神经网络架构，利用柯西积分为基础的激活函数实现高阶函数逼近。通过理论分析，我们证明了XNet 中使用的柯西激活函数能够实现任意阶多项式收敛，从根本上优于依赖于增加深度或B-样条激活的传统多层感知机 (MLP) 和柯尔莫哥洛夫-阿诺尔德网络 (KAN)。我们在函数逼近、偏微分方程求解和强化学习方面的广泛实验表明，XNet 具有显著的优越性能——与现有方法相比，其逼近误差最多可减少50000倍，训练速度最多可加快10倍。这些结果确立了XNet 是一种在科学计算和人工智能应用中都非常高效的架构。 

---
# Deep Learning based Quasi-consciousness Training for Robot Intelligent Model 

**Title (ZH)**: 基于深度学习的类意识训练方法研究：用于机器人智能模型 

**Authors**: Yuchun Li, Fang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18955)  

**Abstract**: This paper explores a deep learning based robot intelligent model that renders robots learn and reason for complex tasks. First, by constructing a network of environmental factor matrix to stimulate the learning process of the robot intelligent model, the model parameters must be subjected to coarse & fine tuning to optimize the loss function for minimizing the loss score, meanwhile robot intelligent model can fuse all previously known concepts together to represent things never experienced before, which need robot intelligent model can be generalized extensively. Secondly, in order to progressively develop a robot intelligent model with primary consciousness, every robot must be subjected to at least 1~3 years of special school for training anthropomorphic behaviour patterns to understand and process complex environmental information and make rational decisions. This work explores and delivers the potential application of deep learning-based quasi-consciousness training in the field of robot intelligent model. 

**Abstract (ZH)**: 本文探讨了一种基于深度学习的机器人智能模型，该模型使机器人能够学习和推理以应对复杂的任务。首先，通过构建环境因素矩阵网络来模拟机器人智能模型的学习过程，模型参数需要进行粗调和细调，以优化损失函数并最小化损失分数。同时，机器人智能模型可以融合所有已知概念，以表示从未经历过的新型事物，这就要求机器人智能模型具有广泛的应用能力。其次，为了逐步构建具有初级意识的机器人智能模型，每台机器人必须接受至少1至3年的特殊学校训练，以学习和发展拟人行为模式，理解和处理复杂环境信息，并做出合理的决策。本文研究并展示了基于深度学习的拟意识训练在机器人智能模型领域的潜在应用。 

---
# Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them 

**Title (ZH)**: 扩散模型中概念抹除的奇妙目标及其来源 

**Authors**: Anh Bui, Trang Vu, Long Vuong, Trung Le, Paul Montague, Tamas Abraham, Junae Kim, Dinh Phung  

**Link**: [PDF](https://arxiv.org/pdf/2501.18950)  

**Abstract**: Concept erasure has emerged as a promising technique for mitigating the risk of harmful content generation in diffusion models by selectively unlearning undesirable concepts. The common principle of previous works to remove a specific concept is to map it to a fixed generic concept, such as a neutral concept or just an empty text prompt. In this paper, we demonstrate that this fixed-target strategy is suboptimal, as it fails to account for the impact of erasing one concept on the others. To address this limitation, we model the concept space as a graph and empirically analyze the effects of erasing one concept on the remaining concepts. Our analysis uncovers intriguing geometric properties of the concept space, where the influence of erasing a concept is confined to a local region. Building on this insight, we propose the Adaptive Guided Erasure (AGE) method, which \emph{dynamically} selects optimal target concepts tailored to each undesirable concept, minimizing unintended side effects. Experimental results show that AGE significantly outperforms state-of-the-art erasure methods on preserving unrelated concepts while maintaining effective erasure performance. Our code is published at {this https URL}. 

**Abstract (ZH)**: 概念擦除已经作为一种有前景的技术，用于通过选择性地忘却不希望的概念来减轻扩散模型生成有害内容的风险。先前作品中删除特定概念的常见方法是将其映射到固定的标准概念，例如中性概念或一个空白文本提示。在本文中，我们证明这种固定目标策略是次优的，因为它未能考虑到擦除一个概念对其他概念的影响。为了弥补这一局限，我们将概念空间建模为图，并实证分析擦除一个概念对剩余概念的影响。我们的分析揭示了概念空间中有趣的几何特性，即擦除一个概念的影响仅限于局部区域。基于这一见解，我们提出了自适应引导擦除（AGE）方法，该方法根据每个不希望的概念动态选择优化的目标概念，从而最小化意外副作用。实验结果显示，AGE在保持无关概念的同时，显著优于最先进的擦除方法，在擦除性能方面表现出色。我们的代码已发布在 {this https URL}。 

---
# KBQA-o1: Agentic Knowledge Base Question Answering with Monte Carlo Tree Search 

**Title (ZH)**: KBQA-o1: 基于蒙特卡罗树搜索的知识库问答 

**Authors**: Haoran Luo, Haihong E, Yikai Guo, Qika Lin, Xiaobao Wu, Xinyu Mu, Wenhao Liu, Meina Song, Yifan Zhu, Luu Anh Tuan  

**Link**: [PDF](https://arxiv.org/pdf/2501.18922)  

**Abstract**: Knowledge Base Question Answering (KBQA) aims to answer natural language questions with a large-scale structured knowledge base (KB). Despite advancements with large language models (LLMs), KBQA still faces challenges in weak KB awareness, imbalance between effectiveness and efficiency, and high reliance on annotated data. To address these challenges, we propose KBQA-o1, a novel agentic KBQA method with Monte Carlo Tree Search (MCTS). It introduces a ReAct-based agent process for stepwise logical form generation with KB environment exploration. Moreover, it employs MCTS, a heuristic search method driven by policy and reward models, to balance agentic exploration's performance and search space. With heuristic exploration, KBQA-o1 generates high-quality annotations for further improvement by incremental fine-tuning. Experimental results show that KBQA-o1 outperforms previous low-resource KBQA methods with limited annotated data, boosting Llama-3.1-8B model's GrailQA F1 performance to 78.5% compared to 48.5% of the previous sota method with GPT-3.5-turbo. 

**Abstract (ZH)**: 知识库问答（KBQA）旨在使用大规模结构化知识库（KB）回答自然语言问题。尽管大型语言模型（LLMs）取得了进展，但KBQA仍然面临着知识库意识薄弱、效果与效率不平衡以及高度依赖标注数据的挑战。为应对这些挑战，我们提出了一种名为KBQA-o1的新型基于代理的KBQA方法，该方法结合了蒙特卡洛树搜索（MCTS）。它引入了一种基于ReAct的代理过程，用于逐步逻辑形式生成，并通过探索知识库环境实现。此外，它采用由策略和奖励模型驱动的MCTS进行启发式搜索，以平衡代理探索的性能和搜索空间。借助启发式探索，KBQA-o1生成高质量的标注，进一步通过增量微调进行改进。实验结果表明，相比于以前依赖少量标注数据的KBQA方法，KBQA-o1表现出色，使得Llama-3.1-8B模型在GrailQA F1指标上的性能提升至78.5%，而先前最先进的方法（使用GPT-3.5-turbo）这一指标仅为48.5%。 

---
# Deepfake Detection of Singing Voices With Whisper Encodings 

**Title (ZH)**: 使用Whisper编码进行唱歌声音的Deepfake检测 

**Authors**: Falguni Sharma, Priyanka Gupta  

**Link**: [PDF](https://arxiv.org/pdf/2501.18919)  

**Abstract**: The deepfake generation of singing vocals is a concerning issue for artists in the music industry. In this work, we propose a singing voice deepfake detection (SVDD) system, which uses noise-variant encodings of open-AI's Whisper model. As counter-intuitive as it may sound, even though the Whisper model is known to be noise-robust, the encodings are rich in non-speech information, and are noise-variant. This leads us to evaluate Whisper encodings as feature representations for the SVDD task. Therefore, in this work, the SVDD task is performed on vocals and mixtures, and the performance is evaluated in \%EER over varying Whisper model sizes and two classifiers- CNN and ResNet34, under different testing conditions. 

**Abstract (ZH)**: 音乐行业内的人工智能仿声技术生成是一个令人担忧的问题。本文提出了一种歌唱声音深度伪造检测（SVDD）系统，该系统利用了OpenAI的Whisper模型的噪声变异编码。尽管Whisper模型以其对噪声的鲁棒性而闻名，其编码却富含非语音信息，并且是噪声变异的。这促使我们将其视为SVDD任务的特征表示。因此，在本文中，SVDD任务被应用于人声和混合信号，并在不同Whisper模型大小和两种分类器（CNN和ResNet34）条件下，通过%EER（错误接受率）评估其性能。 

---
# Lightspeed Geometric Dataset Distance via Sliced Optimal Transport 

**Title (ZH)**: 光速几何数据集距离通过切片最优传输 

**Authors**: Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho  

**Link**: [PDF](https://arxiv.org/pdf/2501.18901)  

**Abstract**: We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation. 

**Abstract (ZH)**: 我们引入了切片最优传输数据集距离 (s-OTDD)，这是一种模型无关、嵌入无关的数据集比较方法，不需要训练，对类数的变化具有鲁棒性，能够处理分离的标签集。核心创新是矩变换投影 (MTP)，它可以将标签（表示为特征的分布）映射到一个实数。利用 MTP，我们推导出一种数据点投影方法，能够将数据集转换为一维分布。s-OTDD 定义为投影分布之间的期望 Wasserstein 距离，相对于随机投影参数。通过利用一维最优传输的闭式解，s-OTDD 实现了近线性的计算复杂度，与数据点数量和特征维度无关，并且与类数无关。凭借其几何上有意义的投影，s-OTDD 与最优传输数据集距离高度相关，同时在计算效率上优于现有的数据集差异性度量。此外，s-OTDD 与迁移学习中的性能差距以及数据增强中的分类准确率具有良好的相关性。 

---
# Building Bridges, Not Walls -- Advancing Interpretability by Unifying Feature, Data, and Model Component Attribution 

**Title (ZH)**: 构建桥梁而非高墙——通过统一特征、数据和模型组件的归因推动可解释性的发展 

**Authors**: Shichang Zhang, Tessa Han, Usha Bhalla, Hima Lakkaraju  

**Link**: [PDF](https://arxiv.org/pdf/2501.18887)  

**Abstract**: The increasing complexity of AI systems has made understanding their behavior a critical challenge. Numerous methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of approaches and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and bridging them can benefit interpretability research. We conduct a detailed analysis of successful methods across three domains and present a unified view to demonstrate that these seemingly distinct methods employ similar approaches, such as perturbations, gradients, and linear approximations, differing primarily in their perspectives rather than core techniques. Our unified perspective enhances understanding of existing attribution methods, identifies shared concepts and challenges, makes this field more accessible to newcomers, and highlights new directions not only for attribution and interpretability but also for broader AI research, including model editing, steering, and regulation. 

**Abstract (ZH)**: 随着AI系统的日益复杂，理解和解释其行为已成为一个关键挑战。已经开发出多种方法来将模型的行为归因于三个关键方面：输入特征、训练数据和内部模型组件。然而，这些归因方法的研究和应用相对独立，导致方法和术语呈现出碎片化的局面。本文认为，特征、数据和组件的归因方法在根本上存在相似之处，将它们联系起来可以促进解释性研究。我们对这三个领域中成功的方法进行了详细的分析，并提出了一种统一的观点，表明这些看似不同的方法在使用扰动、梯度和线性近似等方面采取了相似的方法，主要差异在于视角而非核心技术。我们统一的观点增进了对现有归因方法的理解，识别了共同的概念和挑战，使这一领域更加易于新入学者理解，并突出了新的发展方向，不仅限于归因和解释性研究，还包括更广泛的AI研究，如模型编辑、引导和监管。 

---
# UP-VLA: A Unified Understanding and Prediction Model for Embodied Agent 

**Title (ZH)**: UP-VLA：统一的具身智能体理解与预测模型 

**Authors**: Jianke Zhang, Yanjiang Guo, Yucheng Hu, Xiaoyu Chen, Xiang Zhu, Jianyu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.18867)  

**Abstract**: Recent advancements in Vision-Language-Action (VLA) models have leveraged pre-trained Vision-Language Models (VLMs) to improve the generalization capabilities. VLMs, typically pre-trained on vision-language understanding tasks, provide rich semantic knowledge and reasoning abilities. However, prior research has shown that VLMs often focus on high-level semantic content and neglect low-level features, limiting their ability to capture detailed spatial information and understand physical dynamics. These aspects, which are crucial for embodied control tasks, remain underexplored in existing pre-training paradigms. In this paper, we investigate the training paradigm for VLAs, and introduce \textbf{UP-VLA}, a \textbf{U}nified VLA model training with both multi-modal \textbf{U}nderstanding and future \textbf{P}rediction objectives, enhancing both high-level semantic comprehension and low-level spatial understanding. Experimental results show that UP-VLA achieves a 33% improvement on the Calvin ABC-D benchmark compared to the previous state-of-the-art method. Additionally, UP-VLA demonstrates improved success rates in real-world manipulation tasks, particularly those requiring precise spatial information. 

**Abstract (ZH)**: 近年来，视觉-语言-动作（VLA）模型的进步利用预训练的视觉-语言模型（VLMs）来提高泛化能力。VLMs通常在视觉-语言理解任务上进行预训练，提供丰富的语义知识和推理能力。然而，先前的研究表明，VLMs往往专注于高层次的语义内容，而忽略了低级特征，这限制了它们捕获详细的空间信息和理解物理动态的能力。这些方面对于实体控制任务至关重要，在现有的预训练范式中仍然没有得到充分探索。本文探讨了VLA的训练范式，并引入了**UP-VLA**模型，该模型结合了多模态**U**nderstanding和未来**P**rediction的目标，从而增强高层次语义理解和低层次空间理解。实验结果表明，UP-VLA在Calvin ABC-D基准测试上的表现比之前最先进的方法提高了33%。此外，UP-VLA在现实世界的操作任务中表现出更高的成功率，特别是在需要精确空间信息的任务中。 

---
# REG: Rectified Gradient Guidance for Conditional Diffusion Models 

**Title (ZH)**: REG：修正梯度指导下的条件扩散模型 

**Authors**: Zhengqi Gao, Kaiwen Zha, Tianyuan Zhang, Zihui Xue, Duane S. Boning  

**Link**: [PDF](https://arxiv.org/pdf/2501.18865)  

**Abstract**: Guidance techniques are simple yet effective for improving conditional generation in diffusion models. Albeit their empirical success, the practical implementation of guidance diverges significantly from its theoretical motivation. In this paper, we reconcile this discrepancy by replacing the scaled marginal distribution target, which we prove theoretically invalid, with a valid scaled joint distribution objective. Additionally, we show that the established guidance implementations are approximations to the intractable optimal solution under no future foresight constraint. Building on these theoretical insights, we propose rectified gradient guidance (REG), a versatile enhancement designed to boost the performance of existing guidance methods. Experiments on 1D and 2D demonstrate that REG provides a better approximation to the optimal solution than prior guidance techniques, validating the proposed theoretical framework. Extensive experiments on class-conditional ImageNet and text-to-image generation tasks show that incorporating REG consistently improves FID and Inception/CLIP scores across various settings compared to its absence. 

**Abstract (ZH)**: 指导技术尽管简单但有效，能提升扩散模型中的条件生成。尽管它们在实践中取得了成功，但指导方法的实际应用与理论动机之间存在显著差异。本文通过将理论上无效的缩放边缘分布目标替换为有效的缩放联合分布目标，来弥合这种差异。此外，我们证明了现有的指导方法是无未来预见约束下的难以求解的最优解的近似。基于这些理论洞见，我们提出了一种修复梯度指导（RECTIFIED GRADIENT GUIDANCE, REG），这是一种通用的增强方法，旨在提升现有指导方法的性能。实验结果表明，REG相比之前的指导技术提供了更接近最优解的近似，验证了提出的理论框架。在类别条件的ImageNet和文本到图像生成任务中进行的大量实验表明，REG在各种设置中都比其缺失的情况下显著改善了FID和Inception/CLIP分数。 

---
# BRiTE: Bootstrapping Reinforced Thinking Process to Enhance Language Model Reasoning 

**Title (ZH)**: BRiTE: 通过强化推理过程来提升语言模型推理能力的自Bootstrap方法 

**Authors**: Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, Liwei Wang, Mingyi Hong, Zhaoran Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18858)  

**Abstract**: Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, yet generating reliable reasoning processes remains a significant challenge. We present a unified probabilistic framework that formalizes LLM reasoning through a novel graphical model incorporating latent thinking processes and evaluation signals. Within this framework, we introduce the Bootstrapping Reinforced Thinking Process (BRiTE) algorithm, which works in two steps. First, it generates high-quality rationales by approximating the optimal thinking process through reinforcement learning, using a novel reward shaping mechanism. Second, it enhances the base LLM by maximizing the joint probability of rationale generation with respect to the model's parameters. Theoretically, we demonstrate BRiTE's convergence at a rate of $1/T$ with $T$ representing the number of iterations. Empirical evaluations on math and coding benchmarks demonstrate that our approach consistently improves performance across different base models without requiring human-annotated thinking processes. In addition, BRiTE demonstrates superior performance compared to existing algorithms that bootstrap thinking processes use alternative methods such as rejection sampling, and can even match or exceed the results achieved through supervised fine-tuning with human-annotated data. 

**Abstract (ZH)**: 大型语言模型（LLMs）在复杂推理任务中展现了非凡的能力，但生成可靠的推理过程仍然是一个重大挑战。我们提出了一种统一的概率框架，通过一个新颖的图形模型正式化LLM的推理过程，该模型结合了潜在的思考过程和评估信号。在这个框架中，我们引入了增强强化思考过程（Bootstrapping Reinforced Thinking Process, BRiTE）算法，该算法分为两步。首先，它通过强化学习近似最优的思考过程生成高质量的依据，利用一种新颖的奖励塑造机制。其次，它通过最大化生成依据与模型参数的联合概率来增强基础语言模型。理论上，我们证明了BRiTE的收敛速率为$1/T$，其中$T$表示迭代次数。在数学和编程基准测试中的实证评估显示，我们的方法可以在不需要人工标注思考过程的情况下，提高不同基础模型的性能。此外，BRiTE在使用替代方法如拒绝采样等进行思考过程增强的情况下，表现出优于现有算法的性能，并且甚至能够与或超越通过人工标注数据进行监督微调所获得的结果。 

---
# Constitutional Classifiers: Defending against Universal Jailbreaks across Thousands of Hours of Red Teaming 

**Title (ZH)**: 宪法分类器：防御数千小时红队测试中的通用破解攻击 

**Authors**: Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, Rob Gilson, Logan Graham, Logan Howard, Nimit Kalra, Taesung Lee, Kevin Lin, Peter Lofgren, Francesco Mosconi, Clare O'Hara, Catherine Olsson, Linda Petrini, Samir Rajani, Nikhil Saxena, Alex Silverstein, Tanya Singh, Theodore Sumers, Leonard Tang, Kevin K. Troy, Constantin Weisser, Ruiqi Zhong, Giulio Zhou, Jan Leike, Jared Kaplan, Ethan Perez  

**Link**: [PDF](https://arxiv.org/pdf/2501.18837)  

**Abstract**: Large language models (LLMs) are vulnerable to universal jailbreaks-prompting strategies that systematically bypass model safeguards and enable users to carry out harmful processes that require many model interactions, like manufacturing illegal substances at scale. To defend against these attacks, we introduce Constitutional Classifiers: safeguards trained on synthetic data, generated by prompting LLMs with natural language rules (i.e., a constitution) specifying permitted and restricted content. In over 3,000 estimated hours of red teaming, no red teamer found a universal jailbreak that could extract information from an early classifier-guarded LLM at a similar level of detail to an unguarded model across most target queries. On automated evaluations, enhanced classifiers demonstrated robust defense against held-out domain-specific jailbreaks. These classifiers also maintain deployment viability, with an absolute 0.38% increase in production-traffic refusals and a 23.7% inference overhead. Our work demonstrates that defending against universal jailbreaks while maintaining practical deployment viability is tractable. 

**Abstract (ZH)**: 大规模语言模型（LLMs）对通用型体制破坏（Jailbreaks）非常脆弱，这些破坏策略能够系统地绕过模型的安全防护，从而使用户能够执行需多次模型交互才能完成的有害操作，例如大规模制造非法物质。为了防范这些攻击，我们提出了一种宪法分类器（Constitutional Classifiers）：这些防护措施是通过让LLM生成合成数据，并使用自然语言规则（即宪法）界定允许和限制的内容来训练的。在超过3000个小时的红队测试中，未有任何红队成员找到能够从早期分类器保护下的LLM中提取与未保护模型相似详细信息的通用型体制破坏策略，尤其是在大多数目标查询中。在自动化评估中，增强型分类器对保留域特定性问题表现出了稳健的防御效果。同时，这些分类器也保持了部署的可行性，生产流量拒绝率绝对增加了0.38%，推理开销增加了23.7%。我们的研究证明，在确保实际部署可行性的同时防范通用型体制破坏是可行的。 

---
# Pitfalls of defacing whole-head MRI: re-identification risk with diffusion models and compromised research potential 

**Title (ZH)**: 全头MRI去标识化的陷阱：基于扩散模型的重新识别风险及其对研究潜力的影响 

**Authors**: Chenyu Gao, Kaiwen Xu, Michael E. Kim, Lianrui Zuo, Zhiyuan Li, Derek B. Archer, Timothy J. Hohman, Ann Zenobia Moore, Luigi Ferrucci, Lori L. Beason-Held, Susan M. Resnick, Christos Davatzikos, Jerry L. Prince, Bennett A. Landman  

**Link**: [PDF](https://arxiv.org/pdf/2501.18834)  

**Abstract**: Defacing is often applied to head magnetic resonance image (MRI) datasets prior to public release to address privacy concerns. The alteration of facial and nearby voxels has provoked discussions about the true capability of these techniques to ensure privacy as well as their impact on downstream tasks. With advancements in deep generative models, the extent to which defacing can protect privacy is uncertain. Additionally, while the altered voxels are known to contain valuable anatomical information, their potential to support research beyond the anatomical regions directly affected by defacing remains uncertain. To evaluate these considerations, we develop a refacing pipeline that recovers faces in defaced head MRIs using cascaded diffusion probabilistic models (DPMs). The DPMs are trained on images from 180 subjects and tested on images from 484 unseen subjects, 469 of whom are from a different dataset. To assess whether the altered voxels in defacing contain universally useful information, we also predict computed tomography (CT)-derived skeletal muscle radiodensity from facial voxels in both defaced and original MRIs. The results show that DPMs can generate high-fidelity faces that resemble the original faces from defaced images, with surface distances to the original faces significantly smaller than those of a population average face (p < 0.05). This performance also generalizes well to previously unseen datasets. For skeletal muscle radiodensity predictions, using defaced images results in significantly weaker Spearman's rank correlation coefficients compared to using original images (p < 10-4). For shin muscle, the correlation is statistically significant (p < 0.05) when using original images but not statistically significant (p > 0.05) when any defacing method is applied, suggesting that defacing might not only fail to protect privacy but also eliminate valuable information. 

**Abstract (ZH)**: 在公共发布前，常对头部磁共振成像（MRI）数据集进行去标识处理以应对隐私担忧。面部和邻近体素的改变引发了关于这些技术真正保护隐私能力以及对下游任务影响的讨论。随着深度生成模型的发展，去标识处理在保护隐私方面的有效性变得不确定。此外，虽然已知改变的体素包含有价值的空间解剖信息，但它们在直接受去标识影响的解剖区域之外支持研究的潜力仍然不确定。为评估这些考虑，我们开发了一条重新标识处理管线，使用级联扩散概率模型（DPMs）从去标识头部MRI中恢复面部。DPMs基于180名受试者的图像进行训练，并在来自484名未见过的受试者的图像上进行测试，其中469名来自不同的数据集。为了评估去标识处理中改变的体素是否包含普遍有用的信息，我们还从去标识和原始MRI中预测面部体素的计算机断层扫描（CT）衍生的骨骼肌肉放射密度。结果显示，DPMs可以生成高保真的面部，这些面部与去标识图像中的原始面部相似，表面距离显著小于人口平均水平面部（p < 0.05）。该性能也很好地推广到了以前未见过的数据集。对于骨骼肌肉放射密度预测，使用去标识图像相比使用原始图像，显著降低了 Spearman 等级相关系数（p < 10^-4）。对于小腿肌肉，使用原始图像时相关性在统计学上显著（p < 0.05），但在应用任何去标识方法时，相关性不再统计学显著（p > 0.05），这表明去标识不仅可能未能保护隐私，还可能消除有价值的 information。 

---
# An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for Anomaly Detection in CAN Bus 

**Title (ZH)**: 面向CAN总线异常检测的一种最优级联时空特征融合策略 

**Authors**: Mohammad Fatahi, Danial Sadrian Zadeh, Benyamin Ghojogh, Behzad Moshiri, Otman Basir  

**Link**: [PDF](https://arxiv.org/pdf/2501.18821)  

**Abstract**: Autonomous vehicles represent a revolutionary advancement driven by the integration of artificial intelligence within intelligent transportation systems. However, they remain vulnerable due to the absence of robust security mechanisms in the Controller Area Network (CAN) bus. In order to mitigate the security issue, many machine learning models and strategies have been proposed, which primarily focus on a subset of dominant patterns of anomalies and lack rigorous evaluation in terms of reliability and robustness. Therefore, to address the limitations of previous works and mitigate the security vulnerability in CAN bus, the current study develops a model based on the intrinsic nature of the problem to cover all dominant patterns of anomalies. To achieve this, a cascade feature-level fusion strategy optimized by a two-parameter genetic algorithm is proposed to combine temporal and spatial information. Subsequently, the model is evaluated using a paired t-test to ensure reliability and robustness. Finally, a comprehensive comparative analysis conducted on two widely used datasets advocates that the proposed model outperforms other models and achieves superior accuracy and F1-score, demonstrating the best performance among all models presented to date. 

**Abstract (ZH)**: 自动驾驶车辆代表了通过在智能交通系统中整合人工智能而实现的革命性进步。然而，由于缺乏针对控制局域网（CAN）总线的稳健安全机制，它们仍然存在安全漏洞。为了缓解这一问题，许多机器学习模型和策略已被提出，这些模型主要集中在异常的主导模式上，缺乏在可靠性和鲁棒性方面的严格评估。因此，本研究为了克服前人工作的局限性并缓解CAN总线的安全漏洞，基于问题的本质特征开发了一个模型，旨在涵盖所有主导模式的异常。为此，提出了一种由双参数遗传算法优化的级联特征级融合策略，以结合时间和空间信息。随后，通过配对t检验对模型进行评估，以确保可靠性和鲁棒性。最终，在两个广泛使用的数据集上进行的全面比较分析表明，所提出模型的表现优于其他模型，在准确性和F1分数上均表现出更佳的效果，展示了迄今为止所有提出模型中的最佳性能。 

---
# Large Language Models as Common-Sense Heuristics 

**Title (ZH)**: 大规模语言模型作为常识启发式方法 

**Authors**: Andrey Borro, Patricia J Riddle, Michael W Barley, Michael J Witbrock  

**Link**: [PDF](https://arxiv.org/pdf/2501.18816)  

**Abstract**: While systems designed for solving planning tasks vastly outperform Large Language Models (LLMs) in this domain, they usually discard the rich semantic information embedded within task descriptions. In contrast, LLMs possess parametrised knowledge across a wide range of topics, enabling them to leverage the natural language descriptions of planning tasks in their solutions. However, current research in this direction faces challenges in generating correct and executable plans. Furthermore, these approaches depend on the LLM to output solutions in an intermediate language, which must be translated into the representation language of the planning task. We introduce a novel planning method, which leverages the parametrised knowledge of LLMs by using their output as a heuristic for Hill-Climbing Search. This approach is further enhanced by prompting the LLM to generate a solution estimate to guide the search. Our method outperforms the task success rate of similar systems within a common household environment by 22 percentage points, with consistently executable plans. All actions are encoded in their original representation, demonstrating that strong results can be achieved without an intermediate language, thus eliminating the need for a translation step. 

**Abstract (ZH)**: 虽然用于解决规划任务的系统在这一领域远远优于大型语言模型（LLMs），但它们通常会丢弃任务描述中包含的丰富语义信息。相比之下，LLMs 拥有广泛主题下的参数化知识，使其能够利用规划任务的自然语言描述来解决问题。然而，当前在此方向上的研究在生成正确且可执行的规划方案方面面临挑战。此外，这些方法依赖于LLMs以中间语言形式输出解决方案，然后需要将其转换为规划任务的表现语言。我们提出了一种新的规划方法，该方法通过使用LLM的输出作为Hill-Climbing 搜索的启发式方法来利用LLM的参数化知识。该方法进一步通过提示LLM生成解估计来指导搜索。我们的方法在通用家庭环境中的任务成功率上比类似系统高出22个百分点，并且生成的规划方案始终可执行。所有操作都采用了原始表示形式，表明在没有中间语言的情况下也可以取得优异的结果，从而消除了翻译步骤的需要。 

---
# An Adversarial Approach to Register Extreme Resolution Tissue Cleared 3D Brain Images 

**Title (ZH)**: 一种对抗性方法用于注册极端分辨率的透明脑3D图像 

**Authors**: Abdullah Naziba, Clinton Fookes, Dimitri Perrin  

**Link**: [PDF](https://arxiv.org/pdf/2501.18815)  

**Abstract**: We developed a generative patch based 3D image registration model that can register very high resolution images obtained from a biochemical process name tissue clearing. Tissue clearing process removes lipids and fats from the tissue and make the tissue transparent. When cleared tissues are imaged with Light-sheet fluorescent microscopy, the resulting images give a clear window to the cellular activities and dynamics inside the this http URL the images obtained are very rich with cellular information and hence their resolution is extremely high (eg .2560x2160x676). Analyzing images with such high resolution is a difficult task for any image analysis this http URL registration is a common step in image analysis pipeline when comparison between images are required. Traditional image registration methods fail to register images with such extant. In this paper we addressed this very high resolution image registration issue by proposing a patch-based generative network named InvGAN. Our proposed network can register very high resolution tissue cleared images. The tissue cleared dataset used in this paper are obtained from a tissue clearing protocol named CUBIC. We compared our method both with traditional and deep-learning based registration this http URL different versions of CUBIC dataset are used, representing two different resolutions 25% and 100% respectively. Experiments on two different resolutions clearly show the impact of resolution on the registration quality. At 25% resolution, our method achieves comparable registration accuracy with very short time (7 minutes approximately). At 100% resolution, most of the traditional registration methods fail except Elastix registration this http URL takes 28 hours to register where proposed InvGAN takes only 10 minutes. 

**Abstract (ZH)**: 我们开发了一种基于生成性补丁的3D图像配准模型，能够对生物化学过程（称为组织透明化）所得的非常高分辨率图像进行配准。组织透明化过程通过去除组织中的脂质和脂肪使其变得透明。当使用体薄荧光显微镜对透明组织进行成像时，所得图像可以清晰地显示细胞活动和动力学。所得图像富含细胞信息，因此其分辨率非常高（例如，2560x2160x676）。对如此高分辨率的图像进行分析是任何图像分析中的一个艰巨任务。图像分析管道中的常见步骤是在需要比较图像时进行配准。传统图像配准方法无法成功配准具有如此高分辨率的图像。在本文中，我们通过提出一种名为InvGAN的补丁基生成网络解决了这一高分辨率图像配准问题。我们提出的网络可以对高分辨率的透明组织图像进行配准。本文使用的透明组织数据集来源于一种名为CUBIC的组织透明化协议。我们将我们的方法与传统的和基于深度学习的配准方法进行了比较。不同版本的CUBIC数据集被用于实验，分别代表两种不同的分辨率：25%和100%。实验结果在两个不同分辨率下清楚地显示了分辨率对配准质量的影响。在25%分辨率下，我们的方法在较短时间内（约7分钟）实现了与传统方法相当的配准精度。而在100%分辨率下，除了Elastix配准方法之外，大多数传统配准方法都无法成功，Elastix配准方法耗时28小时，而我们提出的InvGAN仅需10分钟。 

---
# Every Image Listens, Every Image Dances: Music-Driven Image Animation 

**Title (ZH)**: 每幅图像都在倾听，每幅图像都在舞蹈：基于音乐的图像动画 

**Authors**: Zhikang Dong, Weituo Hao, Ju-Chiang Wang, Peng Zhang, Pawel Polak  

**Link**: [PDF](https://arxiv.org/pdf/2501.18801)  

**Abstract**: Image animation has become a promising area in multimodal research, with a focus on generating videos from reference images. While prior work has largely emphasized generic video generation guided by text, music-driven dance video generation remains underexplored. In this paper, we introduce MuseDance, an innovative end-to-end model that animates reference images using both music and text inputs. This dual input enables MuseDance to generate personalized videos that follow text descriptions and synchronize character movements with the music. Unlike existing approaches, MuseDance eliminates the need for complex motion guidance inputs, such as pose or depth sequences, making flexible and creative video generation accessible to users of all expertise levels. To advance research in this field, we present a new multimodal dataset comprising 2,904 dance videos with corresponding background music and text descriptions. Our approach leverages diffusion-based methods to achieve robust generalization, precise control, and temporal consistency, setting a new baseline for the music-driven image animation task. 

**Abstract (ZH)**: 图像动画已成为多模态研究中有前途的领域，重点关注从参考图像生成视频。尽管先前的工作主要集中在基于文本和音乐生成通用视频，但基于音乐的舞蹈视频生成仍处于探索阶段。本文介绍了一种名为 MuseDance 的创新端到端模型，该模型利用音乐和文本输入对参考图像进行动画处理。这种双重输入使 MuseDance 能够生成符合文本描述并能与音乐同步的角色运动的个性化视频。与现有方法不同，MuseDance 消除了对复杂运动引导输入（如姿态或深度序列）的需求，使得各种技能水平的用户能够灵活和创造性地生成视频。为了推动该领域的研究，我们提供了一个新的多模态数据集，包含 2904 个舞蹈视频及其相应的背景音乐和文本描述。我们的方法利用扩散型方法实现了稳健的一般化、精确的控制以及时序一致性，为音乐驱动的图像动画任务设定了新的基准。 

---
# Compositional Generalization Requires More Than Disentangled Representations 

**Title (ZH)**: 组合泛化需要的远不止是互信息表示 

**Authors**: Qiyao Liang, Daoyuan Qian, Liu Ziyin, Ila Fiete  

**Link**: [PDF](https://arxiv.org/pdf/2501.18797)  

**Abstract**: Composition-the ability to generate myriad variations from finite means-is believed to underlie powerful generalization. However, compositional generalization remains a key challenge for deep learning. A widely held assumption is that learning disentangled (factorized) representations naturally supports this kind of extrapolation. Yet, empirical results are mixed, with many generative models failing to recognize and compose factors to generate out-of-distribution (OOD) samples. In this work, we investigate a controlled 2D Gaussian "bump" generation task, demonstrating that standard generative architectures fail in OOD regions when training with partial data, even when supplied with fully disentangled $(x, y)$ coordinates, re-entangling them through subsequent layers. By examining the model's learned kernels and manifold geometry, we show that this failure reflects a "memorization" strategy for generation through the superposition of training data rather than by combining the true factorized features. We show that models forced-through architectural modifications with regularization or curated training data-to create disentangled representations in the full-dimensional representational (pixel) space can be highly data-efficient and effective at learning to compose in OOD regions. These findings underscore that bottlenecks with factorized/disentangled representations in an abstract representation are insufficient: the model must actively maintain or induce factorization directly in the representational space in order to achieve robust compositional generalization. 

**Abstract (ZH)**: 下面是对该论文内容或标题的翻译，符合学术规范：

组成能力——即从有限的手段中生成众多变化的能力——被认为是强大泛化的底层机制。然而，组成性的泛化仍然是深度学习中的一个关键挑战。一个普遍的认识是，学习解耦（因素化）的表示自然支持这种外推。然而，实证结果并不一致，许多生成模型未能识别和组合因素以生成分布外（OOD）样本。在本研究中，我们考察了一个受控的二维高斯“凸起”生成任务，表明即使在提供完全解耦的 $(x, y)$ 坐标的情况下，当使用部分数据进行训练时，标准生成架构在 OOD 地区仍会失败，通过后续层重新组合这些坐标。通过研究模型学习的核函数和流形几何结构，我们表明这种失败反映了生成过程中的“记忆”策略，即将训练数据叠加而不是通过结合真正的因素化特征来进行组合。我们表明，通过采用正则化或精心选择的训练数据来强迫模型在完整维度的表示（像素）空间中创建解耦表示以实现 OOD 地区的有效组成学习的架构修改可以非常高效和有效。这些发现强调，虽然在抽象表示中存在解耦/解耦表示的瓶颈是不够的：模型必须在表示空间中主动维持或诱导解耦，以实现稳健的组成泛化。

这些发现表明，尽管在抽象表示中存在因素化/解耦表示的瓶颈是不够的：模型必须在表示空间中主动维持或诱导因素化，以实现稳健的组成泛化。 

---
# Survey and Improvement Strategies for Gene Prioritization with Large Language Models 

**Title (ZH)**: 大型语言模型在基因优先级确定中的调查与改进策略 

**Authors**: Matthew Neeley, Guantong Qi, Guanchu Wang, Ruixiang Tang, Dongxue Mao, Chaozhong Liu, Sasidhar Pasupuleti, Bo Yuan, Fan Xia, Pengfei Liu, Zhandong Liu, Xia Hu  

**Link**: [PDF](https://arxiv.org/pdf/2501.18794)  

**Abstract**: Rare diseases are challenging to diagnose due to limited patient data and genetic diversity. Despite advances in variant prioritization, many cases remain undiagnosed. While large language models (LLMs) have performed well in medical exams, their effectiveness in diagnosing rare genetic diseases has not been assessed. To identify causal genes, we benchmarked various LLMs for gene prioritization. Using multi-agent and Human Phenotype Ontology (HPO) classification, we categorized patients based on phenotypes and solvability levels. As gene set size increased, LLM performance deteriorated, so we used a divide-and-conquer strategy to break the task into smaller subsets. At baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking causal genes correctly. The multi-agent and HPO approaches helped distinguish confidently solved cases from challenging ones, highlighting the importance of known gene-phenotype associations and phenotype specificity. We found that cases with specific phenotypes or clear associations were more accurately solved. However, we observed biases toward well-studied genes and input order sensitivity, which hindered gene prioritization. Our divide-and-conquer strategy improved accuracy by overcoming these biases. By utilizing HPO classification, novel multi-agent techniques, and our LLM strategy, we improved causal gene identification accuracy compared to our baseline evaluation. This approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved cases, and accelerates gene discovery, supporting the development of targeted diagnostics and therapies. 

**Abstract (ZH)**: 罕见疾病由于患者数据有限和遗传多样性高，诊断起来具有挑战性。尽管在变体优先级排序方面取得了进展，但仍有许多病例未能确诊。虽然大型语言模型（LLMs）在医学考试中表现出色，但它们在诊断罕见遗传疾病方面的有效性尚未得到评估。为了识别致病变异，我们对各种LLM进行了基因优先级排序的基准测试。利用多智能体系统和人类表型ontology（HPO）分类，我们根据表型和解决难度将患者分为不同的类别。随着基因组集的增大，LLM的表现逐渐下降，因此我们采用分而治之的策略将任务分解成更小的子集。基线测试中，GPT-4 在排序致病变异方面表现优于其他LLM，准确率接近30%。多智能体系统和HPO方法有助于区分易解和难解的病例，突显了已知基因-表型关联和表型特异性的重要性。我们发现，具有特定表型或明确关联的病例更易得到准确解决。然而，我们观察到对研究较多的基因存在偏差，并且输入顺序的敏感性影响了基因优先级排序。我们的分而治之策略通过克服这些偏差提高了准确率。通过利用HPO分类、新型多智能体技术和我们的LLM策略，我们提高了致病变异识别的准确性，与基线评估相比有所改进。这种 approach 简化了罕见疾病的诊断，促进了未解病例的重新分析，加速了基因发现，支持了针对性诊断和治疗的发展。 

---
# OT-Transformer: A Continuous-time Transformer Architecture with Optimal Transport Regularization 

**Title (ZH)**: OT-Transformer：一种带有最优传输正则化的连续时间Transformer架构 

**Authors**: Kelvin Kan, Xingjian Li, Stanley Osher  

**Link**: [PDF](https://arxiv.org/pdf/2501.18793)  

**Abstract**: Transformers have achieved state-of-the-art performance in numerous tasks. In this paper, we propose a continuous-time formulation of transformers. Specifically, we consider a dynamical system whose governing equation is parametrized by transformer blocks. We leverage optimal transport theory to regularize the training problem, which enhances stability in training and improves generalization of the resulting model. Moreover, we demonstrate in theory that this regularization is necessary as it promotes uniqueness and regularity of solutions. Our model is flexible in that almost any existing transformer architectures can be adopted to construct the dynamical system with only slight modifications to the existing code. We perform extensive numerical experiments on tasks motivated by natural language processing, image classification, and point cloud classification. Our experimental results show that the proposed method improves the performance of its discrete counterpart and outperforms relevant comparing models. 

**Abstract (ZH)**: 变压器在网络众多任务中已经实现了最先进的性能。本文提出了一种变压器的连续时间形式化方法。具体地说，我们考虑一个其支配方程由变压器块参数化的动力系统。我们利用最优传输理论对训练问题进行正则化，从而增强了训练的稳定性并提高了模型的泛化能力。此外，我们从理论上证明，这种正则化是必要的，因为它促进了解的唯一性和正则性。我们的模型具有灵活性，几乎可以采用任何现有的变压器架构来构建动力系统，只需对现有代码进行轻微修改。我们在自然语言处理、图像分类和点云分类等任务上进行了广泛的数值实验。我们的实验结果表明，所提出的方法提高了其离散对应模型的性能，并且优于相关比较模型。 

---
# Overestimation in LLM Evaluation: A Controlled Large-Scale Study on Data Contamination's Impact on Machine Translation 

**Title (ZH)**: LLM评估中的过度估计：数据污染对机器翻译影响的受控大规模研究 

**Authors**: Muhammed Yusuf Kocyigit, Eleftheria Briakou, Daniel Deutsch, Jiaming Luo, Colin Cherry, Markus Freitag  

**Link**: [PDF](https://arxiv.org/pdf/2501.18771)  

**Abstract**: Data contamination -- the accidental consumption of evaluation examples within the pre-training data -- can undermine the validity of evaluation benchmarks. In this paper, we present a rigorous analysis of the effects of contamination on language models at 1B and 8B scales on the machine translation task. Starting from a carefully decontaminated train-test split, we systematically introduce contamination at various stages, scales, and data formats to isolate its effect and measure its impact on performance metrics. Our experiments reveal that contamination with both source and target substantially inflates BLEU scores, and this inflation is 2.5 times larger (up to 30 BLEU points) for 8B compared to 1B models. In contrast, source-only and target-only contamination generally produce smaller, less consistent over-estimations. Finally, we study how the temporal distribution and frequency of contaminated samples influence performance over-estimation across languages with varying degrees of data resources. 

**Abstract (ZH)**: 数据污染——预训练数据中意外摄入评估样本——可能会削弱评估基准的有效性。本文通过对在机器翻译任务中1B和8B规模的语言模型进行严格分析，探讨数据污染对其性能的影响。我们从精心清除污染的训练-测试拆分开始，系统性地在不同阶段、不同规模和不同数据格式中引入污染，以分离污染的影响并衡量其对性能指标的影响。实验结果表明，同时污染源语言和目标语言会导致BLEU分数显著膨胀，且8B模型的膨胀幅度比1B模型高出2.5倍（最多可达30个BLEU分数点）。相比之下，仅污染源语言和仅污染目标语言通常会产生较小且不一致的高估效应。最后，我们研究污染样本的时间分布和频率如何影响不同数据资源条件下多种语言性能高估的影响。 

---
# Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization 

**Title (ZH)**: 设计中的多样性：利用分布匹配进行离线模型导向的优化 

**Authors**: Michael S. Yao, James C. Gee, Osbert Bastani  

**Link**: [PDF](https://arxiv.org/pdf/2501.18768)  

**Abstract**: The goal of offline model-based optimization (MBO) is to propose new designs that maximize a reward function given only an offline dataset. However, an important desiderata is to also propose a diverse set of final candidates that capture many optimal and near-optimal design configurations. We propose Diversity in Adversarial Model-based Optimization (DynAMO) as a novel method to introduce design diversity as an explicit objective into any MBO problem. Our key insight is to formulate diversity as a distribution matching problem where the distribution of generated designs captures the inherent diversity contained within the offline dataset. Extensive experiments spanning multiple scientific domains show that DynAMO can be used with common optimization methods to significantly improve the diversity of proposed designs while still discovering high-quality candidates. 

**Abstract (ZH)**: 基于离线模型的优化（MBO）的目标是在仅给定离线数据集的情况下，提出能够最大化奖励函数的新设计。然而，一个重要的要求是同样需要提出一个多样性的终选方案集，以捕捉许多最优和近似最优的设计配置。我们提出了一种名为对抗模型基于优化中多样性引入的新方法（DynAMO），以将设计多样性明确地引入到任何MBO问题中。我们的核心洞察是将多样性问题形式化为分布匹配问题，其中生成的设计分布捕捉了离线数据集中固有的多样性。在多个科学领域的广泛实验表明，DynAMO 可以与常用优化方法结合使用，显著提高建议设计的多样性，同时仍然能够发现高质量的候选方案。 

---
# Breaking the Fake News Barrier: Deep Learning Approaches in Bangla Language 

**Title (ZH)**: 突破假新闻障碍：孟加拉语中的深度学习方法 

**Authors**: Pronoy Kumar Mondal, Sadman Sadik Khan, Md. Masud Rana, Shahriar Sultan Ramit, Abdus Sattar, Md. Sadekur Rahman  

**Link**: [PDF](https://arxiv.org/pdf/2501.18766)  

**Abstract**: The rapid development of digital stages has greatly compounded the dispersal of untrue data, dissolving certainty and judgment in society, especially among the Bengali-speaking community. Our ponder addresses this critical issue by presenting an interesting strategy that utilizes a profound learning innovation, particularly the Gated Repetitive Unit (GRU), to recognize fake news within the Bangla dialect. The strategy of our proposed work incorporates intensive information preprocessing, which includes lemmatization, tokenization, and tending to course awkward nature by oversampling. This comes about in a dataset containing 58,478 passages. We appreciate the creation of a demonstration based on GRU (Gated Repetitive Unit) that illustrates remarkable execution with a noteworthy precision rate of 94%. This ponder gives an intensive clarification of the methods included in planning the information, selecting the show, preparing it, and assessing its execution. The performance of the model is investigated by reliable metrics like precision, recall, F1 score, and accuracy. The commitment of the work incorporates making a huge fake news dataset in Bangla and a demonstration that has outperformed other Bangla fake news location models. 

**Abstract (ZH)**: 数字舞台的迅速发展极大地加剧了假信息的扩散，削弱了社会的确定性和判断力，尤其是在讲孟加拉语的社区中。我们针对这一关键问题，提出了一个有趣的战略，利用深刻的学习创新，特别是门控重复单元（GRU），来识别孟加拉方言中的假新闻。我们提出的工作策略包含了详细的信息预处理，包括词干提取、分词和通过过采样解决语义偏差。这些方法生成了一个包含58,478段文本的数据集。我们基于GRU（门控重复单元）创建了演示，其表现卓越，精确率达到94%。本文详细阐述了信息策划、模型选择、训练和评估过程中的方法。模型的性能通过精确度、召回率、F1分数和准确性等可靠的指标进行了研究。本文的贡献包括创建了一个庞大的孟加拉语假新闻数据集以及一个超越其他孟加拉语假新闻识别模型的演示。 

---
# Synthetic Data Generation for Augmenting Small Samples 

**Title (ZH)**: augmentation 小样本数据增强的合成数据生成方法 

**Authors**: Dan Liu, Samer El Kababji, Nicholas Mitsakakis, Lisa Pilgram, Thomas Walters, Mark Clemons, Greg Pond, Alaa El-Hussuna, Khaled El Emam  

**Link**: [PDF](https://arxiv.org/pdf/2501.18741)  

**Abstract**: Small datasets are common in health research. However, the generalization performance of machine learning models is suboptimal when the training datasets are small. To address this, data augmentation is one solution. Augmentation increases sample size and is seen as a form of regularization that increases the diversity of small datasets, leading them to perform better on unseen data. We found that augmentation improves prognostic performance for datasets that: have fewer observations, with smaller baseline AUC, have higher cardinality categorical variables, and have more balanced outcome variables. No specific generative model consistently outperformed the others. We developed a decision support model that can be used to inform analysts if augmentation would be useful. For seven small application datasets, augmenting the existing data results in an increase in AUC between 4.31% (AUC from 0.71 to 0.75) and 43.23% (AUC from 0.51 to 0.73), with an average 15.55% relative improvement, demonstrating the nontrivial impact of augmentation on small datasets (p=0.0078). Augmentation AUC was higher than resampling only AUC (p=0.016). The diversity of augmented datasets was higher than the diversity of resampled datasets (p=0.046). 

**Abstract (ZH)**: 在健康研究中，小数据集是常见的问题。然而，当训练数据集较小时，机器学习模型的泛化性能不佳。为解决这一问题，数据增强是一种解决方案。数据增强可以增加样本量，并被视为一种增加小数据集多样性的正则化形式，从而使它们在未见过的数据上有更好的表现。我们发现，对于以下特征的数据集，数据增强可以提高其预后性能：观测值较少，基线AUC较小，分类变量的卡方较高，以及结果变量更加平衡。没有一种特定的生成模型始终优于其他模型。我们开发了一种决策支持模型，可以用于帮助分析师判断数据增强是否会有用。对于七个小型应用数据集，增强现有数据后，AUC的增加范围从4.31%（AUC从0.71到0.75）到43.23%（AUC从0.51到0.73），平均相对提升15.55%，表明数据增强对小型数据集具有重要影响（p=0.0078）。数据增强后的AUC高于仅使用重采样的AUC（p=0.016）。增强后的数据集的多样性也高于仅重采样的数据集的多样性（p=0.046）。 

---
# Neural Graph Pattern Machine 

**Title (ZH)**: 神经图模式机 

**Authors**: Zehong Wang, Zheyuan Zhang, Tianyi Ma, Nitesh V Chawla, Chuxu Zhang, Yanfang Ye  

**Link**: [PDF](https://arxiv.org/pdf/2501.18739)  

**Abstract**: Graph learning tasks require models to comprehend essential substructure patterns relevant to downstream tasks, such as triadic closures in social networks and benzene rings in molecular graphs. Due to the non-Euclidean nature of graphs, existing graph neural networks (GNNs) rely on message passing to iteratively aggregate information from local neighborhoods. Despite their empirical success, message passing struggles to identify fundamental substructures, such as triangles, limiting its expressiveness. To overcome this limitation, we propose the Neural Graph Pattern Machine (GPM), a framework designed to learn directly from graph patterns. GPM efficiently extracts and encodes substructures while identifying the most relevant ones for downstream tasks. We also demonstrate that GPM offers superior expressivity and improved long-range information modeling compared to message passing. Empirical evaluations on node classification, link prediction, graph classification, and regression show the superiority of GPM over state-of-the-art baselines. Further analysis reveals its desirable out-of-distribution robustness, scalability, and interpretability. We consider GPM to be a step toward going beyond message passing. 

**Abstract (ZH)**: 图学习任务要求模型理解与下游任务相关的关键子结构模式，例如社会网络中的三角闭包和分子图中的苯环。由于图是非欧几里得的，现有的图神经网络（GNNs）依赖于消息传递机制，通过迭代聚合局部邻居的信息。尽管取得了经验上的成功，但消息传递在识别关键子结构（如三角形）方面存在问题，限制了其表达能力。为了克服这一限制，我们提出了神经图模式机器（GPM），这是一种旨在直接从图模式中学习的框架。GPM 有效地提取和编码子结构，并能识别对下游任务最相关的子结构。我们还证明了GPM在表达能力和长范围信息建模方面优于消息传递。在节点分类、边预测、图分类和回归任务上的实证评估显示，GPM 在最新基线方法中表现出优越性。进一步的分析表明，GPM 具备理想的离分布鲁棒性、可扩展性和可解释性。我们认为，GPM 是超越消息传递的重要一步。 

---
# Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation 

**Title (ZH)**: 将下面的论文内容或标题翻译成中文，并符合学术规范：

[Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation]

“集成LMM规划器和三维技能策略以实现通用化操作” 

**Authors**: Yuelei Li, Ge Yan, Annabella Macaluso, Mazeyu Ji, Xueyan Zou, Xiaolong Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18733)  

**Abstract**: The recent advancements in visual reasoning capabilities of large multimodal models (LMMs) and the semantic enrichment of 3D feature fields have expanded the horizons of robotic capabilities. These developments hold significant potential for bridging the gap between high-level reasoning from LMMs and low-level control policies utilizing 3D feature fields. In this work, we introduce LMM-3DP, a framework that can integrate LMM planners and 3D skill Policies. Our approach consists of three key perspectives: high-level planning, low-level control, and effective integration. For high-level planning, LMM-3DP supports dynamic scene understanding for environment disturbances, a critic agent with self-feedback, history policy memorization, and reattempts after failures. For low-level control, LMM-3DP utilizes a semantic-aware 3D feature field for accurate manipulation. In aligning high-level and low-level control for robot actions, language embeddings representing the high-level policy are jointly attended with the 3D feature field in the 3D transformer for seamless integration. We extensively evaluate our approach across multiple skills and long-horizon tasks in a real-world kitchen environment. Our results show a significant 1.45x success rate increase in low-level control and an approximate 1.5x improvement in high-level planning accuracy compared to LLM-based baselines. Demo videos and an overview of LMM-3DP are available at this https URL. 

**Abstract (ZH)**: 近年来，大型多模态模型（LMMs）在视觉推理能力方面的进展以及3D特征域能量的语义丰富化，极大地扩展了机器人能力的边界。这些进展为将高级推理从LMMs与利用3D特征领域的低级控制策略之间的差距提供了巨大的潜力。在此基础上，我们提出了一种名为LMM-3DP的框架，该框架可以整合LMM规划器和3D技能策略。我们的方法包含三个关键视角：高级规划、低级控制以及有效的整合。在高级规划方面，LMM-3DP支持动态场景理解以应对环境干扰、带有自我反馈的批评代理、历史策略记忆，以及失败后的重试。在低级控制方面，LMM-3DP利用具有语义意识的3D特征领域进行精确操纵。为了实现高级控制与低级控制之间的对齐，我们的方法在3D变换中将表示高级策略的语言嵌入与3D特征领域共同关注，实现无缝整合。我们对LMM-3DP在多个技能和长期任务中的实际厨房环境中进行了广泛的评估。结果显示，LMM-3DP的低级控制成功率提高了1.45倍，高级规划的准确性提高了约1.5倍，相较于基于LLM的基准系统。有关演示视频和LMM-3DP的概述，请访问此链接：[请插入链接]。 

---
# Exploring Audio Editing Features as User-Centric Privacy Defenses Against Emotion Inference Attacks 

**Title (ZH)**: 探索以用户为中心的声音编辑特性，作为对抗情绪推断攻击的隐私保护措施 

**Authors**: Mohd. Farhan Israk Soumik, W.K.M. Mithsara, Abdur R. Shahid, Ahmed Imteaj  

**Link**: [PDF](https://arxiv.org/pdf/2501.18727)  

**Abstract**: The rapid proliferation of speech-enabled technologies, including virtual assistants, video conferencing platforms, and wearable devices, has raised significant privacy concerns, particularly regarding the inference of sensitive emotional information from audio data. Existing privacy-preserving methods often compromise usability and security, limiting their adoption in practical scenarios. This paper introduces a novel, user-centric approach that leverages familiar audio editing techniques, specifically pitch and tempo manipulation, to protect emotional privacy without sacrificing usability. By analyzing popular audio editing applications on Android and iOS platforms, we identified these features as both widely available and usable. We rigorously evaluated their effectiveness against a threat model, considering adversarial attacks from diverse sources, including Deep Neural Networks (DNNs), Large Language Models (LLMs), and and reversibility testing. Our experiments, conducted on three distinct datasets, demonstrate that pitch and tempo manipulation effectively obfuscates emotional data. Additionally, we explore the design principles for lightweight, on-device implementation to ensure broad applicability across various devices and platforms. 

**Abstract (ZH)**: 以语音为中心的技术的迅速普及，包括虚拟助手、视频会议平台和可穿戴设备，引发了显著的隐私担忧，特别是在从音频数据中推断敏感情感信息方面的担忧。现有的隐私保护方法往往在易用性和安全性之间做出牺牲，限制了它们在实际场景中的应用。本文引入了一种新颖的以用户为中心的方法，利用熟悉的音频编辑技术，特别是音调和节奏的调整，来保护情感隐私，同时不牺牲易用性。通过对Android和iOS平台上的主流音频编辑应用程序进行分析，我们发现这些功能既广泛可用又易于使用。我们对这些功能的有效性进行了严格的评估，考虑了来自多种来源的对抗性攻击，包括深度神经网络（DNNs）、大语言模型（LLMs）以及可逆性测试。在三个不同的数据集上进行的实验表明，音调和节奏的调整有效模糊了情感数据。此外，我们探讨了轻量级、设备端实现的设计原则，以确保其在各种设备和平台上的广泛适用性。 

---
# Scaling Policy Gradient Quality-Diversity with Massive Parallelization via Behavioral Variations 

**Title (ZH)**: 通过行为变异实现大规模并行化以提高政策梯度多变性质量 

**Authors**: Konstantinos Mitsides, Maxence Faldor, Antoine Cully  

**Link**: [PDF](https://arxiv.org/pdf/2501.18723)  

**Abstract**: Quality-Diversity optimization comprises a family of evolutionary algorithms aimed at generating a collection of diverse and high-performing solutions. MAP-Elites (ME), a notable example, is used effectively in fields like evolutionary robotics. However, the reliance of ME on random mutations from Genetic Algorithms limits its ability to evolve high-dimensional solutions. Methods proposed to overcome this include using gradient-based operators like policy gradients or natural evolution strategies. While successful at scaling ME for neuroevolution, these methods often suffer from slow training speeds, or difficulties in scaling with massive parallelization due to high computational demands or reliance on centralized actor-critic training. In this work, we introduce a fast, sample-efficient ME based algorithm capable of scaling up with massive parallelization, significantly reducing runtimes without compromising performance. Our method, ASCII-ME, unlike existing policy gradient quality-diversity methods, does not rely on centralized actor-critic training. It performs behavioral variations based on time step performance metrics and maps these variations to solutions using policy gradients. Our experiments show that ASCII-ME can generate a diverse collection of high-performing deep neural network policies in less than 250 seconds on a single GPU. Additionally, it operates on average, five times faster than state-of-the-art algorithms while still maintaining competitive sample efficiency. 

**Abstract (ZH)**: 质量多样性优化是一类旨在生成多样且高性能解的进化算法家族。MAP-Elites（ME）是这种优化方法的一个突出例子，被广泛应用于进化机器人等领域。然而，ME 对遗传算法随机突变的依赖性限制了其在高维解生成中的能力。为克服这一限制，研究人员提出了一些方法，包括使用基于梯度的操作符，如策略梯度或自然进化策略。尽管这些方法在神经进化领域适用于扩展ME，但它们通常会面临训练速度较慢的问题，或者大规模并行化时难以扩展，这是因为高度计算需求或依赖集中式的演员-评论家训练。在本文中，我们提出了一种快速、样本高效的ME变体算法，该算法能够支持大规模并行化，显著减少了运行时间，同时不牺牲性能。我们的方法ASCII-ME 不依赖于集中式的演员-评论家训练，而是基于时间步的性能指标来执行行为变化，并使用策略梯度将这些变化映射到解决方案上。我们的实验表明，ASCII-ME 能在单个GPU上用不到250秒的时间生成多种高性能的深度神经网络策略。此外，它在平均速度上比最先进的算法快五倍，同时依然保持了良好的样本效率。 

---
# High-Accuracy ECG Image Interpretation using Parameter-Efficient LoRA Fine-Tuning with Multimodal LLaMA 3.2 

**Title (ZH)**: 使用参数高效LoRA微调和多模态LLaMA 3.2进行高精度ECG图像解释 

**Authors**: Nandakishor M, Anjali M  

**Link**: [PDF](https://arxiv.org/pdf/2501.18670)  

**Abstract**: Electrocardiogram (ECG) interpretation is a cornerstone of cardiac diagnostics. This paper explores a practical approach to enhance ECG image interpretation using the multimodal LLaMA 3.2 model. We used a parameter-efficient fine-tuning strategy, Low-Rank Adaptation (LoRA), specifically designed to boost the model's ability to understand ECG images and achieve better outcomes across a wide range of cardiac conditions. Our method is tailored for ECG analysis and leverages ECGInstruct, a large-scale instruction dataset with 1 Million samples. This dataset is a rich collection of synthesized ECG images, generated from raw ECG data from trusted open-source repositories like MIMIC-IV ECG and PTB-XL. Each ECG image in ECGInstruct comes with expert-written questions and detailed answers, covering diverse ECG interpretation scenarios, including complex cardiac conditions like Myocardial Infarction and Conduction Disturbances. Our fine-tuning approach efficiently adapts the LLaMA 3.2 model (built upon LLaMA 3) by integrating low-rank adaptation techniques, focusing on efficiency by updating only a small set of parameters, specifically ignoring the `lm_head` and `embed_tokens` layers. This paper details the model setup, our efficient fine-tuning method, and implementation specifics. We provide a thorough evaluation through extensive experiments, demonstrating the effectiveness of our method across various ECG interpretation tasks. The results convincingly show that our parameter-efficient LoRA fine-tuning achieves excellent performance in ECG image interpretation, significantly outperforming baseline models and reaching accuracy comparable to or exceeding traditional CNN-based methods in identifying a wide range of cardiac abnormalities, including over 70 conditions from the PTB-XL dataset. 

**Abstract (ZH)**: 心电图（ECG）解读是心脏诊断的基础。本文探讨了一种实用的方法，利用多模态LLaMA 3.2模型来提升ECG图像的解读能力。我们采用了参数高效微调策略——低秩适应（LoRA），专门设计用于增强模型对ECG图像的理解能力，并在广泛的心脏病条件下取得更好的结果。该方法针对ECG分析进行了优化，并利用了一个名为ECGInstruct的大型指令数据集，包含100万样本。此数据集是由来自MIMIC-IV ECG和PTB-XL等可靠开源仓库的原始ECG数据生成的合成ECG图像的丰富集合。ECGInstruct中的每个ECG图像都附有专家编写的问题和详细的答案，涵盖了各种ECG解读场景，包括心肌梗死和传导障碍等复杂心脏状况。我们的微调方法通过结合低秩适应技术高效地适应了LLaMA 3.2模型（基于LLaMA 3构建），仅更新了一小部分参数，并且特别忽略了`lm_head`和`embed_tokens`层，从而提高了模型的效率。本文详细介绍了模型设置、我们高效微调方法以及实现的具体内容。我们通过广泛的实验进行了详细评估，证明了该方法在各种ECG解读任务中的有效性。结果表明，我们的参数高效LoRA微调方法在ECG图像解读中取得了出色的表现，显著优于基线模型，并达到了与传统基于CNN的方法相比或优于识别广泛心脏异常（包括PTB-XL数据集中超过70种条件）的准确性。 

---
# The Pitfalls of "Security by Obscurity" And What They Mean for Transparent AI 

**Title (ZH)**: “隐秘安全”之陷阱及其对透明AI的意义 

**Authors**: Peter Hall, Olivia Mundahl, Sunoo Park  

**Link**: [PDF](https://arxiv.org/pdf/2501.18669)  

**Abstract**: Calls for transparency in AI systems are growing in number and urgency from diverse stakeholders ranging from regulators to researchers to users (with a comparative absence of companies developing AI). Notions of transparency for AI abound, each addressing distinct interests and concerns.
In computer security, transparency is likewise regarded as a key concept. The security community has for decades pushed back against so-called security by obscurity -- the idea that hiding how a system works protects it from attack -- against significant pressure from industry and other stakeholders. Over the decades, in a community process that is imperfect and ongoing, security researchers and practitioners have gradually built up some norms and practices around how to balance transparency interests with possible negative side effects. This paper asks: What insights can the AI community take from the security community's experience with transparency?
We identify three key themes in the security community's perspective on the benefits of transparency and their approach to balancing transparency against countervailing interests. For each, we investigate parallels and insights relevant to transparency in AI. We then provide a case study discussion on how transparency has shaped the research subfield of anonymization. Finally, shifting our focus from similarities to differences, we highlight key transparency issues where modern AI systems present challenges different from other kinds of security-critical systems, raising interesting open questions for the security and AI communities alike. 

**Abstract (ZH)**: 来自监管机构、研究人员和用户等多元利益方对于人工智能系统的透明度要求日益增多且迫在眉睫。尽管如此，开发人工智能技术的公司对此的关注相对较少。透明度的概念在人工智能领域极为丰富，每个概念都针对不同的利益和关注点。

在计算机安全领域，透明度也被视为一个核心概念。安全社区长期以来一直在反对所谓的“秘密安全”——即认为隐藏系统的工作原理可以保护系统免受攻击的看法，这在来自行业和其他利益方的巨大压力下显得更为突出。数十年来，在一个不完美但持续进行的社区过程中，安全研究人员和实践者逐渐建立了一些规范和方法，以平衡透明度的利益与可能的负面影响之间的关系。这篇文章提出了一个关键问题：人工智能社区可以从安全社区在透明度方面的经验中学到什么？

我们归纳了安全社区在透明度益处和如何在与相抗衡的利益之间进行平衡方面的三个关键主题。针对每个主题，我们将探讨与人工智能透明度相关的人类学对比和见解。随后，我们将进行案例研究讨论，探讨透明度如何塑造匿名化研究子领域。最后，转向相似性与差异性的讨论，我们突出了一些现代人工智能系统所带来的透明度问题，这些问题是其他类型的安全关键系统所面临的挑战，这为安全和人工智能领域双方都提出了有趣的开放性问题。 

---
# Structure Development in List-Sorting Transformers 

**Title (ZH)**: 列表排序变换器中的结构发展 

**Authors**: Einar Urdshals, Jasmina Urdshals  

**Link**: [PDF](https://arxiv.org/pdf/2501.18666)  

**Abstract**: We study how a one-layer attention-only transformer develops relevant structures while learning to sort lists of numbers. At the end of training, the model organizes its attention heads in two main modes that we refer to as vocabulary-splitting and copy-suppression. Both represent simpler modes than having multiple heads handle overlapping ranges of numbers. Interestingly, vocabulary-splitting is present regardless of whether we use weight decay, a common regularization technique thought to drive simplification, supporting the thesis that neural networks naturally prefer simpler solutions. We relate copy-suppression to a mechanism in GPT-2 and investigate its functional role in our model. Guided by insights from a developmental analysis of the model, we identify features in the training data that drive the model's final acquired solution. This provides a concrete example of how the training data shape the internal organization of transformers, paving the way for future studies that could help us better understand how LLMs develop their internal structures. 

**Abstract (ZH)**: 我们在研究一种仅使用注意力机制的一层变压器如何在学习对数字列表进行排序时发展出相关的结构。经过训练后，模型在其注意力头中形成了两种主要模式，我们称之为词汇分割和复制抑制。这两种模式都比多个头处理重叠的数字范围要简单。有趣的是，无论我们是否使用权重衰减（一种常用的正则化技术，据认为可以促进简化），词汇分割始终存在，这支持了神经网络自然偏好简单解法的论点。我们将复制抑制与GPT-2中的某种机制相联系，并探讨其在我们模型中的功能性角色。根据对模型发展分析的见解，我们识别出训练数据中的特征，这些特征促使模型最终获得的解决方案。这提供了一个具体示例，说明训练数据如何塑造变压器的内部组织，为未来的研究铺平了道路，这些研究有助于我们更好地了解大型语言模型是如何发展其内部结构的。 

---
# BARNN: A Bayesian Autoregressive and Recurrent Neural Network 

**Title (ZH)**: BARNN：贝叶斯自回归和递归神经网络 

**Authors**: Dario Coscia, Max Welling, Nicola Demo, Gianluigi Rozza  

**Link**: [PDF](https://arxiv.org/pdf/2501.18665)  

**Abstract**: Autoregressive and recurrent networks have achieved remarkable progress across various fields, from weather forecasting to molecular generation and Large Language Models. Despite their strong predictive capabilities, these models lack a rigorous framework for addressing uncertainty, which is key in scientific applications such as PDE solving, molecular generation and Machine Learning Force Fields. To address this shortcoming we present BARNN: a variational Bayesian Autoregressive and Recurrent Neural Network. BARNNs aim to provide a principled way to turn any autoregressive or recurrent model into its Bayesian version. BARNN is based on the variational dropout method, allowing to apply it to large recurrent neural networks as well. We also introduce a temporal version of the "Variational Mixtures of Posteriors" prior (tVAMP-prior) to make Bayesian inference efficient and well-calibrated. Extensive experiments on PDE modelling and molecular generation demonstrate that BARNN not only achieves comparable or superior accuracy compared to existing methods, but also excels in uncertainty quantification and modelling long-range dependencies. 

**Abstract (ZH)**: 自回归和循环网络在天气预报、分子生成和大规模语言模型等领域已经取得了显著的进步。尽管这些模型具备强大的预测能力，但在科学应用中（如偏微分方程求解、分子生成和机器学习力场）需要一个严谨的不确定性处理框架。为解决这一问题，我们提出了BARNN：一种变分贝叶斯自回归和循环神经网络。BARNN旨在为任何自回归或循环模型提供将其转化为贝叶斯版本的原理性方法。BARNN基于变分dropout方法，使得它能够应用于大型循环神经网络。我们还引入了“变分后验混合”时间序列版本（tVAMP-prior）作为先验，以提高贝叶斯推断的效率和准确性。在偏微分方程建模和分子生成的广泛实验中，BARNN不仅在准确性方面与现有方法相当或更优，还在不确定性量化和建模长距离依赖方面表现出色。 

---
# Rethinking the Upsampling Layer in Hyperspectral Image Super Resolution 

**Title (ZH)**: 重新审视超谱成像超分辨率中的上采样层 

**Authors**: Haohan Shi, Fei Zhou, Xin Sun, Jungong Han  

**Link**: [PDF](https://arxiv.org/pdf/2501.18664)  

**Abstract**: Deep learning has achieved significant success in single hyperspectral image super-resolution (SHSR); however, the high spectral dimensionality leads to a heavy computational burden, thus making it difficult to deploy in real-time scenarios. To address this issue, this paper proposes a novel lightweight SHSR network, i.e., LKCA-Net, that incorporates channel attention to calibrate multi-scale channel features of hyperspectral images. Furthermore, we demonstrate, for the first time, that the low-rank property of the learnable upsampling layer is a key bottleneck in lightweight SHSR methods. To address this, we employ the low-rank approximation strategy to optimize the parameter redundancy of the learnable upsampling layer. Additionally, we introduce a knowledge distillation-based feature alignment technique to ensure the low-rank approximated network retains the same feature representation capacity as the original. We conducted extensive experiments on the Chikusei, Houston 2018, and Pavia Center datasets compared to some SOTAs. The results demonstrate that our method is competitive in performance while achieving speedups of several dozen to even hundreds of times compared to other well-performing SHSR methods. 

**Abstract (ZH)**: 深度学习在单谱域图像超分辨率（SHSR）方面取得了显著成功；然而，高光谱维度导致了沉重的计算负担，因此在实时场景中难以部署。为解决这一问题，本文提出了一种新型的轻量级SHSR网络，即LKCA-Net，该网络结合了通道注意力机制，以校准 hyperspectral 图像的多尺度通道特征。此外，我们首次证明了可学习上采样层的低秩性质是轻量级SHSR方法中的一个关键瓶颈。为此，我们采用低秩逼近策略优化可学习上采样层的参数冗余度。此外，我们引入了一种基于知识蒸馏的特征对齐技术，以确保低秩逼近网络保留与原始模型相同的特征表示能力。我们通过在 Chikusei、Houston 2018 和 Pavia Center 数据集上与一些最先进的方法进行广泛的实验对比。结果表明，我们的方法在性能上具有竞争力，同时相比其他表现良好的SHSR方法可实现数十到数百倍的速度提升。 

---
# Joint Optimization of Prompt Security and System Performance in Edge-Cloud LLM Systems 

**Title (ZH)**: 边缘-云大语言模型系统中提示安全与系统性能的联合优化 

**Authors**: Haiyang Huang, Tianhui Meng, Weijia Jia  

**Link**: [PDF](https://arxiv.org/pdf/2501.18663)  

**Abstract**: Large language models (LLMs) have significantly facilitated human life, and prompt engineering has improved the efficiency of these models. However, recent years have witnessed a rise in prompt engineering-empowered attacks, leading to issues such as privacy leaks, increased latency, and system resource wastage. Though safety fine-tuning based methods with Reinforcement Learning from Human Feedback (RLHF) are proposed to align the LLMs, existing security mechanisms fail to cope with fickle prompt attacks, highlighting the necessity of performing security detection on prompts. In this paper, we jointly consider prompt security, service latency, and system resource optimization in Edge-Cloud LLM (EC-LLM) systems under various prompt attacks. To enhance prompt security, a vector-database-enabled lightweight attack detector is proposed. We formalize the problem of joint prompt detection, latency, and resource optimization into a multi-stage dynamic Bayesian game model. The equilibrium strategy is determined by predicting the number of malicious tasks and updating beliefs at each stage through Bayesian updates. The proposed scheme is evaluated on a real implemented EC-LLM system, and the results demonstrate that our approach offers enhanced security, reduces the service latency for benign users, and decreases system resource consumption compared to state-of-the-art algorithms. 

**Abstract (ZH)**: 大规模语言模型（LLMs）极大地改善了人类生活，而提示工程提高了这些模型的效率。然而，近年来，借助提示工程的攻击有所增加，导致隐私泄露、响应延迟增加和系统资源浪费等问题。尽管提出了基于强化学习从人类反馈进行安全调优（RLHF）的方法来使LLMs更加一致，但现有的安全机制难以应对多变的提示攻击，这突显了在LLM边缘-云系统（EC-LLM）中进行提示攻击检测的必要性。本文在不同的提示攻击下，联合考虑边缘-云LLM系统中的提示安全、服务延迟以及系统资源优化问题。为增强提示安全性，我们提出了一种基于向量数据库的轻量级攻击检测器。我们将联合提示检测、延迟和资源优化的问题形式化为多阶段动态贝叶斯博弈模型。均衡策略通过贝叶斯更新预测恶意任务的数量并更新信念。我们提出的方案已在实际部署的EC-LLM系统中进行了评估，结果显示，我们的方法提供了增强的安全性，降低了良性用户的服务延迟，同时减少了系统资源消耗，优于现有的算法。 

---
# Cogito, ergo sum: A Neurobiologically-Inspired Cognition-Memory-Growth System for Code Generation 

**Title (ZH)**: 我思故我在：一种受神经生物学启发的思维-记忆-成长系统及其在代码生成中的应用 

**Authors**: Yanlong Li, Jindong Li, Qi Wang, Menglin Yang, He Kong, Shengsheng Wang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18653)  

**Abstract**: Large language models based Multi Agent Systems (MAS) have demonstrated promising performance for enhancing the efficiency and accuracy of code generation tasks. However,most existing methods follow a conventional sequence of planning, coding, and debugging,which contradicts the growth-driven nature of human learning process. Additionally,the frequent information interaction between multiple agents inevitably involves high computational costs. In this paper,we propose Cogito,a neurobiologically inspired multi-agent framework to enhance the problem-solving capabilities in code generation tasks with lower cost. Specifically,Cogito adopts a reverse sequence: it first undergoes debugging, then coding,and finally planning. This approach mimics human learning and development,where knowledge is acquired progressively. Accordingly,a hippocampus-like memory module with different functions is designed to work with the pipeline to provide quick retrieval in similar tasks. Through this growth-based learning model,Cogito accumulates knowledge and cognitive skills at each stage,ultimately forming a Super Role an all capable agent to perform the code generation task. Extensive experiments against representative baselines demonstrate the superior performance and efficiency of Cogito. The code is publicly available at this https URL. 

**Abstract (ZH)**: 基于大规模语言模型的多智能体系统（MAS）在提升代码生成任务的效率和准确性方面展现出了令人鼓舞的性能。然而，大多数现有方法遵循传统的规划、编码和调试顺序，这与人类学习过程的成长驱动性质相矛盾。此外，多个智能体之间的频繁信息交互不可避免地带来了高昂的计算成本。在本文中，我们提出了一种名为Cogito的神经生物学启发式多智能体框架，以较低的成本提升代码生成任务中的问题解决能力。具体来说，Cogito采用逆序流程：首先进行调试，然后编码，最后是规划。这种方法模仿了人类的学习和发展过程，在该过程中，知识是逐步获取的。因此，我们设计了一个类似于海马体的记忆模块，它与工作流结合，以提供类似任务的快速检索功能。通过基于成长的学习模型，Cogito在每个阶段积累知识和认知技能，最终形成一个全能的超级智能体来执行代码生成任务。 extensive实验表明，Cogito在代表性的基线方法上具有更优越的性能和效率。代码已在此处公开：[此链接]。 

---
# Fake News Detection After LLM Laundering: Measurement and Explanation 

**Title (ZH)**: 经过大语言模型漂白后的假新闻检测：测量与解释 

**Authors**: Rupak Kumar Das, Jonathan Dodge  

**Link**: [PDF](https://arxiv.org/pdf/2501.18649)  

**Abstract**: With their advanced capabilities, Large Language Models (LLMs) can generate highly convincing and contextually relevant fake news, which can contribute to disseminating misinformation. Though there is much research on fake news detection for human-written text, the field of detecting LLM-generated fake news is still under-explored. This research measures the efficacy of detectors in identifying LLM-paraphrased fake news, in particular, determining whether adding a paraphrase step in the detection pipeline helps or impedes detection. This study contributes: (1) Detectors struggle to detect LLM-paraphrased fake news more than human-written text, (2) We find which models excel at which tasks (evading detection, paraphrasing to evade detection, and paraphrasing for semantic similarity). (3) Via LIME explanations, we discovered a possible reason for detection failures: sentiment shift. (4) We discover a worrisome trend for paraphrase quality measurement: samples that exhibit sentiment shift despite a high BERTSCORE. (5) We provide a pair of datasets augmenting existing datasets with paraphrase outputs and scores. The dataset is available on GitHub 

**Abstract (ZH)**: 借助其先进的能力，大规模语言模型（LLMs）可以生成高度可信且上下文相关的人造新闻，这可能导致传播虚假信息。尽管已经有许多关于检测人类撰写的虚假新闻的研究，但在检测LLM生成的虚假新闻方面，该领域仍有待进一步探索。本研究评估了检测器在识别LLM改写虚假新闻方面的有效性，特别是在确定检测管道中加入改写步骤是否有助于或妨碍检测方面提供了见解。本研究的主要贡献包括：（1）检测器在识别LLM改写虚假新闻方面要比识别人类撰写的文本困难得多，（2）我们发现哪些模型在哪些任务上表现优异（避开检测、改写以避开检测以及为语义相似性改写），（3）通过LIME解释，我们发现检测失败的一个可能原因：情感偏移，（4）我们发现改写质量测量的一个令人担忧的趋势：即使具有高BERTScore，仍表现出情感偏移的样本比例较大，（5）我们提供了一组数据集，将改写输出及其分数添加到现有数据集中。该数据集可在GitHub上获取。 

---
# Layered Chain-of-Thought Prompting for Multi-Agent LLM Systems: A Comprehensive Approach to Explainable Large Language Models 

**Title (ZH)**: 多agent大型语言模型系统的分层链式推理提示：一种全面的可解释大型语言模型方法 

**Authors**: Manish Sanwal  

**Link**: [PDF](https://arxiv.org/pdf/2501.18645)  

**Abstract**: Large Language Models (LLMs) leverage chain-of-thought (CoT) prompting to provide step-by-step rationales, improving performance on complex tasks. Despite its benefits, vanilla CoT often fails to fully verify intermediate inferences and can produce misleading explanations. In this work, we propose Layered Chain-of-Thought (Layered-CoT) Prompting, a novel framework that systematically segments the reasoning process into multiple layers, each subjected to external checks and optional user feedback. We expand on the key concepts, present three scenarios -- medical triage, financial risk assessment, and agile engineering -- and demonstrate how Layered-CoT surpasses vanilla CoT in terms of transparency, correctness, and user engagement. By integrating references from recent arXiv papers on interactive explainability, multi-agent frameworks, and agent-based collaboration, we illustrate how Layered-CoT paves the way for more reliable and grounded explanations in high-stakes domains. 

**Abstract (ZH)**: 大型语言模型（LLMs）利用链式思维（CoT）提示来提供逐步推理，从而在复杂任务上的表现得到提升。尽管具有这些优势，传统的CoT往往不能充分验证中间推断，且可能生成误导性的解释。在此项工作中，我们提出了分层链式思维（Layered-CoT）提示，这是一种新颖的框架，系统地将推理过程划分为多个层次，并且每个层次都接受外部检查和可选用户反馈。我们详细阐述了关键概念，并提出了三个应用场景——医疗急救、金融风险评估和敏捷工程，展示了Layered-CoT在透明度、正确性和用户参与度方面如何超越传统的CoT。通过整合最近arXiv论文中关于交互式可解释性、多智能体框架和基于代理的合作的参考材料，我们展示了如何通过Layered-CoT在高风险领域为更可靠和具体的解释铺平道路。 

---
# 3D Reconstruction of Shoes for Augmented Reality 

**Title (ZH)**: 增强现实中的鞋类三维重建 

**Authors**: Pratik Shrestha, Sujan Kapali, Swikar Gautam, Vishal Pokharel, Santosh Giri  

**Link**: [PDF](https://arxiv.org/pdf/2501.18643)  

**Abstract**: This paper introduces a mobile-based solution that enhances online shoe shopping through 3D modeling and Augmented Reality (AR), leveraging the efficiency of 3D Gaussian Splatting. Addressing the limitations of static 2D images, the framework generates realistic 3D shoe models from 2D images, achieving an average Peak Signal-to-Noise Ratio (PSNR) of 0.32, and enables immersive AR interactions via smartphones. A custom shoe segmentation dataset of 3120 images was created, with the best-performing segmentation model achieving an Intersection over Union (IoU) score of 0.95. This paper demonstrates the potential of 3D modeling and AR to revolutionize online shopping by offering realistic virtual interactions, with applicability across broader fashion categories. 

**Abstract (ZH)**: 本文介绍了一种基于移动设备的解决方案，通过3D建模和增强现实（AR）来提升在线鞋类购物体验，利用3D高斯斑点图的优势提高效率。该框架能够从2D图像中生成逼真的3D鞋类模型，平均峰值信噪比（PSNR）达到0.32，并通过智能手机实现沉浸式的AR交互。我们创建了一个包含3120张图像的自定义鞋类分割数据集，最佳分割模型的交并比（IoU）为0.95。本文展示了3D建模和AR在提升在线购物体验方面的潜在能力，可通过提供逼真的虚拟交互来革新在线购物，其应用范围不仅限于鞋类，还可扩展到更广泛的时尚品类。 

---
# DebiasPI: Inference-time Debiasing by Prompt Iteration of a Text-to-Image Generative Model 

**Title (ZH)**: DebiasPI：文本到图像生成模型的提示迭代推理时去偏差化方法 

**Authors**: Sarah Bonna, Yu-Cheng Huang, Ekaterina Novozhilova, Sejin Paik, Zhengyang Shan, Michelle Yilin Feng, Ge Gao, Yonish Tayal, Rushil Kulkarni, Jialin Yu, Nupur Divekar, Deepti Ghadiyaram, Derry Wijaya, Margrit Betke  

**Link**: [PDF](https://arxiv.org/pdf/2501.18642)  

**Abstract**: Ethical intervention prompting has emerged as a tool to counter demographic biases of text-to-image generative AI models. Existing solutions either require to retrain the model or struggle to generate images that reflect desired distributions on gender and race. We propose an inference-time process called DebiasPI for Debiasing-by-Prompt-Iteration that provides prompt intervention by enabling the user to control the distributions of individuals' demographic attributes in image generation. DebiasPI keeps track of which attributes have been generated either by probing the internal state of the model or by using external attribute classifiers. Its control loop guides the text-to-image model to select not yet sufficiently represented attributes, With DebiasPI, we were able to create images with equal representations of race and gender that visualize challenging concepts of news headlines. We also experimented with the attributes age, body type, profession, and skin tone, and measured how attributes change when our intervention prompt targets the distribution of an unrelated attribute type. We found, for example, if the text-to-image model is asked to balance racial representation, gender representation improves but the skin tone becomes less diverse. Attempts to cover a wide range of skin colors with various intervention prompts showed that the model struggles to generate the palest skin tones. We conducted various ablation studies, in which we removed DebiasPI's attribute control, that reveal the model's propensity to generate young, male characters. It sometimes visualized career success by generating two-panel images with a pre-success dark-skinned person becoming light-skinned with success, or switching gender from pre-success female to post-success male, thus further motivating ethical intervention prompting with DebiasPI. 

**Abstract (ZH)**: 伦理干预提示技术已发展成为一种工具，用于对抗文本到图像生成AI模型中的群体统计偏差。现有解决方案要么需要重新训练模型，要么难以生成反映性别和种族期望分布的图像。我们提出了一个名为DebiasPI（基于提示迭代去偏）的推理时过程，通过让用户控制图像生成中个体人口统计属性的分布来提供提示干预。DebiasPI 会跟踪哪些属性已被生成，这可以通过探测模型的内部状态或使用外部属性分类器来实现。其控制循环引导文本到图像模型选择尚未足够代表的属性。借助DebiasPI，我们能够创建种族和性别平等代表的图像，以可视化新闻标题中的复杂概念。我们还尝试了年龄、体型、职业和肤色等属性，并测量了当我们的干预提示针对无关属性类型分布时属性的变化情况。例如，如果要求文本到图像模型平衡种族代表性，则性别代表性会有所提高，但肤色的多样性会减少。通过各种干预提示尝试涵盖广泛的肤色时，我们发现模型难以生成最浅的肤色。我们进行了多种消融研究，在这些研究中，我们移除了DebiasPI的属性控制功能，从而揭示了模型倾向于生成年轻男性形象的倾向。有时，模型通过生成两图对比图像来可视化职业成功，例如，前期较深肤色的人在成功后变得肤色较浅，或者从前期女性角色转变为后期男性角色，从而进一步凸显了使用DebiasPI进行伦理干预提示的重要性。 

---
# Graph of Attacks with Pruning: Optimizing Stealthy Jailbreak Prompt Generation for Enhanced LLM Content Moderation 

**Title (ZH)**: 剪枝攻击图：优化隐秘型 Jailbreak 提示生成以增强大语言模型内容审核 

**Authors**: Daniel Schwartz, Dmitriy Bespalov, Zhe Wang, Ninad Kulkarni, Yanjun Qi  

**Link**: [PDF](https://arxiv.org/pdf/2501.18638)  

**Abstract**: We present a modular pipeline that automates the generation of stealthy jailbreak prompts derived from high-level content policies, enhancing LLM content moderation. First, we address query inefficiency and jailbreak strength by developing Graph of Attacks with Pruning (GAP), a method that utilizes strategies from prior jailbreaks, resulting in 92% attack success rate on GPT-3.5 using only 54% of the queries of the prior algorithm. Second, we address the cold-start issue by automatically generating seed prompts from the high-level policy using LLMs. Finally, we demonstrate the utility of these generated jailbreak prompts of improving content moderation by fine-tuning PromptGuard, a model trained to detect jailbreaks, increasing its accuracy on the Toxic-Chat dataset from 5.1% to 93.89%. 

**Abstract (ZH)**: 我们提出了一种模块化的流水线，能够自动化生成源自高级内容政策的隐蔽型 Jailbreak 提示，从而增强大语言模型 (LLM) 内容管理。首先，我们通过开发一种名为 Graph of Attacks with Pruning (GAP) 的方法来解决查询效率低下和 Jailbreak 强度问题，该方法利用了先前 Jailbreak 的策略，在仅使用前一个算法 54% 查询量的情况下，获得了 GPT-3.5 上 92% 的攻击成功率。其次，我们通过使用大语言模型自动从高级策略中生成种子提示来解决冷启动问题。最后，我们展示了这些生成的 Jailbreak 提示在通过微调 PromptGuard 模型改进内容管理方面的实用性，PromptGuard 是一种用于检测 Jailbreak 的模型，将其在 Toxic-Chat 数据集上的准确率从 5.1% 提高到了 93.89%。 

---
# SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model 

**Title (ZH)**: SafeRAG：检索增强生成中大型语言模型安全性基准测试 

**Authors**: Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jason Zhaoxin Fan, Bo Tang, Shichao Song, Mengwei Wang, Jiawei Yang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18636)  

**Abstract**: The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: this https URL. 

**Abstract (ZH)**: 检索-增强生成（RAG）的索引-检索-生成范式在通过将外部知识整合到大型语言模型（LLMs）中解决知识密集型任务方面取得了巨大成功。然而，外部未经验证知识的引入增加了LLMs的脆弱性，因为攻击者可以通过操纵知识来执行攻击任务。本文旨在介绍一个名为SafeRAG的基准测试，用于评估RAG的安全性。首先，我们将攻击任务分为银噪声、跨上下文冲突、软广告和白道服务拒绝（DoS）。接着，我们主要通过手动方式为每个任务构建了一个RAG安全性评估数据集（即SafeRAG数据集）。然后，我们利用SafeRAG数据集模拟RAG可能遇到的各种攻击场景。对14个代表性RAG组件进行的实验表明，RAG对所有攻击任务都具有明显的脆弱性，甚至最明显的攻击任务也能轻易绕过现有的检索器、过滤器或先进的LLMs，导致RAG服务质量下降。相关代码可在此处获取：this https URL。 

---
# Indiana Jones: There Are Always Some Useful Ancient Relics 

**Title (ZH)**: 印第安纳琼斯：总是有一些有用的古代遗迹 

**Authors**: Junchen Ding, Jiahao Zhang, Yi Liu, Ziqi Ding, Gelei Deng, Yuekang Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.18628)  

**Abstract**: This paper introduces Indiana Jones, an innovative approach to jailbreaking Large Language Models (LLMs) by leveraging inter-model dialogues and keyword-driven prompts. Through orchestrating interactions among three specialised LLMs, the method achieves near-perfect success rates in bypassing content safeguards in both white-box and black-box LLMs. The research exposes systemic vulnerabilities within contemporary models, particularly their susceptibility to producing harmful or unethical outputs when guided by ostensibly innocuous prompts framed in historical or contextual contexts. Experimental evaluations highlight the efficacy and adaptability of Indiana Jones, demonstrating its superiority over existing jailbreak methods. These findings emphasise the urgent need for enhanced ethical safeguards and robust security measures in the development of LLMs. Moreover, this work provides a critical foundation for future studies aimed at fortifying LLMs against adversarial exploitation while preserving their utility and flexibility. 

**Abstract (ZH)**: 本文介绍了一种创新方法——Indiana Jones，该方法通过利用模型间对话和关键词驱动的提示进行大语言模型（LLM）的越狱。通过协调三个专门化LLM之间的交互，该方法在白色盒模型和黑色盒模型中绕过内容防护措施方面实现了近乎完美的成功率。研究揭示了当前模型中的系统性漏洞，尤其是它们在受到看似无害但具有历史或情境框架的提示引导时，生成有害或不道德输出的脆弱性。实验评估突显了Indiana Jones的有效性和适应性，证明了它在现有越狱方法中的优越性。这些发现强调了在LLM开发过程中加强伦理防护和安全措施的迫切需求。此外，本文为未来旨在增强LLM对抗敌对手法攻击的研究提供了关键基础，同时保持其实用性和灵活性。 

---
# The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt Adversarial Attacks on LLMs 

**Title (ZH)**: 冰山一角：揭示一种隐藏的基于提示的任务对抗性攻击类别在大规模语言模型中 

**Authors**: Sergey Berezin, Reza Farahbakhsh, Noel Crespi  

**Link**: [PDF](https://arxiv.org/pdf/2501.18626)  

**Abstract**: We present a novel class of jailbreak adversarial attacks on LLMs, termed Task-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks (e.g., cipher decoding, riddles, code execution) into the model's prompt to indirectly generate prohibited inputs. To systematically assess the effectiveness of these attacks, we introduce the PHRYGE benchmark. We demonstrate that our techniques successfully circumvent safeguards in six state-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings highlight critical weaknesses in current LLM safety alignments and underscore the urgent need for more sophisticated defence strategies.
Warning: this paper contains examples of unethical inquiries used solely for research purposes. 

**Abstract (ZH)**: 我们提出了一类针对大规模语言模型（LLM）的新颖类 jailbreak 恶意攻击方法，称为任务在提示中（Task-in-Prompt, TIP）攻击。我们的方法通过将序列到序列的任务（例如，密码解码、谜语、代码执行）嵌入模型的提示中，间接生成被禁止的输入。为了系统地评估这些攻击的有效性，我们引入了 PHRYGE 基准。研究表明，我们的技术能够成功规避六种最先进的语言模型的安全保护措施，包括 GPT-4o 和 LLaMA 3.2。我们的发现强调了当前 LLM 安全对齐中的关键弱点，并突显了需要更为复杂的防御策略的紧迫性。
请注意：本文包含仅供研究目的使用的不道德询问示例。 

---
# Membership Inference Attacks Against Vision-Language Models 

**Title (ZH)**: 针对视觉-语言模型的成员推理攻击 

**Authors**: Yuke Hu, Zheng Li, Zhihao Liu, Yang Zhang, Zhan Qin, Kui Ren, Chun Chen  

**Link**: [PDF](https://arxiv.org/pdf/2501.18624)  

**Abstract**: Vision-Language Models (VLMs), built on pre-trained vision encoders and large language models (LLMs), have shown exceptional multi-modal understanding and dialog capabilities, positioning them as catalysts for the next technological revolution. However, while most VLM research focuses on enhancing multi-modal interaction, the risks of data misuse and leakage have been largely unexplored. This prompts the need for a comprehensive investigation of such risks in VLMs. In this paper, we conduct the first analysis of misuse and leakage detection in VLMs through the lens of membership inference attack (MIA). In specific, we focus on the instruction tuning data of VLMs, which is more likely to contain sensitive or unauthorized information. To address the limitation of existing MIA methods, we introduce a novel approach that infers membership based on a set of samples and their sensitivity to temperature, a unique parameter in VLMs. Based on this, we propose four membership inference methods, each tailored to different levels of background knowledge, ultimately arriving at the most challenging scenario. Our comprehensive evaluations show that these methods can accurately determine membership status, e.g., achieving an AUC greater than 0.8 targeting a small set consisting of only 5 samples on LLaVA. 

**Abstract (ZH)**: 基于预训练视觉编码器和大型语言模型（LLMs）构建的多模态视觉语言模型（VLMs）已经显示出卓越的多模态理解和对话能力，使其成为了下一次技术革命的催化剂。然而，尽管大多数VLM研究侧重于增强多模态交互，但数据滥用和泄露的风险却很少被探讨。这促使我们需要对VLM中的这些风险进行全面调查。在本文中，我们首次通过成员推理攻击（MIA）的角度分析了VLM中的滥用和泄露检测。具体而言，我们关注的是VLM的指令调优数据，这些数据更有可能包含敏感或未经授权的信息。为了解决现有MIA方法的局限性，我们提出了一个新型方法，通过一组样本及其对温度参数的敏感性来推断成员身份，温度是VLM中一个独有的参数。基于此，我们提出了四种成员推理方法，每种方法针对不同类型的知识背景进行设计，最终达到最具挑战性的场景。我们的全面评估表明，这些方法能够准确确定成员身份状态，例如，在LLaVA数据集上仅包含5个样本的小集合中，这些方法能够达到AUC大于0.8的效果。 

---
# STAMP: Scalable Task And Model-agnostic Collaborative Perception 

**Title (ZH)**: STAMP：可扩展的任务和模型无关协作感知 

**Authors**: Xiangbo Gao, Runsheng Xu, Jiachen Li, Ziran Wang, Zhiwen Fan, Zhengzhong Tu  

**Link**: [PDF](https://arxiv.org/pdf/2501.18616)  

**Abstract**: Perception is crucial for autonomous driving, but single-agent perception is often constrained by sensors' physical limitations, leading to degraded performance under severe occlusion, adverse weather conditions, and when detecting distant objects. Multi-agent collaborative perception offers a solution, yet challenges arise when integrating heterogeneous agents with varying model architectures. To address these challenges, we propose STAMP, a scalable task- and model-agnostic, collaborative perception pipeline for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific and shared protocol domains, enabling efficient feature sharing and fusion. This approach minimizes computational overhead, enhances scalability, and preserves model security. Experiments on simulated and real-world datasets demonstrate STAMP's comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As a first-of-its-kind task- and model-agnostic framework, STAMP aims to advance research in scalable and secure mobility systems towards Level 5 autonomy. Our project page is at this https URL and the code is available at this https URL. 

**Abstract (ZH)**: 感知在自主驾驶中至关重要，但单个代理的感知常常受到传感器物理限制的制约，在严重遮挡、恶劣天气条件以及检测远距离目标时，其性能会下降。多代理协作感知提供了解决方案，但在集成具有不同模型架构的异构代理时，存在挑战。为应对这些挑战，我们提出了一种名为STAMP的新框架，它是一种可扩展、任务和模型无关的协作感知管道，适用于异构代理。STAMP利用轻量级的适配器-反向器对，在特定代理领域和共享协议域之间转换鸟瞰视图(BEV)特征，从而实现高效的特征共享和融合。这种方法减少了计算开销、增强了可扩展性，并确保了模型安全。实验表明，STAMP在模拟数据集和真实世界数据集上的准确度与最先进的模型相当或更优，且计算成本显著降低。作为首款任务和模型无关的框架，STAMP旨在推进可扩展和安全的移动系统研究，朝着Level 5自主驾驶迈进。我们的项目页面可以访问此处[this https URL]，代码可以在此处[this https URL]获取。 

---
# Review and Recommendations for using Artificial Intelligence in Intracoronary Optical Coherence Tomography Analysis 

**Title (ZH)**: 关于使用人工智能进行冠状动脉光学相干断层成像分析的复习与建议 

**Authors**: Xu Chen, Yuan Huang, Benn Jessney, Jason Sangha, Sophie Gu, Carola-Bibiane Schönlieb, Martin Bennett, Michael Roberts  

**Link**: [PDF](https://arxiv.org/pdf/2501.18614)  

**Abstract**: Artificial intelligence (AI) methodologies hold great promise for the rapid and accurate diagnosis of coronary artery disease (CAD) from intravascular optical coherent tomography (IVOCT) images. Numerous papers have been published describing AI-based models for different diagnostic tasks, yet it remains unclear which models have potential clinical utility and have been properly validated. This systematic review considered published literature between January 2015 and February 2023 describing AI-based diagnosis of CAD using IVOCT. Our search identified 5,576 studies, with 513 included after initial screening and 35 studies included in the final systematic review after quality screening. Our findings indicate that most of the identified models are not currently suitable for clinical use, primarily due to methodological flaws and underlying biases. To address these issues, we provide recommendations to improve model quality and research practices to enhance the development of clinically useful AI products. 

**Abstract (ZH)**: 人工智能（AI）方法在从冠状动脉内光学相干断层扫描（IVOCT）图像快速准确诊断冠状动脉疾病（CAD）方面前景广阔。大量论文描述了不同诊断任务的基于AI的模型，但尚不清楚哪些模型具有临床应用潜力且得到了充分验证。本系统综述研究了2015年1月至2023年2月期间发表的关于使用IVOCT进行CAD人工智能诊断的相关文献。我们的搜索共识别出5,576篇研究，经过初步筛选后有513篇被包括，最终在质量筛选后有35项研究被纳入系统综述。我们的研究结果表明，大多数识别出的模型目前不适用于临床使用，主要原因是方法学缺陷和潜在的偏见。为解决这些问题，我们提出了改善模型质量并提高研究实践的建议，以促进开发出具有临床应用价值的人工智能产品。 

---
# Deeply Optimizing the SAT Solver for the IC3 Algorithm 

**Title (ZH)**: 针对IC3算法深度优化的SAT求解器 

**Authors**: Yuheng Su, Qiusong Yang, Yiwei Ci, Yingcheng Li, Tianjun Bu, Ziyu Huang  

**Link**: [PDF](https://arxiv.org/pdf/2501.18612)  

**Abstract**: The IC3 algorithm, also known as PDR, is a SAT-based model checking algorithm that has significantly influenced the field in recent years due to its efficiency, scalability, and completeness. It utilizes SAT solvers to solve a series of SAT queries associated with relative induction. Based on our observations of the unique characteristics of SAT queries in IC3, this paper introduces GipSAT, a lightweight SAT solver specifically optimized for IC3. By observing that SAT queries do not necessarily require decisions on all variables, GipSAT calculates a subset of variables that need to be decided before each solving, while ensuring that the result remains unaffected. By observing that the overhead of binary heap operations in VSIDS is not negligible, GipSAT utilizes buckets instead of binary heap to achieve constant-time operations. GipSAT supports temporary clauses without the need to allocate a new activation variable before each solving, thus eliminating the need to reset solvers. The comprehensive evaluation demonstrates a significant performance improvement achieved by GipSAT. When compared to Minisat, GipSAT achieves an average speedup of 3.61 times in solving time. 

**Abstract (ZH)**: IC3算法，也称为PDR（Progressive Debugging and Refinement），是一种基于SAT求解器的模型检查算法，近年来由于其效率、可扩展性和完整性，在该领域产生了重大影响。该算法利用SAT求解器解决与相对归纳相关的系列SAT查询。基于对IC3中SAT查询特性的观察，本文介绍了一种专为IC3优化的轻量级SAT求解器GipSAT。通过观察到SAT查询并不一定需要在每个变量上做出决策，GipSAT在每次求解前计算出需要做出决策的变量子集，同时确保结果不受影响。观察到VSIDS中的二元堆操作开销不可忽视，GipSAT使用桶结构而非二元堆来实现常量时间操作。GipSAT在每次求解前不需要分配新的激活变量来支持临时子句，从而避免了重新初始化求解器的需求。全面的评估表明，GipSAT实现了显著的性能提升。与Minisat相比，在求解时间上，GipSAT平均加速了3.61倍。 

---
# Faster Configuration Performance Bug Testing with Neural Dual-level Prioritization 

**Title (ZH)**: 基于神经网络的双层优先级测试加速配置性能错误检测 

**Authors**: Youpeng Ma, Tao Chen, Ke Li  

**Link**: [PDF](https://arxiv.org/pdf/2501.15392)  

**Abstract**: As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle. In this paper, we seek to achieve significantly faster CPBug testing by neurally prioritizing the testing at both the configuration option and value range levels with automated oracle estimation. Our proposed tool, dubbed NDP, is a general framework that works with different heuristic generators. The idea is to leverage two neural language models: one to estimate the CPBug types that serve as the oracle while, more vitally, the other to infer the probabilities of an option being CPBug-related, based on which the options and the value ranges to be searched can be prioritized. Experiments on several widely-used systems of different versions reveal that NDP can, in general, better predict CPBug type in 87% cases and find more CPBugs with up to 88.88x testing efficiency speedup over the state-of-the-art tools. 

**Abstract (ZH)**: 随着软件系统变得日益复杂和可配置，更多性能问题往往源自配置设计。这导致一些配置选项的性能意外地下降，这与开发人员最初的设计预期不符。这种不一致被称为配置性能漏洞（CPBugs），它们可能深深地隐藏在源代码中。然而，高效地测试CPBugs是困难的，不仅因为测试结果难以确定，而且因为配置测量成本高昂，且可能存在的配置组合数量过于庞大。因此，现有的测试工具要么运行时间很长，要么在预算有限的情况下对CPBugs检测不力，且测试 oracle 的不准确性进一步加剧了问题。本文旨在通过神经网络在配置选项和值范围级别上自动优先级排序来显著加快CPBugs的测试，并使用 oracle 估计。我们提出的工具称为NDP，是一个通用框架，可以与不同的启发式生成器兼容。这一想法是利用两个神经语言模型：一个是估计 CPBugs 类型的 oracle，更重要的是，另一个则是根据选项是否与 CPBugs 相关的概率来推断这些选项及其值范围需要优先测试。实验结果显示，NDP 在 87% 的情况下一般能更好地预测 CPBugs 类型，并且相对于最先进的工具，发现更多 CPBugs 时，测试效率可提高 88.88 倍。 

---
