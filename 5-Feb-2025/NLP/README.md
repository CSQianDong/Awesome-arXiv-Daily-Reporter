# Spatio-temporal transformer to support automatic sign language translation 

**Title (ZH)**: 时空变换器以支持自动手语翻译 

**Authors**: Christian Ruiz, Fabio Martinez  

**Link**: [PDF](https://arxiv.org/pdf/2502.02587)  

**Abstract**: Sign Language Translation (SLT) systems support hearing-impaired people communication by finding equivalences between signed and spoken languages. This task is however challenging due to multiple sign variations, complexity in language and inherent richness of expressions. Computational approaches have evidenced capabilities to support SLT. Nonetheless, these approaches remain limited to cover gestures variability and support long sequence translations. This paper introduces a Transformer-based architecture that encodes spatio-temporal motion gestures, preserving both local and long-range spatial information through the use of multiple convolutional and attention mechanisms. The proposed approach was validated on the Colombian Sign Language Translation Dataset (CoL-SLTD) outperforming baseline approaches, and achieving a BLEU4 of 46.84%. Additionally, the proposed approach was validated on the RWTH-PHOENIX-Weather-2014T (PHOENIX14T), achieving a BLEU4 score of 30.77%, demonstrating its robustness and effectiveness in handling real-world variations 

**Abstract (ZH)**: 手语翻译（SLT）系统通过在手语和口语之间寻找等效关系，支持听力障碍人士的沟通。然而，由于存在多种手语变体、语言的复杂性以及表达的丰富性，这一任务极具挑战性。计算方法已经证明了支持SLT的能力，但仍受限于对手势变化的覆盖范围和长序列翻译的支持。本文介绍了一种基于Transformer的架构，该架构通过多层卷积和注意力机制编码空间-时间运动手势，同时保留局部和远程的空间信息。在哥伦比亚手语翻译数据集（CoL-SLTD）上的实验证明了该方法优于基线方法，并实现了46.84%的BLEU4得分。此外，该方法还在RWTH-PHOENIX-Weather-2014T（PHOENIX14T）数据集上进行了验证，实现了30.77%的BLEU4得分，展示了其在处理实际世界变异性方面的稳健性和有效性。 

---
# A comparison of translation performance between DeepL and Supertext 

**Title (ZH)**: DeepL与Supertext的翻译性能比较 

**Authors**: Alex Flückiger, Chantal Amrhein, Tim Graf, Philippe Schläpfer, Florian Schottmann, Samuel Läubli  

**Link**: [PDF](https://arxiv.org/pdf/2502.02577)  

**Abstract**: As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at this https URL. 

**Abstract (ZH)**: 随着强大的机器翻译（MT）系统越来越多地基于大规模语言模型（LLMs），可靠的质量基准化需要能够捕捉它们利用扩展上下文能力的方法。本研究通过评估未分段文本的性能，比较了两种商业MT系统——DeepL和Supertext。我们利用专业译者的评分，对包含全文水平上下文的各个片段进行了多语言方向的翻译质量评估。尽管在大多数情况下，段落级别的评估显示出对两种系统之间没有明显偏好的结果，但在四个语言方向中的三个方面，文档级别的分析显示用户更偏好Supertext，这表明在较长文本中具有更好的一致性。我们提倡采用更多上下文敏感的评估方法，以确保MT质量评估能够反映实际使用情况。我们在此提供所有评估数据和脚本用于进一步分析和复制：[https://example.com](https://example.com)。 

---
# Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement 

**Title (ZH)**: 语言模型能否胜任顺序优化问题？从评估到黑格尔启发式的增强 

**Authors**: Soheil Abbasloo  

**Link**: [PDF](https://arxiv.org/pdf/2502.02573)  

**Abstract**: Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经在众多领域展现了令人印象深刻的性能，为优化问题求解这一关键且普遍复杂的领域带来了革命的机会。本文探讨了LLMs在处理序列优化问题（SOPs）方面的能力。我们引入了WorldGen框架，这是一种生成不可见的SOPs且具有可控复杂性的动态框架，用以评估LLMs的性能。初始观察结果显示，尽管LLMs在简单SOPs上表现良好，但随着复杂性的增加，其性能显著下降。受此启发，我们重新审视了推理方面的哲学假设，以提高LLMs的性能。受黑格尔辩证法框架的启发，我们提出了ACE方法，证明了在SOP上下文中，可以显著提高LLMs的性能，而无需重新训练或进一步微调。 

---
# Adaptive Self-improvement LLM Agentic System for ML Library Development 

**Title (ZH)**: 面向机器学习库开发的自适应自我改进代理系统 

**Authors**: Genghan Zhang, Weixin Liang, Olivia Hsu, Kunle Olukotun  

**Link**: [PDF](https://arxiv.org/pdf/2502.02534)  

**Abstract**: ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\times$ over a baseline single LLM. 

**Abstract (ZH)**: 以下是对论文内容或标题的翻译，符合学术规范：

机器学习（ML）库通常用针对特定架构的编程语言（ASPLs，Architecture-specific Programming Languages）编写，是高效ML系统的关键。然而，编写这些高性能ML库具有挑战性，因为这需要对ML算法和ASPL有深刻的专业知识。相比之下，大规模语言模型（LLMs）已经展示了普遍的编码能力。然而，使用LLMs生成基于ASPL的ML库仍然存在挑战，原因在于：1）即使对于经验丰富的程序员，这项任务也非常复杂；2）由于ASPLs的专有性和不断演变性，可用的代码示例较少。因此，LLMs需要在有限的数据下进行复杂的推理以完成这一任务。为了应对这些挑战，我们提出了一种自适应自我改进的代理系统。为了评估该系统的有效性，我们基于一个典型的ML库构建了一个基准测试，并使用开源和闭源的LLMs在此基准测试上生成了ASPL代码。我们的结果显示，与单一LLM基线相比，改进幅度最高可达$3.9\times$。 

---
# Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search 

**Title (ZH)**: 悟性：基于链式行动思考的增强学习增强大语言模型推理能力的研究 

**Authors**: Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan  

**Link**: [PDF](https://arxiv.org/pdf/2502.02508)  

**Abstract**: Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced. 

**Abstract (ZH)**: 大型语言模型（LLMs）已经在多种领域中展现了卓越的推理能力。近期的研究表明，增加推理时的计算量可以进一步提升LLMs的推理能力。这通常涉及到在推理阶段由外部LLM验证器引导的大量采样，形成一个两者的系统。尽管有外部指导，该系统的有效性证明了一个单一LLM有能力处理复杂任务。因此，我们提出一个新的研究问题：我们是否可以将搜索能力内嵌到单一LLM中，从根本上提升其推理能力？本文探讨了一个与传统方法不同的方向，针对自回归搜索后训练LLMs（即包含自我反思和探索新策略的扩展推理过程）。为实现这一目标，我们提出了Action-Thought Chain (COAT) 推理，并采用两阶段训练方法：1）小型格式调优阶段，以内嵌COAT推理格式；2）大规模自我改进阶段，利用强化学习机制。我们的方法训练出了Satori，这是一个基于开源模型和数据训练的拥有7B参数的LLMs。广泛的经验评估表明，Satori在数学推理基准测试中取得了最先进的性能，并且在域外任务上表现出强大的泛化能力。我们将提供完整的代码、数据和模型。 

---
# Multilingual Machine Translation with Open Large Language Models at Practical Scale: An Empirical Study 

**Title (ZH)**: 大规模开放语言模型在实际规模下的多语言机器翻译：一项 Empirical 研究

注释：在翻译学术论文的标题或内容时，通常需要保持标题的简洁性和专业性，同时确保翻译的准确性和规范性。这里“Empirical Study”被翻译为“研究”，其中“Empirical”在学术语境中通常指的是基于具体事实和数据的实证研究，这里的翻译保持了这一含义。此外，“Multilingual Machine Translation”翻译为“多语言机器翻译”，“Open Large Language Models”翻译为“大规模开放语言模型”，这样的翻译既符合学术规范，也易于学术界读者理解和接受。 

**Authors**: Menglong Cui, Pengzhi Gao, Wei Liu, Jian Luan, BinWang  

**Link**: [PDF](https://arxiv.org/pdf/2502.02481)  

**Abstract**: Large language models (LLMs) have shown continuously improving multilingual capabilities, and even small-scale open-source models have demonstrated rapid performance enhancement. In this paper, we systematically explore the abilities of open LLMs with less than ten billion parameters to handle multilingual machine translation (MT) tasks. We conduct comprehensive evaluations on six popular LLMs and find that models like Gemma2-9B exhibit impressive multilingual translation capabilities. We then introduce the Parallel-First Monolingual-Second (PFMS) data mixing strategy in the continual pretraining stage to further enhance the MT performance and present GemmaX2-28, a 9B model achieving top-tier multilingual translation performance across 28 languages. Specifically, GemmaX2-28 consistently outperforms the state-of-the-art (SOTA) models such as TowerInstruct and XALMA and achieves competitive performance with Google Translate and GPT-4-turbo. 

**Abstract (ZH)**: 大型语言模型（LLMs）在多语种能力方面表现出不断提升，即使是小型开源模型也显示出了快速的性能提升。在本文中，我们系统地研究了拥有不到十亿参数的开源LLM在多语种机器翻译（MT）任务中的能力。我们对六种流行的LLM进行了全面评估，并发现Gemma2-9B等模型展示了令人印象深刻的多语种翻译能力。随后，我们在持续预训练阶段引入了一种并行优先单语种第二的（PFMS）数据混合策略，以进一步提高MT性能，并提出了一个同时在28种语言中表现出顶级多语种翻译性能的GemmaX2-28模型。具体而言，GemmaX2-28在多语种翻译任务中持续优于当前最先进的模型（如TowerInstruct和XALMA），并且与Google Translate和GPT-4-turbo相比具有竞争力。 

---
# SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency 

**Title (ZH)**: SAISA：面向高效训练与推理的多模态大型语言模型 

**Authors**: Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun  

**Link**: [PDF](https://arxiv.org/pdf/2502.02458)  

**Abstract**: Multimodal Large Language Models (MLLMs) mainly fall into two architectures, each involving a trade-off between training and inference efficiency: embedding space alignment (e.g., LLaVA-1.5) is inefficient during inference, while cross-attention space alignment (e.g., Flamingo) is inefficient in training. In this paper, we compare these two architectures and identify the key factors for building efficient MLLMs. A primary difference between them lies in how attention is applied to visual tokens, particularly in their interactions with each other. To investigate whether attention among visual tokens is necessary, we propose a new self-attention mechanism, NAAViT (\textbf{N}o \textbf{A}ttention \textbf{A}mong \textbf{Vi}sual \textbf{T}okens), which eliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant. Based on these insights, we introduce SAISA (\textbf{S}elf-\textbf{A}ttention \textbf{I}nput \textbf{S}pace \textbf{A}lignment), a novel architecture that enhance both training and inference efficiency. SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\% and training budget by 26\%, while achieving superior performance in terms of accuracy. Comprehensive ablation studies further validate the effectiveness of SAISA across various LLMs and visual encoders. The code and model will be publicly available at this https URL. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）主要划分为两种架构，每种架构在训练和推理效率之间存在权衡：嵌入空间对齐（例如，LLaVA-1.5）在推理过程中效率较低，而跨注意力空间对齐（例如，Flamingo）在训练过程中效率较低。在本文中，我们比较了这两种架构，并确定了构建高效MLLMs的关键因素。它们之间的一个主要区别在于视觉标记之间如何应用注意力，特别是在它们之间的交互方式上。为了探讨视觉标记之间注意力是否必要，我们提出了一种新的自注意力机制NAAViT（No Attention Among Visual Tokens，视觉标记之间无注意力），该机制消除了这种类型的注意力。在LLaVA-1.5的小规模实验中，我们发现视觉标记之间的注意力是高度冗余的。基于这些洞见，我们引入了SAISA（Self-Attention Input Space Alignment，自注意力输入空间对齐）架构，该架构能够同时提高训练和推理效率。SAISA直接将视觉特征与NAAViT自注意力块的输入空间对齐，从而减少自注意力块和前馈网络（FFNs）中的计算开销。使用与LLaVA-1.5相同的配置，SAISA将推理FLOPs减少了66%，训练预算减少了26%，同时在准确性方面取得了更好的表现。综合消融实验进一步验证了SAISA在各种大型语言模型和视觉编码器上均具有有效性。代码和模型将在以下链接公开：https://github.com/alibaba/Qwen。 

---
# Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study 

**Title (ZH)**: 超越英语：在非英语语境中评估道德基础自动化测量方法——以一项中文案例研究为例 

**Authors**: Calvin Yixiang Cheng, Scott A Hale  

**Link**: [PDF](https://arxiv.org/pdf/2502.02451)  

**Abstract**: This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks. 

**Abstract (ZH)**: 本文探讨了在非英语语料中测量道德基础（MFs）的计算方法。由于大多数资源主要针对英语开发，因此道德基础理论在跨语言应用中的使用仍受到限制。以汉语为例，本文评估了将英语资源应用于机器翻译文本、本地语言词典、多语言语言模型以及大型语言模型（LLMs）在测量非英语文本中道德基础的有效性。研究结果表明，机器翻译和本地词典方法在复杂的道德评估中不足，往往导致大量文化信息的丢失。相比之下，多语言模型和LLMs在迁移学习中表现出可靠的语言间性能，而LLMs在数据效率方面尤为突出。此外，研究还强调了在自动化道德基础评估中需要人工校验的重要性，即使最先进的模型也可能在跨语言测量中忽略文化细微差别。研究结果突显了大型语言模型在跨语言道德基础测量以及复杂多语言演绎编码任务中的潜在应用价值。 

---
# Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models 

**Title (ZH)**: 面向生成的心理语义学方法在大型语言模型中构建价值系统 

**Authors**: Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song  

**Link**: [PDF](https://arxiv.org/pdf/2502.02444)  

**Abstract**: Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values. 

**Abstract (ZH)**: 价值观是影响个体和集体感知、认知和行为的核心驱动力。价值体系，如施瓦茨的基本人类价值观理论，界定了这些价值观的层次结构及相互作用，促进了跨学科对决策和社会动态的研究。近期，大型语言模型（LLMs）的发展引发了对其内在价值观难以捉摸的担忧。尽管在评价、理解和对齐LLM价值观方面做出了不断的努力，但基于心理理论的LLM价值体系仍处于探索阶段。本研究通过引入生成心理词汇方法（GPLA，Generative Psycho-Lexical Approach），填补了这一空白，GPLA是一种可扩展、适应性强且基于理论的方法，用于构建价值体系。利用GPLA，我们提出了一个基于心理理论的五因素价值体系，专门针对LLMs。为了系统验证，我们提出了三项基准任务，将心理原则与前沿的人工智能优先事项相结合。研究结果表明，所提出的值体系符合标准的心理学标准，更准确地捕捉了LLM的价值观，提高了LLM的安全性预测，并增强了LLM的对齐效果，相比之下，优于施瓦茨的标准价值观体系。 

---
# Activation-Informed Merging of Large Language Models 

**Title (ZH)**: 激活导向的大语言模型融合方法 

**Authors**: Amin Heyrani Nobari, Kaveh Alimohammadi, Ali ArjomandBigdeli, Akash Srivastava, Faez Ahmed, Navid Azizan  

**Link**: [PDF](https://arxiv.org/pdf/2502.02421)  

**Abstract**: Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\% increase in benchmark performance. 

**Abstract (ZH)**: 模型合并是一种将多个微调大型语言模型（LLM）的参数和嵌入相结合的方法，它提供了一种在各种任务中增强模型性能的同时保持计算效率的有前途的方法。本文介绍了激活信息引导合并（AIM），这是一种将大型语言模型激活空间中的信息集成到合并过程中的技术，以提高性能和鲁棒性。AIM 是一个灵活且互补的解决方案，适用于任何现有的合并方法。它旨在保留基础模型中的关键权重，借鉴了持续学习（CL）和模型压缩的原则。通过使用一个任务无关的校准集，AIM 有选择地在合并过程中优先考虑关键权重。我们通过实验证明，AIM 显著提高了多个基准测试中合并模型的性能。我们的研究结果表明，考虑激活空间信息可以在 LLM 的模型合并策略中提供重大改进，基准测试性能最多可提高40%。 

---
# FewTopNER: Integrating Few-Shot Learning with Topic Modeling and Named Entity Recognition in a Multilingual Framework 

**Title (ZH)**: FewTopNER：一种结合少量样本学习、主题建模和命名实体识别的多语种框架 

**Authors**: Ibrahim Bouabdallaoui, Fatima Guerouate, Samya Bouhaddour, Chaimae Saadi, Mohammed Sbihi  

**Link**: [PDF](https://arxiv.org/pdf/2502.02391)  

**Abstract**: We introduce FewTopNER, a novel framework that integrates few-shot named entity recognition (NER) with topic-aware contextual modeling to address the challenges of cross-lingual and low-resource scenarios. FewTopNER leverages a shared multilingual encoder based on XLM-RoBERTa, augmented with language-specific calibration mechanisms, to generate robust contextual embeddings. The architecture comprises a prototype-based entity recognition branch, employing BiLSTM and Conditional Random Fields for sequence labeling, and a topic modeling branch that extracts document-level semantic features through hybrid probabilistic and neural methods. A cross-task bridge facilitates dynamic bidirectional attention and feature fusion between entity and topic representations, thereby enhancing entity disambiguation by incorporating global semantic context. Empirical evaluations on multilingual benchmarks across English, French, Spanish, German, and Italian demonstrate that FewTopNER significantly outperforms existing state-of-the-art few-shot NER models. In particular, the framework achieves improvements of 2.5-4.0 percentage points in F1 score and exhibits enhanced topic coherence, as measured by normalized pointwise mutual information. Ablation studies further confirm the critical contributions of the shared encoder and cross-task integration mechanisms to the overall performance. These results underscore the efficacy of incorporating topic-aware context into few-shot NER and highlight the potential of FewTopNER for robust cross-lingual applications in low-resource settings. 

**Abstract (ZH)**: 以下是经过专业翻译且符合学术规范的中文版本：

我们引入了一种新型框架FewTopNER，该框架结合了少样本命名实体识别（NER）和主题感知上下文建模，以解决跨语言和低资源场景中的挑战。FewTopNER 利用基于 XLM-RoBERTa 的共享多语言编码器，并通过语言特定的校准机制进行增强，生成稳健的上下文嵌入。该架构包括基于原型的实体识别分支，使用双向长短期记忆网络（BiLSTM）和条件随机字段（CRF）进行序列标注，以及通过混合概率和神经方法提取文档级语义特征的主题建模分支。跨任务桥梁促进了实体和主题表示之间的动态双向注意和特征融合，从而通过整合全局语义上下文增强实体消岐。在涵盖英语、法语、西班牙语、德语和意大利语的多语言基准测试上的实证评估表明，FewTopNER 显著优于现有的少样本 NER 模型。特别是，该框架在 F1 分数上实现了 2.5-4.0 个百分点的改进，并在规范化点互信息（NPMI）测量的主题一致性方面表现出增强。进一步的消融研究也证实了共享编码器和跨任务整合机制对整体性能的贡献。这些结果强调了将主题感知上下文纳入少样本 NER 的有效性，并突显了FewTopNER 在低资源环境中的稳健跨语言应用的潜力。 

---
# CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning 

**Title (ZH)**: CoAT：增强大规模语言模型推理能力的关联思维链框架 

**Authors**: Jianfeng Pan, Senyou Deng, Shaomang Huang  

**Link**: [PDF](https://arxiv.org/pdf/2502.02390)  

**Abstract**: Research on LLM technologies is rapidly emerging, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results. 

**Abstract (ZH)**: 大规模语言模型（LLM）技术研究正迅速兴起，大多数研究采用“快速思考”方法进行推理。大多数LLM仅基于单个查询和模型的推理能力生成最终结果。然而，随着OpenAI-o1的出现，“慢思考”技术逐渐受到关注，因为其过程更接近人类的思维过程。受到人类不断关联和补充知识的能力启发，我们开发了新颖的连续关联思维（CoAT）框架，该框架引入了蒙特卡洛树搜索（MCTS）算法与一种动态的新关键信息整合机制“关联记忆”的创新结合。通过结合MCTS的结构化探索能力和关联记忆的自适应学习能力，CoAT显著扩大了LLM的搜索空间，使我们的框架能够探索多种推理路径，并实时动态更新其知识库。这不仅允许框架重新审视和改进早期推断，还能适应性地纳入不断变化的信息，确保最终输出既准确又全面。为了验证我们框架的有效性，我们在多种生成性和推理任务上进行了广泛的实验。实验结果表明，与传统的推理过程相比，我们的框架在准确性、连贯性和多样性方面表现更优。框架能够在不断提升其搜索空间的同时保留相关背景信息。 

---
# STAIR: Improving Safety Alignment with Introspective Reasoning 

**Title (ZH)**: STAIR：通过反省推理提高安全性对齐 

**Authors**: Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng Dong, Jun Zhu  

**Link**: [PDF](https://arxiv.org/pdf/2502.02384)  

**Abstract**: Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. Relevant resources in this work are available at this https URL. 

**Abstract (ZH)**: 确保大型语言模型（LLMs）的安全性和无害性已成为与它们在应用中的性能同样重要的问题。然而，现有的安全对齐方法通常会遭受安全性能权衡和对抗囚笼攻击的脆弱性，主要是因为它们依赖于直接拒绝恶意查询。本文提出了一种新型框架STAIR，该框架将安全对齐与内省推理相结合。通过自我提升的逐步分析和带有安全意识的推理链（CoT）来使LLMs能够识别安全风险。STAIR首先增强模型的结构化推理能力，然后通过迭代偏好优化逐步推理数据，这些数据是通过我们新提出的Safety-Informed Monte Carlo Tree Search（SI-MCTS）生成的。我们进一步在这些数据上训练一个过程奖励模型，以指导测试时的搜索，从而改进响应质量。广泛的实验表明，与本能的安全对齐策略相比，STAIR能更有效地减少有害输出，同时更好地保留有用性。在测试时扩展后，STAIR在对抗流行的囚笼攻击方面达到了与Claude-3.5相似的安全性能。本工作中相关的资源可在如下链接获取：[提供的链接]。 

---
# Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs 

**Title (ZH)**: 增强前提的推理链在LLM进行数学推理中的错误识别中效果提升 

**Authors**: Sagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, Dilek Hakkani Tur  

**Link**: [PDF](https://arxiv.org/pdf/2502.02362)  

**Abstract**: Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations. 

**Abstract (ZH)**: chain-of-thought (CoT) 嵌入增强大型语言模型（LLMs）的数学推理能力，通过实现详细的逐步解决方案。然而，由于LLMs的冗长特性，生成的推理链可能变得很长，这使得验证推理步骤变得更加困难，并且难以追踪由于步骤之间依赖关系而可能导致的更远处的问题。重要的是，数学推理允许每个步骤都从之前几步的一个小集合的假设推导出来。在这篇论文中，我们提出了一种框架，用于识别每个步骤的假设，以改进推理的评估。我们将传统的线性推理链重构为前提增强推理链（PARC），通过引入前提链接，形成了一个有向无环图，其中节点是步骤，边是前提链接。通过使用我们构建的基于PARC的数据集PERL（前提和错误识别在LLMs中），我们展示了LLMs可以可靠地在复杂的推理链中识别前提。特别是，即使是开源的LLMs在前提识别中的召回率达到了90%。我们还展示了PARC有助于更可靠地识别推理链中的错误。在有前提的PARC中进行逐步验证时，错误识别的准确性提高了6%到16%。我们的发现突出了以前提为中心的表示方法在解决复杂问题方面的效用，并为提高基于LLMs的推理评估的可靠性打开了新的途径。 

---
# Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking 

**Title (ZH)**: 使用MCTS自动结构化思维增强多模态推理 

**Authors**: Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao  

**Link**: [PDF](https://arxiv.org/pdf/2502.02339)  

**Abstract**: Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\%$) while maintaining substantial data and computational efficiency. 

**Abstract (ZH)**: 多模态大型语言模型（MLLMs）展现了令人印象深刻的能力，但在复杂的视觉推理方面仍然面临挑战。尽管近期的努力试图通过引入类似于OpenAI o1的结构化思考方式，如显式搜索结构或教师引导的蒸馏来增强MLLMs的推理能力，但它们往往难以在性能和效率之间取得平衡。一个关键限制是，这些模型对大量数据和搜索空间的依赖性很强，导致了低效的隐含洞察提取和数据利用。为了解决这一问题，我们提出了AStar，这是一种通过蒙特卡洛树搜索（MCTS）实现自动结构化思考的多模态推理范式。AStar 利用MCTS驱动的分层结构，从有限的数据中自动推导出高级的认知推理模式。基于这些显式的模式，我们设计了一个统一的推理框架，该框架可以无缝地整合模型的内部推理能力与外部推理准则，从而通过最少的树迭代实现高效的推理。这一新的范式在性能和效率之间取得了令人 impressive 的平衡。广泛实验表明，AStar 的有效性得到了验证：它在使用7B参数骨干网络的情况下，在MathVerse基准上达到了54.0%的准确率，超越了GPT-4o（50.2%），同时保持了显著的数据和计算效率。 

---
# Evalita-LLM: Benchmarking Large Language Models on Italian 

**Title (ZH)**: Evalita-LLM：评估大型语言模型在意大利语上的性能 

**Authors**: Bernardo Magnini, Roberto Zanoli, Michele Resta, Martin Cimmino, Paolo Albano, Marco Madeddu, Viviana Patti  

**Link**: [PDF](https://arxiv.org/pdf/2502.02289)  

**Abstract**: We describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs. 

**Abstract (ZH)**: 我们描述了Evalita-LLM，这是一个新的基准，用于在意大利语任务上评估大型语言模型（LLMs）。Evalita-LLM 的主要创新特征如下：（i）所有任务均为地道的意大利语，避免了从意大利语翻译带来的问题和潜在的文化偏见；（ii）除了常用的多项选择任务外，该基准还包含生成任务，从而使与LLMs的交互更加自然；（iii）所有任务都采用多种提示进行评估，从而减轻了模型对特定提示的敏感性，并提供了更为公平和客观的评估。我们提出了一个迭代的方法论，其中候选任务和候选提示通过一组用于开发的LLMs进行验证。我们报告了基准测试开发阶段的实验结果，并提供了几种最先进的LLMs的性能统计数据。 

---
# Conversation AI Dialog for Medicare powered by Finetuning and Retrieval Augmented Generation 

**Title (ZH)**: 由微调和检索增强生成驱动的医保对话AI对话 

**Authors**: Atharva Mangeshkumar Agrawal, Rutika Pandurang Shinde, Vasanth Kumar Bhukya, Ashmita Chakraborty, Sagar Bharat Shah, Tanmay Shukla, Sree Pradeep Kumar Relangi, Nilesh Mutyam  

**Link**: [PDF](https://arxiv.org/pdf/2502.02249)  

**Abstract**: Large language models (LLMs) have shown impressive capabilities in natural language processing tasks, including dialogue generation. This research aims to conduct a novel comparative analysis of two prominent techniques, fine-tuning with LoRA (Low-Rank Adaptation) and the Retrieval-Augmented Generation (RAG) framework, in the context of doctor-patient chat conversations with multiple datasets of mixed medical domains. The analysis involves three state-of-the-art models: Llama-2, GPT, and the LSTM model. Employing real-world doctor-patient dialogues, we comprehensively evaluate the performance of models, assessing key metrics such as language quality (perplexity, BLEU score), factual accuracy (fact-checking against medical knowledge bases), adherence to medical guidelines, and overall human judgments (coherence, empathy, safety). The findings provide insights into the strengths and limitations of each approach, shedding light on their suitability for healthcare applications. Furthermore, the research investigates the robustness of the models in handling diverse patient queries, ranging from general health inquiries to specific medical conditions. The impact of domain-specific knowledge integration is also explored, highlighting the potential for enhancing LLM performance through targeted data augmentation and retrieval strategies. 

**Abstract (ZH)**: 大型语言模型（LLMs）在自然语言处理任务中展现了令人印象深刻的性能，包括对话生成。本研究旨在在多种医学领域混合的数据集背景下，对比分析两种突出的技术——基于LoRA（低秩适应）的微调和检索增强生成（RAG）框架——在医生与患者对话中的性能。分析将涉及三款最先进的模型：Llama-2、GPT以及LSTM模型。通过使用真实的医生与患者对话数据，我们将全面评估这些模型的表现，评估关键指标，如语言质量（困惑度、BLEU分数）、事实准确性（通过医学知识库进行事实核查）、遵守医学指南情况，以及总体的人类判断（连贯性、同理心、安全性）。研究结果将揭示每种方法的优势和局限性，为其在医疗保健应用中的适用性提供洞察。此外，研究还将探讨模型在处理各种患者查询时的鲁棒性，这些问题范围从一般健康咨询到特定医学状况。研究还将探索领域特定知识整合的影响，突显通过针对性的数据增强和检索策略增强LLM性能的潜力。 

---
# When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks 

**Title (ZH)**: 当高维性带来负面影响：LLM嵌入压缩在嘈杂回归任务中的作用 

**Authors**: Felix Drinkall, Janet B. Pierrehumbert, Stefan Zohren  

**Link**: [PDF](https://arxiv.org/pdf/2502.02199)  

**Abstract**: Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation. Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks. In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring. Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data. Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect. 

**Abstract (ZH)**: 大规模语言模型（LLMs）由于在模型大小和隐藏维度方面发现的尺度律，在语言建模方面展现了显著的成功。然而，我们证明压缩的文本表示可以在基于LLM的回归任务中获得更好的性能。在本文中，我们比较了嵌入压缩在三种不同信噪比情境下的相对性能：金融回报预测、写作质量评估和评论评分。我们的结果显示，使用自编码器的隐藏表示以最少监督的方式压缩嵌入，可以减轻过拟合并在噪声较大的任务（如金融回报预测）中改善性能；但在输入数据与目标数据之间具有高因果依赖性的任务中，压缩会降低性能。我们的结果表明，可解释的压缩表示（如情感）的成功可能归因于其正则化效应。 

---
# Mass-Editing Memory with Attention in Transformers: A cross-lingual exploration of knowledge 

**Title (ZH)**: 基于注意机制的大规模编辑记忆在变换器中的研究：跨语言知识探索 

**Authors**: Daniel Tamayo, Aitor Gonzalez-Agirre, Javier Hernando, Marta Villegas  

**Link**: [PDF](https://arxiv.org/pdf/2502.02173)  

**Abstract**: Recent research has explored methods for updating and modifying factual knowledge in large language models, often focusing on specific multi-layer perceptron blocks. This study expands on this work by examining the effectiveness of existing knowledge editing methods across languages and delving into the role of attention mechanisms in this process. Drawing from the insights gained, we propose Mass-Editing Memory with Attention in Transformers (MEMAT), a method that achieves significant improvements in all metrics while requiring minimal parameter modifications. MEMAT delivers a remarkable 10% increase in magnitude metrics, benefits languages not included in the training data and also demonstrates a high degree of portability. Our code and data are at this https URL. 

**Abstract (ZH)**: 近年来，研究主要探索了在大规模语言模型中更新和修改事实型知识的方法，通常集中在特定的多层感知机块上。本研究在此基础上进一步扩展，通过考察现有的知识编辑方法在不同语言中的效果，并深入研究注意机制在这一过程中的作用。借鉴所得的见解，我们提出了基于注意机制的大规模编辑记忆（MEMAT）方法。该方法在所有指标上均实现了显著改进，同时仅需进行少量参数调整。MEMAT 在幅度指标上实现了高达 10% 的显著提升，并能受益于未包含在训练数据中的语言，同时表现出高度的可移植性。我们的代码和数据可以在此链接中找到：[提供链接]。 

---
# Multilingual Attribute Extraction from News Web Pages 

**Title (ZH)**: 新闻网页上的多语言属性提取 

**Authors**: Pavel Bedrin, Maksim Varlamov, Alexander Yatskov  

**Link**: [PDF](https://arxiv.org/pdf/2502.02167)  

**Abstract**: This paper addresses the challenge of automatically extracting attributes from news article web pages across multiple languages. Recent neural network models have shown high efficacy in extracting information from semi-structured web pages. However, these models are predominantly applied to domains like e-commerce and are pre-trained using English data, complicating their application to web pages in other languages. We prepared a multilingual dataset comprising 3,172 marked-up news web pages across six languages (English, German, Russian, Chinese, Korean, and Arabic) from 161 websites. The dataset is publicly available on GitHub. We fine-tuned the pre-trained state-of-the-art model, MarkupLM, to extract news attributes from these pages and evaluated the impact of translating pages into English on extraction quality. Additionally, we pre-trained another state-of-the-art model, DOM-LM, on multilingual data and fine-tuned it on our dataset. We compared both fine-tuned models to existing open-source news data extraction tools, achieving superior extraction metrics. 

**Abstract (ZH)**: 本文探讨了跨境多语言新闻文章网页自动提取属性的挑战。近年来，神经网络模型在提取半结构化网页信息方面展现了很高的有效性。然而，这些模型主要应用于电子商务等领域，并且大多使用英文数据进行预训练，这使得它们在应用到其他语言的网页上时遇到了复杂性。为此，我们准备了一个包含六种语言（英语、德语、俄语、中文、韩语和阿拉伯语）的多语言数据集，共计3,172个标注过的新闻网页，这些数据集来源于161个网站，并已公开发布在GitHub上。我们对预训练的当前最佳模型MarkupLM进行了微调，以从这些页面提取新闻属性，并评估了将页面翻译成英文对提取质量的影响。此外，我们还在多语言数据上对另一个当前最佳模型DOM-LM进行了预训练，并在我们的数据集上进行了微调。我们将这两种微调模型与现有的开源新闻数据提取工具进行了比较，取得了更优的提取指标。 

---
# Topic Modeling in Marathi 

**Title (ZH)**: 马哈拉希特语的主题建模 

**Authors**: Sanket Shinde, Raviraj Joshi  

**Link**: [PDF](https://arxiv.org/pdf/2502.02100)  

**Abstract**: While topic modeling in English has become a prevalent and well-explored area, venturing into topic modeling for Indic languages remains relatively rare. The limited availability of resources, diverse linguistic structures, and unique challenges posed by Indic languages contribute to the scarcity of research and applications in this domain. Despite the growing interest in natural language processing and machine learning, there exists a noticeable gap in the comprehensive exploration of topic modeling methodologies tailored specifically for languages such as Hindi, Marathi, Tamil, and others. In this paper, we examine several topic modeling approaches applied to the Marathi language. Specifically, we compare various BERT and non-BERT approaches, including multilingual and monolingual BERT models, using topic coherence and topic diversity as evaluation metrics. Our analysis provides insights into the performance of these approaches for Marathi language topic modeling. The key finding of the paper is that BERTopic, when combined with BERT models trained on Indic languages, outperforms LDA in terms of topic modeling performance. 

**Abstract (ZH)**: 尽管英语话题建模已经成为一个普及且研究成熟的领域，但对印地语系语言进行话题建模的探索仍然相对较少。有限的资源、多样的语法结构以及印地语系语言带来的独特挑战，导致该领域的研究和应用相对匮乏。尽管自然语言处理和机器学习的兴趣日益增长，但针对如印地语、马拉地语、泰米尔语等印地语系语言的话题建模方法的全面探索仍显不足。本文旨在探讨几种应用于马拉地语的话题建模方法。具体而言，我们比较了各种BERT及其非BERT方法，包括基于印地语系语言训练的多语言和单语言BERT模型，并使用主题连贯性和主题多样性作为评估指标。我们的分析提供了这些方法在马拉地语话题建模中的性能洞见。本文的主要发现是，结合了针对印地语系语言训练的BERT模型的BERTopic，在话题建模性能上优于LDA。 

---
# LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information 

**Title (ZH)**: 长段落生成：通过批评增强的逐步信息生成策略提升大型语言模型的长文本生成能力 

**Authors**: Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, Shanghang Zhang  

**Link**: [PDF](https://arxiv.org/pdf/2502.02095)  

**Abstract**: Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones. 

**Abstract (ZH)**: 长文本生成在学术论文写作和仓库级别代码生成中至关重要。尽管如此，现有的包括GPT-4o在内的模型仍然表现出不尽如人意的表现。现有利用偏好学习和结果监督的方法往往无法为扩展上下文提供详细的反馈，这一不足可能导致生成的内容不能充分满足查询要求，从而出现长度偏离和质量下降等问题。在本文中，我们通过引入过程监督来增强长文本生成。我们采用了蒙特卡洛树搜索来收集逐步偏好对，并利用全局记忆池来保持一致性。为了应对候选选择不理想的问题，我们整合外部评论以精炼和提高偏好对的质量。最后，我们使用收集的逐步偏好对应用逐步骤DPO。实验结果表明，我们的方法在长文本生成基准上提高了长度和质量，在不同模型基础的一般基准上性能几乎无损。 

---
# Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models 

**Title (ZH)**: 重思立场检测：基于理论的语言模型在用户级推断研究议程 

**Authors**: Prasanta Bhattacharya, Hong Zhang, Yiming Cao, Wei Gao, Brandon Siyuan Loh, Joseph J.P. Simons, Liang Ze Wong  

**Link**: [PDF](https://arxiv.org/pdf/2502.02074)  

**Abstract**: Stance detection has emerged as a popular task in natural language processing research, enabled largely by the abundance of target-specific social media data. While there has been considerable research on the development of stance detection models, datasets, and application, we highlight important gaps pertaining to (i) a lack of theoretical conceptualization of stance, and (ii) the treatment of stance at an individual- or user-level, as opposed to message-level. In this paper, we first review the interdisciplinary origins of stance as an individual-level construct to highlight relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models. Further, we argue that recent pre-trained and large language models (LLMs) might offer a way to flexibly infer such user-level attributes and/or incorporate them in modelling stance. To better illustrate this, we briefly review and synthesize the emerging corpus of studies on using LLMs for inferring stance, and specifically on incorporating user attributes in such tasks. We conclude by proposing a four-point agenda for pursuing stance detection research that is theoretically informed, inclusive, and practically impactful. 

**Abstract (ZH)**: 立场检测已成为自然语言处理研究中的一个热门任务，这在很大程度上得益于特定目标社交媒体数据的丰富性。尽管在立场检测模型、数据集和应用方面已有相当多的研究，但我们仍指出存在两个重要缺口：（i）缺乏对立场的理论概念化，以及（ii）通常将立场视为个体级别或用户级别的属性，而非消息级别的属性。在本文中，我们首先回顾立场作为个体级别建构的多学科起源，以突出在立场检测模型中可能需要纳入的相关属性（例如，心理特征）。此外，我们认为最近的预训练和大型语言模型（LLMs）可能提供了一种灵活推断此类用户级别属性或在模型中纳入这些属性的方法。为了更好地说明这一点，我们简要回顾并综合了使用LLMs推断立场的研究，特别是关注嵌入用户属性的研究领域。最后，我们提出了一项四项议程，旨在推动理论基础、包容性和实际影响兼备的立场检测研究。 

---
# ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping 

**Title (ZH)**: ASCenD-BDS：自适应、随机且情境感知的偏见、歧视和刻板印象检测框架 

**Authors**: Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Diman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy  

**Link**: [PDF](https://arxiv.org/pdf/2502.02072)  

**Abstract**: The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development. 

**Abstract (ZH)**: 大规模语言模型（LLMs）的迅速演进已经改变了自然语言处理的领域，但同时也引发了对其部署和跨多种语言和社会文化背景使用过程中固有的偏见问题的严重关切。本文提出了一种框架，名为ASCenD BDS（Adaptable、Stochastic和Context-aware框架，用于检测偏见、歧视和刻板印象）。该框架提供了一种方法，用于检测各种类别（如性别、种姓、年龄、残疾、社会经济地位、语言差异等）中的偏见、歧视和刻板印象，采用的是Adaptable（适应性）、Stochastic（随机性）和Context-aware（情境感知）的方法。现有的框架主要依赖于使用数据集来构建检测偏见、歧视和刻板印象的场景，例如Civil Comments、Wino Gender、WinoBias、BOLD、CrowS Pairs和BBQ等数据集。然而，这种方法仅提供了特定的解决方案，导致数据集提供的评估场景数量有限。当前框架通过具备适应性、随机性和情境感知的特征来克服这一局限性。情境感知可以自定义任何国家、文化和亚文化（例如一个组织的独特文化），本研究中已在印度背景下实现了情境感知。内容团队借鉴了2011年印度人口普查的数据，以建立分类的共同性。本框架通过运用Category（类别）、Sub-Category（子类别）、STEM（主题词组）、X-Factor（额外因子）、Synonym（同义词）等工具来实现适应性、随机性和情境感知的特征，这些内容在第3节中进行了详细描述。该团队由圣狐狸咨询有限公司的顾问组成，总共开发了800多个STEM词组、10个类别和31个独特的子类别。该概念已在圣狐狸实验室的产品开发过程中进行了测试。 

---
# AmaSQuAD: A Benchmark for Amharic Extractive Question Answering 

**Title (ZH)**: AmaSQuAD：阿姆哈拉语提取式问答基准评测 

**Authors**: Nebiyou Daniel Hailemariam, Blessed Guda, Tsegazeab Tefferi  

**Link**: [PDF](https://arxiv.org/pdf/2502.02047)  

**Abstract**: This research presents a novel framework for translating extractive question-answering datasets into low-resource languages, as demonstrated by the creation of the AmaSQuAD dataset, a translation of SQuAD 2.0 into Amharic. The methodology addresses challenges related to misalignment between translated questions and answers, as well as the presence of multiple answer instances in the translated context. For this purpose, we used cosine similarity utilizing embeddings from a fine-tuned BERT-based model for Amharic and Longest Common Subsequence (LCS). Additionally, we fine-tune the XLM-R model on the AmaSQuAD synthetic dataset for Amharic Question-Answering. The results show an improvement in baseline performance, with the fine-tuned model achieving an increase in the F1 score from 36.55% to 44.41% and 50.01% to 57.5% on the AmaSQuAD development dataset. Moreover, the model demonstrates improvement on the human-curated AmQA dataset, increasing the F1 score from 67.80% to 68.80% and the exact match score from 52.50% to 52.66%.The AmaSQuAD dataset is publicly available Datasets 

**Abstract (ZH)**: 本研究提出了一种新颖的框架，用于将提取式问答数据集翻译成资源稀缺语言，以阿姆哈拉语翻译SQuAD 2.0数据集为例进行演示。该方法解决了翻译问题与答案之间的对齐不一致以及翻译语境中存在的多个答案实例的问题。为此，我们使用余弦相似度和经过微调的基于Bert的阿姆哈拉语模型以及最长公共子序列（LCS）方法来处理这些问题。此外，我们还对XLM-R模型进行了微调，以适应AmaSQuAD合成数据集中的阿姆哈拉语问答任务。结果显示，微调后的模型在AmaSQuAD开发数据集上的F1分数提高了，从36.55%提高到44.41%，从50.01%提高到57.5%。此外，该模型在人工精选的AmQA数据集上也显示出改进效果，F1分数从67.80%提高到68.80%，精确匹配分数从52.50%提高到52.66%。AmaSQuAD数据集已公开发布，可供下载使用。 

---
# Contextual Memory Reweaving in Large Language Models Using Layered Latent State Reconstruction 

**Title (ZH)**: 在大型语言模型中使用分层潜在状态重构实现上下文记忆重新编织 

**Authors**: Frederick Dillon, Gregor Halvorsen, Simon Tattershall, Magnus Rowntree, Gareth Vanderpool  

**Link**: [PDF](https://arxiv.org/pdf/2502.02046)  

**Abstract**: Memory retention challenges in deep neural architectures have ongoing limitations in the ability to process and recall extended contextual information. Token dependencies degrade as sequence length increases, leading to a decline in coherence and factual consistency across longer outputs. A structured approach is introduced to mitigate this issue through the reweaving of latent states captured at different processing layers, reinforcing token representations over extended sequences. The proposed Contextual Memory Reweaving framework incorporates a Layered Latent State Reconstruction mechanism to systematically integrate past contextual embeddings without introducing external memory modules. Experimental results demonstrate improvements in recall accuracy across a range of sequence lengths, with notable gains in the retention of rarely occurring tokens and numerical reasoning consistency. Further analysis of computational efficiency indicates that the additional processing overhead remains within acceptable thresholds, enabling scalability across different model sizes. Evaluations in long-form text generation and ambiguous query resolution highlight the capacity of memory reweaving to enhance continuity and reduce inconsistencies over extended outputs. Attention weight distributions reveal more structured allocation patterns, suggesting that reweaved latent states contribute to improved contextual awareness. The findings establish a framework for refining memory retention mechanisms in language models, addressing long-standing challenges in handling complex, multi-step reasoning tasks. 

**Abstract (ZH)**: 深度神经架构中的记忆保持挑战限制了其处理和回忆长时间上下文信息的能力。随着序列长度的增加，标记之间的依赖性减弱，导致较长输出的一致性和事实准确性下降。本文提出了一种结构化方法，通过重新编织不同处理层捕获的潜在状态来缓解这一问题，强化扩展序列中的标记表示。提出的上下文记忆重新编织框架结合了一种分层潜在状态重构机制，系统地整合过去上下文嵌入，而不引入外部内存模块。实验结果表明，在不同序列长度下回忆准确度均有提高，特别是在稀有标记和数值推理一致性方面的显著提升。进一步的计算效率分析表明，额外的处理开销保持在可接受的范围内，从而使得该框架可以在不同模型规模下扩展。在长文本生成和模糊查询解析中的评估表明，记忆重新编织能够增强连续性并降低长时间输出中的不一致性。注意力权重分布揭示了更结构化的分配模式，表明重新编织的潜在状态有助于提高上下文意识。这些发现为改进语言模型中的记忆保持机制奠定了框架，解决了长期存在的处理复杂多步推理任务的挑战。 

---
# M2R2: Mixture of Multi-Rate Residuals for Efficient Transformer Inference 

**Title (ZH)**: M2R2：高效的变压器推理中的混合多速率残差 Mitarbeiter 

**Authors**: Nikhil Bhendawade, Mahyar Najibi, Devang Naik, Irina Belousova  

**Link**: [PDF](https://arxiv.org/pdf/2502.02040)  

**Abstract**: Residual transformations enhance the representational depth and expressive power of large language models (LLMs). However, applying static residual transformations across all tokens in auto-regressive generation leads to a suboptimal trade-off between inference efficiency and generation fidelity. Existing methods, including Early Exiting, Skip Decoding, and Mixture-of-Depth address this by modulating the residual transformation based on token-level complexity. Nevertheless, these approaches predominantly consider the distance traversed by tokens through the model layers, neglecting the underlying velocity of residual evolution. We introduce Mixture of Multi-rate Residuals (M2R2), a framework that dynamically modulates residual velocity to improve early alignment, enhancing inference efficiency. Evaluations on reasoning oriented tasks such as Koala, Self-Instruct, WizardLM, and MT-Bench show M2R2 surpasses state-of-the-art distance-based strategies, balancing generation quality and speedup. In self-speculative decoding setup, M2R2 achieves up to 2.8x speedups on MT-Bench, outperforming methods like 2-model speculative decoding, Medusa, LookAhead Decoding, and DEED. In Mixture-of-Experts (MoE) architectures, integrating early residual alignment with ahead-of-time expert loading into high-bandwidth memory (HBM) accelerates decoding, reduces expert-switching bottlenecks, and achieves a 2.9x speedup, making it highly effective in resource-constrained environments. 

**Abstract (ZH)**: 残差变换能够增强大型语言模型（LLMs）的表示深度和表达能力。然而，在自回归生成过程中，对所有标记应用静态残差变换会导致推理效率和生成保真度之间的次优权衡。现有方法，包括早期退出、跳过解码和深度混合，通过根据标记复杂度调整残差变换来解决这一问题。尽管如此，这些方法主要考虑标记通过模型层遍历的距离，而忽略了残差演化的基本速度。我们提出了多速率残差混合（M2R2）框架，该框架动态调整残差速度以改善早期对齐，从而提升推理效率。在Koala、Self-Instruct、WizardLM和MT-Bench等基于推理的任务上进行的评估表明，M2R2超过了基于距离的最先进策略，平衡了生成质量和加速效果。在自助推测解码设置下，M2R2在MT-Bench上实现了高达2.8倍的加速，优于2-模型推测解码、Medusa、前瞻解码和DEED等方法。在专家混合架构（MoE）中，将早期残差对齐与专家预加载集成到高宽带内存（HBM）中，可以加速解码、减少专家切换瓶颈，并实现2.9倍的加速，使其在资源受限环境中效果显著。 

---
# Fine-tuning Language Models for Recipe Generation: A Comparative Analysis and Benchmark Study 

**Title (ZH)**: 基于精益调优的语言模型在食谱生成中的应用：一项比较分析与基准研究 

**Authors**: Anneketh Vij, Changhao Liu, Rahul Anil Nair, Theo Ho, Edward Shi, Ayan Bhowmick  

**Link**: [PDF](https://arxiv.org/pdf/2502.02028)  

**Abstract**: This research presents an exploration and study of the recipe generation task by fine-tuning various very small language models, with a focus on developing robust evaluation metrics and comparing across different language models the open-ended task of recipe generation. This study presents extensive experiments with multiple model architectures, ranging from T5-small (Raffel et al., 2023) and SmolLM-135M (Allal et al., 2024) to Phi-2 (Research, 2023),implementing both traditional NLP metrics and custom domain-specific evaluation metrics. Our novel evaluation framework incorporates recipe-specific metrics for assessing content quality and introduces an approach to allergen substitution. The results indicate that, while larger models generally perform better on standard metrics, the relationship between model size and recipe quality is more nuanced when considering domain-specific metrics. We find that SmolLM-360M and SmolLM-1.7B demonstrate comparable performance despite their size difference, while Phi-2 shows limitations in recipe generation despite its larger parameter count. Our comprehensive evaluation framework and allergen substitution system provide valuable insights for future work in recipe generation and broader NLG tasks that require domain expertise and safety considerations. 

**Abstract (ZH)**: 本研究通过微调各种非常小型的语言模型，对食谱生成任务进行了探索和研究，重点关注开发稳健的评估指标，并在不同语言模型之间对比开放式食谱生成任务。本研究进行了广泛的实验，涵盖了从T5-small（Raffel等人，2023）、SmolLM-135M（Allal等人，2024）到Phi-2（Research，2023）等多种模型架构，既采用了传统的自然语言处理（NLP）指标，也采用了定制的领域特定评估指标。我们提出了一种新颖的评估框架，包括针对内容质量的食谱特定指标，并引入了一种过敏原替代的方法。研究结果表明，在标准指标上，较大的模型通常表现更好，但当考虑领域特定指标时，模型大小与食谱质量之间的关系更为复杂。我们发现，尽管SmolLM-360M和SmolLM-1.7B的规模不同，但它们在性能上表现出可比性，而Phi-2尽管参数量更大，在食谱生成方面却显示出局限性。我们全面的评估框架和过敏原替代系统为未来食谱生成及需要领域知识和安全考虑的更广泛自然语言生成（NLG）任务提供了有价值的见解。 

---
# Reasoning Bias of Next Token Prediction Training 

**Title (ZH)**: 下一词预测训练中的推理偏差 

**Authors**: Pengxiao Lin, Zhongwang Zhang, Zhi-Qin John Xu  

**Link**: [PDF](https://arxiv.org/pdf/2502.02007)  

**Abstract**: Since the inception of Large Language Models (LLMs), the quest to efficiently train them for superior reasoning capabilities has been a pivotal challenge. The dominant training paradigm for LLMs is based on next token prediction (NTP). Alternative methodologies, called Critical Token Prediction (CTP), focused exclusively on specific critical tokens (such as the answer in Q\&A dataset), aiming to reduce the overfitting of extraneous information and noise. Contrary to initial assumptions, our research reveals that despite NTP's exposure to noise during training, it surpasses CTP in reasoning ability. We attribute this counterintuitive outcome to the regularizing influence of noise on the training dynamics. Our empirical analysis shows that NTP-trained models exhibit enhanced generalization and robustness across various benchmark reasoning datasets, demonstrating greater resilience to perturbations and achieving flatter loss minima. These findings illuminate that NTP is instrumental in fostering reasoning abilities during pretraining, whereas CTP is more effective for finetuning, thereby enriching our comprehension of optimal training strategies in LLM development. 

**Abstract (ZH)**: 自大型语言模型（LLMs）问世以来，如何高效训练以提升其推理能力成为了一个关键挑战。LLMs的主要训练范式基于下一个令牌预测（NTP）。而替代性方法，称为关键令牌预测（CTP），则专注于特定的关键令牌（如问答（Q&A）数据集中答案），旨在减少无关信息和噪声的过拟合。与最初的假设相反，我们的研究表明，尽管NTP在训练过程中会暴露于噪声，但它的推理能力仍然优于CTP。我们归因于这种反直觉结果的原因是噪声对训练动力学的正则化影响。我们的实证分析表明，NTP训练的模型在各种基准推理数据集上表现出更强的泛化能力和稳健性，更能抵御干扰并达到更平坦的损失极小值。这些研究成果揭示了NTP在预训练期间有助于促进推理能力，而CTP则在微调阶段更为有效，从而丰富了我们对LLM开发中最佳训练策略的理解。 

---
# Wavelet-based Positional Representation for Long Context 

**Title (ZH)**: 基于小波的位置表示方法用于长上下文 

**Authors**: Yui Oka, Taku Hasegawa, Kyosuke Nishida, Kuniko Saito  

**Link**: [PDF](https://arxiv.org/pdf/2502.02004)  

**Abstract**: In the realm of large-scale language models, a significant challenge arises when extrapolating sequences beyond the maximum allowable length. This is because the model's position embedding mechanisms are limited to positions encountered during training, thus preventing effective representation of positions in longer sequences. We analyzed conventional position encoding methods for long contexts and found the following characteristics. (1) When the representation dimension is regarded as the time axis, Rotary Position Embedding (RoPE) can be interpreted as a restricted wavelet transform using Haar-like wavelets. However, because it uses only a fixed scale parameter, it does not fully exploit the advantages of wavelet transforms, which capture the fine movements of non-stationary signals using multiple scales (window sizes). This limitation could explain why RoPE performs poorly in extrapolation. (2) Previous research as well as our own analysis indicates that Attention with Linear Biases (ALiBi) functions similarly to windowed attention, using windows of varying sizes. However, it has limitations in capturing deep dependencies because it restricts the receptive field of the model. From these insights, we propose a new position representation method that captures multiple scales (i.e., window sizes) by leveraging wavelet transforms without limiting the model's attention field. Experimental results show that this new method improves the performance of the model in both short and long contexts. In particular, our method allows extrapolation of position information without limiting the model's attention field. 

**Abstract (ZH)**: 在大规模语言模型领域，一个显著的挑战是在超出模型允许的最大长度范围内进行序列外推。这是因为模型的位置嵌入机制仅限于训练过程中遇到的位置，从而无法有效地表示更长序列中的位置信息。我们分析了长上下文中的常规位置编码方法，并发现了以下特点：（1）将表示维度视为时间轴时，旋转位置嵌入（RoPE）可以被解释为使用类似于哈耳波浪的限制波拉特定理。然而，由于RoPE仅使用固定的比例参数，它未能充分利用波拉定变换的优点，波拉定变换通过多个尺度（窗口大小）捕捉非稳态信号的微小变化。这一限制可能解释了为什么RoPE在外推时表现不佳。（2）之前的研究以及我们自己的分析表明，带有线性偏置的注意力机制（ALiBi）类似于窗口注意力机制，使用大小不同的窗口。然而，它在捕捉深层依赖关系方面存在限制，因为它限制了模型的感受野。从这些见解出发，我们提出了一种新的位置表示方法，通过利用波拉定变换捕获多个尺度（即窗口大小）而不限制模型的注意力范围。实验结果表明，该新方法在短上下文和长上下文中均能提高模型的性能，特别是我们的方法允许在不限制模型的注意力范围的情况下进行位置信息的外推。 

---
# Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media 

**Title (ZH)**: 大型语言模型能否辅助注释者识别道德框架——社交媒体疫苗辩论案例研究 

**Authors**: Tunazzina Islam, Dan Goldwasser  

**Link**: [PDF](https://arxiv.org/pdf/2502.01991)  

**Abstract**: Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks such as identifying morality frames makes relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a "think-aloud" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks. 

**Abstract (ZH)**: 当今，社交媒体在塑造公众舆论方面发挥着关键作用，尤其是在涉及疫苗等 polarization问题上，不同的道德视角影响着个人的观点。在自然语言处理（NLP）中，数据稀缺性和心理语言学任务（如识别道德框架）的复杂性使得依赖人工标注员进行标注成本高、耗时长且容易由于认知负担而导致一致性差。为解决这些问题，我们运用了大型语言模型（LLMs），这些模型通过少样本学习擅长于适应新的任务，利用少量上下文示例及其与任务原则的联系进行解释。我们的研究探讨了LLMs在协助人工标注员识别社交媒体上关于疫苗讨论中的道德框架方面的潜力。我们采用两步过程：首先使用LLMs生成概念和解释，然后使用“思考 aloud”工具进行人工评估。研究结果显示，将LLMs集成到标注过程中能够提高准确性、降低任务难度、减轻认知负担，这表明在复杂心理语言学任务中人类-AI协作的一个有前景的方向。 

---
# Gradient-Regularized Latent Space Modulation in Large Language Models for Structured Contextual Synthesis 

**Title (ZH)**: 用于结构化上下文合成的大语言模型中的梯度正则化隐空间调控 

**Authors**: Derek Yotheringhay, Beatrix Nightingale, Maximilian Featherstone, Edmund Worthington, Hugo Ashdown  

**Link**: [PDF](https://arxiv.org/pdf/2502.01979)  

**Abstract**: Generating structured textual content requires mechanisms that enforce coherence, stability, and adherence to predefined constraints while maintaining semantic fidelity. Conventional approaches often rely on rule-based heuristics or fine-tuning strategies that lack flexibility and generalizability across diverse tasks. The incorporation of Gradient-Regularized Latent Space Modulation (GRLSM) introduces a novel paradigm for guiding text generation through the application of structured constraints within the latent space. The integration of gradient-based regularization mitigates abrupt variations in latent representations, ensuring a smoother encoding process that enhances structural consistency and logical progression within generated sequences. Comparative evaluations demonstrate that latent space modulation leads to a reduction in perplexity, increased coherence scores, and improved structural alignment across multiple domains. Stability assessments further indicate that the imposition of spectral norm constraints facilitates more controlled variations in generated text, preserving semantic consistency under input perturbations. Empirical results confirm that structured latent space constraints not only refine the organization of generated outputs but also enhance interpretability through more predictable and reliable synthesis patterns. Performance metrics illustrate that the GRLSM framework substantially reduces structural inconsistencies while preserving the generative flexibility inherent in neural models. 

**Abstract (ZH)**: 生成结构化文本内容需要机制来确保连贯性、稳定性和对预定义约束的遵守，同时保持语义保真度。传统的做法往往依赖于基于规则的启发式方法或微调策略，这些方法缺乏跨不同任务的灵活性和普适性。将梯度正则化潜空间调制（GRLSM）纳入其中，引入了一种新的范式，通过在潜空间中应用结构化约束来引导文本生成。基于梯度的正则化整合可以缓解潜表示的突变，确保一个更平滑的编码过程，提高生成序列的结构一致性和逻辑进展。对比性评估显示，潜空间调制可以降低困惑度、提高连贯性评分，并在多个领域中改善结构对齐。稳定性评估进一步表明，施加谱范数约束有助于更可控制地生成文本的变化，从而在输入扰动下保持语义一致性。实验证实，结构化的潜空间约束不仅细化了生成输出的组织，还通过更可预测且可靠的合成模式增强了可解释性。性能指标表明，GRLSM框架在减少结构不一致性的同时，保留了神经模型固有的生成灵活性。 

---
# CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing 

**Title (ZH)**: CITER：基于token级路由的协作推理方法，以实现高效的大规模语言模型解码 

**Authors**: Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao  

**Link**: [PDF](https://arxiv.org/pdf/2502.01976)  

**Abstract**: Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\textbf{C}ollaborative \textbf{I}nference with \textbf{T}oken-l\textbf{E}vel \textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications. 

**Abstract (ZH)**: 大规模语言模型在各种任务中取得了显著的成功，但在推断过程中面临着高昂的计算成本，限制了它们在资源受限应用中的部署。为解决这一问题，我们提出了一种名为CITER（Collaborative Inference with Token-Level Routing）的新框架，该框架通过令牌水平路由策略在小型和大型语言模型（小语言模型(SLM)和大语言模型(LLM)）之间实现了高效的协作。具体而言，CITER将非关键令牌路由到SLM以提高效率，将关键令牌路由到LLM以获得泛化质量。我们将路由器训练形式化为策略优化问题，其中路由器根据预测质量以及生成过程的推断成本获得奖励。这使得路由器能够学习预测令牌级路由得分，并根据当前令牌及其决策未来影响做出路由决策。为了进一步加速奖励评估过程，我们引入了一个捷径，显著降低了奖励估计成本，从而提高了我们方法的实用性。在五个基准数据集上的广泛实验表明，CITER能够在保持高质量生成的同时减少推断成本，为实时和资源受限应用提供了一种有前景的解决方案。 

---
# Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning 

**Title (ZH)**: tokens清洗：面向LLM监督微调的细粒度数据选择 

**Authors**: Jinlong Pang, Na Di, Zhaowei Zhu, Jiaheng Wei, Hao Cheng, Chen Qian, Yang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.01968)  

**Abstract**: Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks. 

**Abstract (ZH)**: 近期的研究表明，在大型语言模型（LLMs）的监督微调（SFT）过程中，数据质量比数量更为重要。虽然大多数数据清洗方法主要集中在过滤整个样本，但样本中的单个词元质量可能会有很大差异。预训练后，即使在高质量的样本中，也可能包含与任务无关且冗余或信息量不足的模式或短语。继续在这些模式上进行微调可能会带来有限的好处，甚至可能降低下游任务的性能。本文从嘈杂标签的角度研究词元质量，并提出了一种通用的词元清洗管道以用于SFT任务。该方法通过过滤掉无信息的词元并保留那些携带关键任务特定信息的词元，从而在保持有用信息的同时去除冗余信息。具体来说，我们首先通过检查模型更新对每个词元的影响来评估词元质量，然后应用基于阈值的分离方法。词元影响可以使用固定参阅模型在一个通过中进行测量，也可以使用自我进化的参阅模型进行迭代测量。通过对这两种方法进行理论上的误差上限分析，得出了各自的优缺点。广泛的实验证明，我们的框架在多个下游任务中都能一致地提高性能。 

---
# Boundary-Driven Table-Filling with Cross-Granularity Contrastive Learning for Aspect Sentiment Triplet Extraction 

**Title (ZH)**: 基于边界驱动的跨粒度对比学习表格填充方法在方面情感三元组提取中的应用 

**Authors**: Qingling Li, Wushao Wen, Jinghui Qin  

**Link**: [PDF](https://arxiv.org/pdf/2502.01942)  

**Abstract**: The Aspect Sentiment Triplet Extraction (ASTE) task aims to extract aspect terms, opinion terms, and their corresponding sentiment polarity from a given sentence. It remains one of the most prominent subtasks in fine-grained sentiment analysis. Most existing approaches frame triplet extraction as a 2D table-filling process in an end-to-end manner, focusing primarily on word-level interactions while often overlooking sentence-level representations. This limitation hampers the model's ability to capture global contextual information, particularly when dealing with multi-word aspect and opinion terms in complex sentences. To address these issues, we propose boundary-driven table-filling with cross-granularity contrastive learning (BTF-CCL) to enhance the semantic consistency between sentence-level representations and word-level representations. By constructing positive and negative sample pairs, the model is forced to learn the associations at both the sentence level and the word level. Additionally, a multi-scale, multi-granularity convolutional method is proposed to capture rich semantic information better. Our approach can capture sentence-level contextual information more effectively while maintaining sensitivity to local details. Experimental results show that the proposed method achieves state-of-the-art performance on public benchmarks according to the F1 score. 

**Abstract (ZH)**: Aspect 情感三元组抽取（ASTE）任务的目标是从给定的句子中提取aspect术语、意见术语及其相应的极性。它仍然是细粒度情感分析中最突出的子任务之一。现有的大多数方法将三元组提取视为端到端的二维表格填充过程，主要关注词汇层面的交互，而往往忽视了句子层面的表示。这种局限性阻碍了模型捕捉全局上下文信息的能力，特别是在处理复杂句子中的多词aspect和意见术语时更为明显。为了应对这些挑战，我们提出了一种基于边界驱动的表格填充与跨粒度对比学习（BTF-CCL）方法，以增强句子层面和词层面表示之间的语义一致性。通过构建正负样本对，模型被强制学习句子层面和词层面的关联。此外，我们还提出了一种多尺度、多粒度卷积方法，以更好地捕捉丰富的语义信息。我们的方法能够在保持对局部细节敏感的同时，更有效地捕捉句子层面的上下文信息。实验结果表明，根据F1分数，所提出的方法在公共基准上达到了最先进的性能。 

---
# Can LLMs Maintain Fundamental Abilities under KV Cache Compression? 

**Title (ZH)**: LLM在KV缓存压缩情况下能否维持基本能力？ 

**Authors**: Xiang Liu, Zhenheng Tang, Hong Chen, Peijie Dong, Zeyu Li, Xiuze Zhou, Bo Li, Xuming Hu, Xiaowen Chu  

**Link**: [PDF](https://arxiv.org/pdf/2502.01941)  

**Abstract**: This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and this http URL analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\%$-$43.3\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\%$-$25.53\%$ performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\%$-$18\%$ performance improvements on long-context generation tasks under aggressive compression ratios. 

**Abstract (ZH)**: 本文探讨了一个在大型语言模型（LLMs）中未被充分研究的挑战：KV缓存压缩方法对LLMs基本能力的影响。尽管现有方法在长上下文基准测试中实现了令人印象深刻的压缩比率，但它们对核心模型能力的影响仍鲜有研究。我们进行了一项全面的经验研究，评估了多种突出的KV缓存压缩方法在不同任务中的表现，涵盖了世界知识、常识推理、算术推理、代码生成、安全性和长上下文理解等多个领域。分析结果显示，KV缓存压缩方法表现出了任务特异性性能下降。算术推理任务对激进压缩特别敏感，不同方法的性能下降幅度在17.4%到43.3%之间。值得注意的是，DeepSeek R1 Distill模型在面对激进压缩时表现出更稳健的压缩容忍度，其性能下降幅度仅为9.67%到25.53%。基于我们对注意力模式和跨任务压缩性能的分析，我们提出了ShotKV，一种新颖的压缩方法，该方法在处理预填充和解码阶段时能保留摄影水平的语义一致性。实验结果表明，在激进的压缩比率下，ShotKV在长上下文生成任务中实现了9%到18%的性能提升。 

---
# PANDAS: Improving Many-shot Jailbreaking via Positive Affirmation, Negative Demonstration, and Adaptive Sampling 

**Title (ZH)**: PANDAS: 通过正面肯定、负面示范和自适应采样提高多视角 Jailbreaking 效果 

**Authors**: Avery Ma, Yangchen Pan, Amir-massoud Farahmand  

**Link**: [PDF](https://arxiv.org/pdf/2502.01925)  

**Abstract**: Many-shot jailbreaking circumvents the safety alignment of large language models by exploiting their ability to process long input sequences. To achieve this, the malicious target prompt is prefixed with hundreds of fabricated conversational turns between the user and the model. These fabricated exchanges are randomly sampled from a pool of malicious questions and responses, making it appear as though the model has already complied with harmful instructions. In this paper, we present PANDAS: a hybrid technique that improves many-shot jailbreaking by modifying these fabricated dialogues with positive affirmations, negative demonstrations, and an optimized adaptive sampling method tailored to the target prompt's topic. Extensive experiments on AdvBench and HarmBench, using state-of-the-art LLMs, demonstrate that PANDAS significantly outperforms baseline methods in long-context scenarios. Through an attention analysis, we provide insights on how long-context vulnerabilities are exploited and show how PANDAS further improves upon many-shot jailbreaking. 

**Abstract (ZH)**: 许多样本量的监狱突破通过利用大型语言模型处理长输入序列的能力，规避了它们的安全对齐。为实现这一点，恶意目标提示会前置几百个虚构的用户与模型之间的对话轮次。这些虚构的对话是从恶意问题和回应的池中随机抽样得出的，使得模型看起来已经按照有害指令执行。在这项研究中，我们提出了一种名为PANDAS的混合技术，通过修改这些虚构对话，加入正面肯定、负面示范，并针对目标提示主题优化自适应抽样方法，来改善许多样本量的监狱突破。利用最新最先进的LLM在AdvBench和HarmBench上进行的广泛实验表明，PANDAS在长上下文场景中显著优于基础方法。通过注意力分析，我们进一步揭示了长上下文漏洞的利用机制，并展示了PANDAS如何进一步提升许多样本量的监狱突破的效果。 

---
# Conceptual Metaphor Theory as a Prompting Paradigm for Large Language Models 

**Title (ZH)**: 概念隐喻理论作为大型语言模型的促动范式 

**Authors**: Oliver Kramer  

**Link**: [PDF](https://arxiv.org/pdf/2502.01901)  

**Abstract**: We introduce Conceptual Metaphor Theory (CMT) as a framework for enhancing large language models (LLMs) through cognitive prompting in complex reasoning tasks. CMT leverages metaphorical mappings to structure abstract reasoning, improving models' ability to process and explain intricate concepts. By incorporating CMT-based prompts, we guide LLMs toward more structured and human-like reasoning patterns. To evaluate this approach, we compare four native models (Llama3.2, Phi3, Gemma2, and Mistral) against their CMT-augmented counterparts on benchmark tasks spanning domain-specific reasoning, creative insight, and metaphor interpretation. Responses were automatically evaluated using the Llama3.3 70B model. Experimental results indicate that CMT prompting significantly enhances reasoning accuracy, clarity, and metaphorical coherence, outperforming baseline models across all evaluated tasks. 

**Abstract (ZH)**: 我们将概念隐喻理论（CMT）引入作为通过认知提示增强大型语言模型（LLMs）的框架，以提高其在复杂推理任务中的能力。CMT利用隐喻映射来结构化抽象推理，从而提高模型处理和解释复杂概念的能力。通过引入基于CMT的提示，我们引导LLMs向更加结构化和人类化的推理模式发展。为了评估这种方法，我们在涵盖领域特定推理、创造性洞察力和隐喻解释的基准任务中，将四个原生模型（Llama3.2、Phi3、Gemma2和Mistral）与其增强的CMT版本进行了对比。响应由Llama3.3 70B模型自动评估。实验结果表明，CMT提示显著提高了推理的准确性和清晰度以及隐喻的一致性，在所有评估任务中均优于基线模型。 

---
# Latent Lexical Projection in Large Language Models: A Novel Approach to Implicit Representation Refinement 

**Title (ZH)**: 大型语言模型中的隐含词项投影：一种新颖的隐式表示精炼方法 

**Authors**: Ziad Shaker, Brendan Ashdown, Hugo Fitzalan, Alistair Heathcote, Jocasta Huntington  

**Link**: [PDF](https://arxiv.org/pdf/2502.01882)  

**Abstract**: Generating semantically coherent text requires a robust internal representation of linguistic structures, which traditional embedding techniques often fail to capture adequately. A novel approach, Latent Lexical Projection (LLP), is introduced to refine lexical representations through a structured transformation into a latent space, thereby enhancing the alignment between input embeddings and their contextual meanings. The method integrates an optimized projection mechanism within an existing language model architecture, enabling more accurate token selection while maintaining syntactic integrity. Evaluations across multiple benchmarks indicate a reduction in perplexity and an increase in BLEU scores, suggesting improvements in predictive accuracy and fluency. The analysis of lexical diversity reveals a more varied vocabulary in generated text, addressing common issues of redundancy and repetitive phrase structures. Further assessments of entropy distributions demonstrate a decline in uncertainty during decoding, reflecting enhanced confidence in word selection. Additionally, long-range dependency retention exhibits measurable gains, with increased classification accuracy at extended token distances. Computational efficiency remains within manageable constraints, despite the added projection mechanism, highlighting the practicality of LLP for integration into existing architectures. 

**Abstract (ZH)**: 生成语义连贯的文本需要一种稳健的内部语言结构表示，而传统嵌入技术往往难以充分捕捉这一点。为此，我们提出了一种名为潜在词汇投影（LLP）的新型方法，通过结构化的转换将词汇表示映射到潜在空间，从而增强输入嵌入与其上下文意义之间的对齐。该方法在现有语言模型架构中引入了优化的投影机制，能够更加准确地选择词项的同时保持句法完整性。在多个基准测试中的评估表明，LLP降低了困惑度并提高了BLEU分数，这表明在预测准确性和流畅性方面取得了改进。通过分析词汇多样性，我们发现生成文本中词汇更加多样化，解决了冗余和重复短语结构的常见问题。此外，进一步评估熵分布表明解码过程中的不确定性有所下降，反映了对词项选择更有信心。另外，长距离依赖关系的保留也表现出可测量的改进，分类准确性在跨多个词项距离时提高。尽管引入了投影机制，计算效率仍保持在可管理的范围内，突显了LLP在现有架构中集成的实用性。 

---
# SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models 

**Title (ZH)**: SelfCheckAgent：生成型大规模语言模型中的零资源幻觉检测 

**Authors**: Diyana Muhammed, Gollam Rabby, Sören Auer  

**Link**: [PDF](https://arxiv.org/pdf/2502.01812)  

**Abstract**: Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent's applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs. 

**Abstract (ZH)**: 在现实世界应用中可靠部署大型语言模型（LLMs）的关键挑战之一是检测幻觉。为此，我们引入了SelfCheckAgent，这是一种新颖的框架，集成了三个不同的智能体：符号智能体、专业化检测智能体以及上下文一致性智能体。这些智能体提供了一种稳健的多维方法来检测幻觉。值得注意的是，上下文一致性智能体利用Llama 3.1和思维链（CoT）在WikiBio数据集上取得了显著性能，非事实幻觉检测得分为93.64%，事实幻觉得分为70.26%，排名得分为78.48%。在AIME数据集上，GPT-4o配以思维链在非事实幻觉检测方面表现出色，得分为94.89%，但在事实幻觉和排名方面分别仅得30.58%和30.68%，这表明在复杂数学领域中幻觉检测的复杂性。该框架还采用了三角测量策略，这增强了SelfCheckAgent的优势，显著改善了实际幻觉识别的性能。通过比较分析可以看出，SelfCheckAgent适用于多种领域，将其确立为增强可信LLMs的关键进步。这些发现突显了以一致性驱动方法在检测LLMs中幻觉方面的潜力。 

---
# On Bob Dylan: A Computational Perspective 

**Title (ZH)**: 从计算视角探讨鲍勃·迪伦 

**Authors**: Prashant Garg  

**Link**: [PDF](https://arxiv.org/pdf/2502.01772)  

**Abstract**: Cass Sunstein's essay 'On Bob Dylan' describes Dylan's 'dishabituating' style -- a constant refusal to conform to expectation and a penchant for reinventing his musical and lyrical identity. In this paper, I extend Sunstein's observations through a large-scale computational analysis of Dylan's lyrics from 1962 to 2012. Using o3-mini-high (a large language model), I extract concept-to-concept relationships from the lyrics and construct directed knowledge graphs that capture Dylan's thematic structure. I then quantify shifts in sentiment, metaphorical expression, thematic diversity, and network complexity over time. The results indicate that Dylan's lyrics increasingly rely on metaphor, display an evolving sentiment profile, and exhibit heightened dishabituation -- measured here as a growing variance in the network centrality of key concepts. I also find that references to movement, protest, and mythic imagery fluctuate in ways that align with well-known phases of Dylan's career, reflecting the dynamic and unpredictable quality of his art. These findings not only deepen our empirical understanding of Sunstein's thesis but also introduce a novel computational method for analyzing an artist's evolution-offering broader applicability to the study of cultural and creative change. 

**Abstract (ZH)**: 康斯（Cass Sunstein）的文章《论鲍勃·迪伦》探讨了迪伦特有的“去习惯化”风格——不断拒绝默认期望，并频繁重塑其音乐和歌词身份。本文在此基础上，通过大规模计算分析从1962年至2012年迪伦的歌词，扩展了Sunstein的观察。利用o3-mini-high（一个大型语言模型），我从歌词中提取概念间的联系，并构建了定向知识图谱来捕捉迪伦的主题结构。随后，我量化了情感、隐喻表达、主题多样性以及网络复杂性随时间的变化。结果表明，迪伦的歌词越来越依赖于隐喻，情感模式呈现出演变的趋势，并表现出增强的去习惯化特性——在这里衡量为关键概念在网络中心性上的增加变异度。此外，我还发现关于运动、抗议和神话意象的提及在迪伦职业生涯的知名阶段表现出波动，反映了其艺术的动态和不可预测性。这些发现不仅深化了我们对Sunstein论点的实证理解，还引入了一种新的计算方法来分析艺术家的成长变化，为文化与创造性变革的研究提供了更广泛的适用性。 

---
# Evaluation of Large Language Models via Coupled Token Generation 

**Title (ZH)**: 通过耦合令牌生成评估大型语言模型 

**Authors**: Nina Corvelo Benz, Stratis Tsirtsis, Eleni Straitouri, Ivi Chatzi, Ander Artola Velasco, Suhas Thejaswi, Manuel Gomez-Rodriguez  

**Link**: [PDF](https://arxiv.org/pdf/2502.01754)  

**Abstract**: State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation. 

**Abstract (ZH)**: 最先进的大语言模型依赖随机性来回应提示。作为直接结果，同一个提示若被模型多次回应，可能会得到不同的结果。本研究中，我们主张在评估和排名大语言模型时，应该控制其运行背后所依赖的随机性。我们的出发点是开发一种因果模型，用于耦合自回归生成，使得不同大语言模型能够使用相同的随机源采样响应。基于我们的因果模型，我们首先表明，在基于基准数据集的评估中，耦合自回归生成与传统的自回归生成得出相同的结论，但使用了可以证明更少的样本。然而，我们进一步表明，在基于（人类的）成对比较的评估中，即使使用无限数量的样本，耦合和传统的自回归生成法也可能导致对超过两个模型的排名不同。这表明，在现有评估协议中，模型相对于其他模型的表象优势可能并非真实存在，而是由生成过程内在的随机性造成的混淆。为了展示并补充我们的理论结果，我们使用了来自Llama家族的几种大语言模型进行了实验。我们发现，在流行的MMLU基准数据集的多个知识领域中，耦合自回归生成需要少至40%的样本，就能达到与传统的自回归生成相同的结果。此外，通过LMSYS Chatbot竞技场平台的数据，我们发现一个强大的大语言模型对提示的胜率在耦合和传统的自回归生成下有所不同。 

---
# Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction 

**Title (ZH)**: Comply: 学习具有复杂权重的句子，受果蝇嗅觉启发 

**Authors**: Alexei Figueroa, Justus Westerhoff, Atefi Golzar, Dennis Fast, Benjamin Winter, Felix Alexader Gers, Alexander Löser, Wolfang Nejdl  

**Link**: [PDF](https://arxiv.org/pdf/2502.01706)  

**Abstract**: Biologically inspired neural networks offer alternative avenues to model data distributions. FlyVec is a recent example that draws inspiration from the fruit fly's olfactory circuit to tackle the task of learning word embeddings. Surprisingly, this model performs competitively even against deep learning approaches specifically designed to encode text, and it does so with the highest degree of computational efficiency. We pose the question of whether this performance can be improved further. For this, we introduce Comply. By incorporating positional information through complex weights, we enable a single-layer neural network to learn sequence representations. Our experiments show that Comply not only supersedes FlyVec but also performs on par with significantly larger state-of-the-art models. We achieve this without additional parameters. Comply yields sparse contextual representations of sentences that can be interpreted explicitly from the neuron weights. 

**Abstract (ZH)**: 受生物启发的神经网络为建模数据分布提供了替代路径。FlyVec是一个最近的例子，它借鉴了果蝇嗅觉电路的灵感，用于学习词嵌入。令人惊讶的是，该模型即使在面对专门设计用于编码文本的深度学习方法时也能表现出色，并且具有最高的计算效率。我们提出了一个问题，即这种性能是否可以进一步提升。为此，我们引入了Comply。通过引入位置信息并通过复数权重，我们使单层神经网络能够学习序列表示。我们的实验结果显示，Comply不仅超越了FlyVec，而且还与显著更大的先进模型相当。我们没有增加额外的参数就实现了这一点。Comply生成了可以明确从神经元权重中解释的稀疏上下文句子表示。 

---
# BARE: Combining Base and Instruction-Tuned Language Models for Better Synthetic Data Generation 

**Title (ZH)**: BARE：结合基础模型和指令调优语言模型以生成更好的合成数据 

**Authors**: Alan Zhu, Parth Asawa, Jared Quincy Davis, Lingjiao Chen, Ion Stoica, Joseph E. Gonzalez, Matei Zaharia  

**Link**: [PDF](https://arxiv.org/pdf/2502.01697)  

**Abstract**: As the demand for high-quality data in model training grows, researchers and developers are increasingly generating synthetic data to tune and train LLMs. A common assumption about synthetic data is that sampling from instruct-tuned models is sufficient; however, these models struggle to produce diverse outputs-a key requirement for generalization. Despite various prompting methods, in this work we show that achieving meaningful diversity from instruct-tuned models remains challenging. In contrast, we find base models without post-training exhibit greater diversity, but are less capable at instruction following and hence of lower quality. Leveraging this insight, we propose Base-Refine (BARE), a synthetic data generation method that combines the diversity of base models with the quality of instruct-tuned models through a two-stage process. With minimal few-shot examples and curation, BARE generates diverse and high-quality datasets, improving downstream task performance. We show that fine-tuning with as few as 1,000 BARE-generated samples can reach performance comparable to the best similarly sized models on LiveCodeBench tasks. Furthermore, fine-tuning with BARE-generated data achieves a 101% improvement over instruct-only data on GSM8K and a 18.4% improvement over SOTA methods on RAFT. 

**Abstract (ZH)**: 随着对高质量训练数据需求的增长，研究人员和开发者越来越多地生成合成数据以调优和训练大规模语言模型（LLMs）。关于合成数据的一个常见假设是从指令调优模型抽样是足够的；然而，这些模型在生成多样化输出方面存在困难，这是泛化所需的关键要求。尽管使用了各种提示方法，本研究仍表明，从指令调优模型中实现有意义的多样性仍然具有挑战性。相反，我们发现未经过后训练的基础模型展现出更大的多样性，但在指令遵循方面能力较弱，因此质量较低。基于这一洞察，我们提出了一种合成数据生成方法Base-Refine（BARE），该方法通过两阶段过程结合了基础模型的多样性和指令调优模型的质量。借助少量的示例和策展，BARE能够生成多样化且高质量的数据集，从而改善下游任务的表现。我们展示了即使使用1,000个BARE生成的样本进行微调，也能在LiveCodeBench任务上达到性能与最佳同类模型相当的结果。此外，使用BARE生成的数据进行微调在GSM8K上实现了101%的改进，在RAFT上实现了18.4%的改进，超过现有的最佳方法。 

---
# Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model 

**Title (ZH)**: 基于代理的不确定性意识到提高了使用开源大规模语言模型的自动化放射学报告标签化效能 

**Authors**: Hadas Ben-Atya, Naama Gavrielov, Zvi Badash, Gili Focht, Ruth Cytter-Kuint, Talar Hagopian, Dan Turner, Moti Freiman  

**Link**: [PDF](https://arxiv.org/pdf/2502.01691)  

**Abstract**: Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen's Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications. 

**Abstract (ZH)**: 使用大型语言模型（LLMs）从放射学报告中可靠地提取结构化数据仍然具有挑战性，尤其是在处理复杂且非英语文本（如希伯来文）时。本研究引入了一种基于代理的不确定性感知方法，以提高医疗应用中LLM预测的可信度。我们分析了来自三个医疗机构的9,683份克罗恩病患者的希伯来文放射学报告（时间范围为2010年至2023年）。其中512份报告被手动标注了六个胃肠道器官和15项病理发现，其余报告则使用HSMP-BERT进行了自动标注。结构化数据提取使用了Llama 3.1（Llama 3-8b-instruct）并结合了贝叶斯提示集（BayesPE），该方法使用六个语义等效提示来估计不确定性。基于代理的决策模型将多种提示输出整合到五个信心水平中，以校准不确定性，并与三种基于熵的方法进行了比较。性能评估使用了准确率、F1分数、精确率、召回率和柯克帕特里克系数（Cohen's Kappa）在筛选出高不确定性病例之前和之后进行。基于代理的模型在所有指标上都优于 baseline，F1分数达到了0.3967，召回率为0.6437，柯克帕特里克系数为0.3006。在筛选出高不确定性病例（大于或等于0.5）之后，F1分数提高到0.4787，Kappa增加到0.4258。不确定性直方图显示了正确和错误预测之间的明确分离，基于代理的模型提供了最准确的不确定性估计。通过结合不确定性感知提示集和基于代理的决策模型，本方法增强了LLMs在放射学报告中结构化数据提取的性能和可靠性，为具有高风险的医疗应用提供了更可解释和可信的解决方案。 

---
# LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient 

**Title (ZH)**: LLM �Drivern基准工厂：可靠、通用且高效 

**Authors**: Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.01683)  

**Abstract**: The rapid advancement of large language models (LLMs) has led to a surge in both model supply and application demands. To facilitate effective matching between them, reliable, generic and efficient benchmark generators are widely needed. However, human annotators are constrained by inefficiency, and current LLM benchmark generators not only lack generalizability but also struggle with limited reliability, as they lack a comprehensive evaluation framework for validation and optimization. To fill this gap, we first propose an automated and unbiased evaluation framework, structured around four dimensions and ten criteria. Under this framework, we carefully analyze the advantages and weaknesses of directly prompting LLMs as generic benchmark generators. To enhance the reliability, we introduce a series of methods to address the identified weaknesses and integrate them as BenchMaker. Experiments across multiple LLMs and tasks confirm that BenchMaker achieves superior or comparable performance to human-annotated benchmarks on all metrics, highlighting its generalizability and reliability. More importantly, it delivers highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation against MMLU-Pro), while taking only $0.005 and 0.38 minutes per sample. 

**Abstract (ZH)**: 大型语言模型（LLMs）的迅速发展导致了模型供应和应用需求的急剧增加。为了有效匹配供需双方，可靠、通用且高效的基准生成器需求广泛。然而，人工注释员受制于效率问题，当前的 LLM 基准生成器不仅缺乏普适性，而且在验证和优化方面表现出有限的可靠性。为弥补这一缺口，我们首先提出了一种自动化且无偏见的评估框架，该框架围绕四个维度和十个标准构建。在这一框架下，我们仔细分析了直接使用 LLM 进行基准生成的优缺点。为了提高可靠性，我们引入了一系列方法来解决识别出的缺点，并将这些方法整合为 BenchMaker。跨多个 LLM 和任务的实验结果表明，BenchMaker 在所有指标上的表现优于或可与人工标注的基准相媲美，突显了其普适性和可靠性。更重要的是，BenchMaker 在 12 个 LLM 上提供了高度一致的评估结果（与 MMLU-Pro 的 Pearson 相关系数为 0.967），并且每个样本只需花费 0.005 美元和 0.38 分钟。 

---
# The exception of humour: Iconicity, Phonemic Surprisal, Memory Recall, and Emotional Associations 

**Title (ZH)**: 幽默的例外：意指性、音位 surprisal、记忆检索和情感关联 

**Authors**: Alexander Kilpatrick, Maria Flaksman  

**Link**: [PDF](https://arxiv.org/pdf/2502.01682)  

**Abstract**: This meta-study explores the relationships between humor, phonemic bigram surprisal, emotional valence, and memory recall. Prior research indicates that words with higher phonemic surprisal are more readily remembered, suggesting that unpredictable phoneme sequences promote long-term memory recall. Emotional valence is another well-documented factor influencing memory, with negative experiences and stimuli typically being remembered more easily than positive ones. Building on existing findings, this study highlights that words with negative associations often exhibit greater surprisal and are easier to recall. Humor, however, presents an exception: while associated with positive emotions, humorous words also display heightened surprisal and enhanced memorability. 

**Abstract (ZH)**: 这篇元研究探讨了幽默、音素双联体 surprisal、情感效价和记忆回溯之间的关系。先前的研究表明，具有较高音素 surprisal 的单词更容易被记住，这暗示着不可预测的音素序列有助于长期记忆的回溯。情感效价是另一个已被充分研究的影响记忆的因素，负面的经历和刺激通常比正面的更容易被记住。基于现有发现，本研究指出，与负面情感相关联的词语通常表现出更高的 surprisal，并且更容易被回忆起来。然而，幽默却是一个例外：尽管与积极情感相关，但幽默词语也表现出较高的 surprisal 和更强的可记忆性。 

---
# Benchmark on Peer Review Toxic Detection: A Challenging Task with a New Dataset 

**Title (ZH)**: Peer Review 有害内容检测基准：一项具有新数据集的挑战性任务 

**Authors**: Man Luo, Bradley Peterson, Rafael Gan, Hari Ramalingame, Navya Gangrade, Ariadne Dimarogona, Imon Banerjee, Phillip Howard  

**Link**: [PDF](https://arxiv.org/pdf/2502.01676)  

**Abstract**: Peer review is crucial for advancing and improving science through constructive criticism. However, toxic feedback can discourage authors and hinder scientific progress. This work explores an important but underexplored area: detecting toxicity in peer reviews. We first define toxicity in peer reviews across four distinct categories and curate a dataset of peer reviews from the OpenReview platform, annotated by human experts according to these definitions. Leveraging this dataset, we benchmark a variety of models, including a dedicated toxicity detection model, a sentiment analysis model, several open-source large language models (LLMs), and two closed-source LLMs. Our experiments explore the impact of different prompt granularities, from coarse to fine-grained instructions, on model performance. Notably, state-of-the-art LLMs like GPT-4 exhibit low alignment with human judgments under simple prompts but achieve improved alignment with detailed instructions. Moreover, the model's confidence score is a good indicator of better alignment with human judgments. For example, GPT-4 achieves a Cohen's Kappa score of 0.56 with human judgments, which increases to 0.63 when using only predictions with a confidence score higher than 95%. Overall, our dataset and benchmarks underscore the need for continued research to enhance toxicity detection capabilities of LLMs. By addressing this issue, our work aims to contribute to a healthy and responsible environment for constructive academic discourse and scientific collaboration. 

**Abstract (ZH)**: 同行评审对于通过建设性的批评促进和改进科学至关重要。然而，有毒反馈可能会 discourourage 作者并阻碍科学进步。本研究探索了一个重要但尚未充分研究的领域：检测同行评审中的毒性言论。我们首先根据这四个不同的类别定义了同行评审中的毒性言论，并从 OpenReview 平台上收集了一个数据集，该数据集由人类专家根据这些定义进行标注。利用该数据集，我们对多种模型进行了基准测试，包括专门的毒性检测模型、情感分析模型、几种开源的大语言模型（LLMs）以及两个闭源的 LLMs。我们的实验探索了不同提示粒度（从粗粒度到细粒度的指令）对模型性能的影响。值得注意的是，像 GPT-4 这样的当前最先进的 LLMs，在简单提示下与人类判断的契合度较低，但在详细的指令下可以实现更高的契合度。此外，模型的置信度分数是与人类判断契合度较好的指标之一。例如，GPT-4 在简单的提示下与人类判断的 Cohen's Kappa 分数为 0.56，但在仅使用置信度分数高于 95% 的预测时，该分数提高到 0.63。总体而言，我们的数据集和基准测试突显了需要继续研究以增强 LLM 的毒性检测能力。通过解决这个问题，我们的工作旨在促进一个健康且负责任的环境，以支持建设性的学术讨论和科学合作。 

---
# Multilingual State Space Models for Structured Question Answering in Indic Languages 

**Title (ZH)**: 多语言状态空间模型在印地语结构化问答中的应用 

**Authors**: Arpita Vats, Rahul Raja, Mrinal Mathur, Vinija Jain, Aman Chadha  

**Link**: [PDF](https://arxiv.org/pdf/2502.01673)  

**Abstract**: The diversity and complexity of Indic languages present unique challenges for natural language processing (NLP) tasks, particularly in the domain of question answering (QA).To address these challenges, this paper explores the application of State Space Models (SSMs),to build efficient and contextually aware QA systems tailored for Indic languages. SSMs are particularly suited for this task due to their ability to model long-term and short-term dependencies in sequential data, making them well-equipped to handle the rich morphology, complex syntax, and contextual intricacies characteristic of Indian languages. We evaluated multiple SSM architectures across diverse datasets representing various Indic languages and conducted a comparative analysis of their performance. Our results demonstrate that these models effectively capture linguistic subtleties, leading to significant improvements in question interpretation, context alignment, and answer generation. This work represents the first application of SSMs to question answering tasks in Indic languages, establishing a foundational benchmark for future research in this domain. We propose enhancements to existing SSM frameworks, optimizing their applicability to low-resource settings and multilingual scenarios prevalent in Indic languages. 

**Abstract (ZH)**: 印度语族语言的多样性和复杂性为自然语言处理（NLP）任务，尤其是在问答（QA）领域，带来了独特的挑战。为了应对这些挑战，本文探讨了状态空间模型（SSMs）的应用，以构建符合印度语族语言需求的高效且上下文感知型QA系统。SSMs特别适用于此项任务，因为它们能够建模序列数据中的长期和短期依赖关系，使其能够很好地处理印度语言中丰富的形态、复杂的句法以及上下文中的细微差别。我们对多种SSM架构进行了评估，这些架构适用于代表不同印度语言的各种数据集，并进行了性能比较分析。我们的结果显示，这些模型有效地捕捉到了语言的细微差别，从而在问题理解、上下文对齐和答案生成方面取得了显著改进。这项工作是首次将SSMs应用于印度语族语言的问答任务，为未来在这个领域开展研究奠定了基础。我们提出了对现有SSM框架的改进，以提高其在资源有限和多语言场景中的适用性，这些场景在印度语族语言中非常普遍。 

---
# Explainable AI for Sentiment Analysis of Human Metapneumovirus (HMPV) Using XLNet 

**Title (ZH)**: 使用XLNet进行人类鼻病毒（HMPV）情感分析的可解释人工智能 

**Authors**: Md. Shahriar Hossain Apu, Md Saiful Islam, Tanjim Taharat Aurpa  

**Link**: [PDF](https://arxiv.org/pdf/2502.01663)  

**Abstract**: In 2024, the outbreak of Human Metapneumovirus (HMPV) in China, which later spread to the UK and other countries, raised significant public concern. While HMPV typically causes mild symptoms, its effects on vulnerable individuals prompted health authorities to emphasize preventive measures. This paper explores how sentiment analysis can enhance our understanding of public reactions to HMPV by analyzing social media data. We apply transformer models, particularly XLNet, achieving 93.50% accuracy in sentiment classification. Additionally, we use explainable AI (XAI) through SHAP to improve model transparency. 

**Abstract (ZH)**: 2024年，中国暴发的人代谢呼吸道病毒（HMPV）疫情随后传播至英国及其他国家，引起了公众的重要关注。虽然HMPV通常引起轻微症状，但其对脆弱人群的影响促使卫生部门强调预防措施。本文探讨了如何通过分析社交媒体数据来提升我们对HMPV公众反应的理解，特别关注情感分析的应用。我们采用了变换器模型，特别是XLNet，实现了93.50%的情感分类准确率。此外，我们还通过SHAP（SHapley Additive exPlanations）可解释人工智能方法提高模型的透明度。 

---
# Speculative Ensemble: Fast Large Language Model Ensemble via Speculation 

**Title (ZH)**: speculation猜测

《推测集合：通过推测加速大规模语言模型集成》

解释：这里的"Speculative Ensemble"中的"Speculative"在该语境下指的是推测、猜测的意思，这是算法中的一个策略，而不是通常的理解为“推测性的”。在学术翻译中，我们将它翻译为“推测”，同时在翻译标题时保持了英文中首字母大写的习惯。 

**Authors**: Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Xin Geng, Xu Yang  

**Link**: [PDF](https://arxiv.org/pdf/2502.01662)  

**Abstract**: Ensemble methods enhance Large Language Models (LLMs) by combining multiple models but suffer from high computational costs. In this paper, we introduce Speculative Ensemble, a novel framework that accelerates LLM ensembles without sacrificing performance, inspired by Speculative Decoding-where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with n models and theoretically prove that SE is never slower than a standard ensemble, typically achieving faster speed. Extensive experiments demonstrate speed improvements of 1.11x-2.23x over standard ensemble techniques without compromising generation quality. Our code is available at this https URL 

**Abstract (ZH)**: Ensemble方法通过结合多个模型来增强大型语言模型（LLM），但是会带来较高的计算成本。本文提出了一种名为推测性组合（Speculative Ensemble）的新颖框架，能够在不牺牲性能的前提下加速LLM组合。该方法受到推测性解码的启发，推测性解码中，一个小的提案模型依次生成令牌，而较大的目标模型则并行验证这些令牌。我们的方法基于两个关键洞察：（1）验证分布可以是提案模型和目标模型组合的分布；（2）交替模型作为提案者和验证者可以进一步提高效率。我们将该方法推广到n个模型的组合中，并从理论上证明，推测性组合（SE）从不会比标准组合慢，通常能够实现更快的速度。实验结果表明，在不牺牲生成质量的前提下，推测性组合比标准组合技术快1.11倍至2.23倍。代码详见：this https URL 

---
# Large Language Models' Accuracy in Emulating Human Experts' Evaluation of Public Sentiments about Heated Tobacco Products on Social Media 

**Title (ZH)**: 大型语言模型在模拟人类专家评估社交媒体上关于加热不燃烧烟草产品公众情绪准确性的研究 

**Authors**: Kwanho Kim, Soojong Kim  

**Link**: [PDF](https://arxiv.org/pdf/2502.01658)  

**Abstract**: Sentiment analysis of alternative tobacco products on social media is important for tobacco control research. Large Language Models (LLMs) can help streamline the labor-intensive human sentiment analysis process. This study examined the accuracy of LLMs in replicating human sentiment evaluation of social media messages about heated tobacco products (HTPs).
The research used GPT-3.5 and GPT-4 Turbo to classify 500 Facebook and 500 Twitter messages, including anti-HTPs, pro-HTPs, and neutral messages. The models evaluated each message up to 20 times, and their majority label was compared to human evaluators.
Results showed that GPT-3.5 accurately replicated human sentiment 61.2% of the time for Facebook messages and 57.0% for Twitter messages. GPT-4 Turbo performed better, with 81.7% accuracy for Facebook and 77.0% for Twitter. Using three response instances, GPT-4 Turbo achieved 99% of the accuracy of twenty instances. GPT-4 Turbo also had higher accuracy for anti- and pro-HTPs messages compared to neutral ones. Misclassifications by GPT-3.5 often involved anti- or pro-HTPs messages being labeled as neutral or irrelevant, while GPT-4 Turbo showed improvements across all categories.
In conclusion, LLMs can be used for sentiment analysis of HTP-related social media messages, with GPT-4 Turbo reaching around 80% accuracy compared to human experts. However, there's a risk of misrepresenting overall sentiment due to differences in accuracy across sentiment categories. 

**Abstract (ZH)**: 社交媒体上替代烟草产品的情绪分析对烟草控制研究至关重要。大型语言模型（LLMs）可以帮助简化情感分析的人工密集过程。本研究旨在评估LLMs在复制人类对加热不燃烧产品（HTPs）相关社交媒体信息的情绪评价方面的准确性。

研究使用了GPT-3.5和GPT-4 Turbo对500条Facebook信息和500条Twitter信息进行分类，内容包括反对HTPs、支持HTPs和中性信息。模型每条信息最多评估20次，结果与人类评估者进行比较。

结果显示，对于Facebook信息，GPT-3.5准确复制了人类情绪评价的61.2%，而对于Twitter信息则为57.0%。GPT-4 Turbo的表现更佳，分别达到了Facebook信息的81.7%和Twitter信息的77.0%的准确率。使用三种回复实例后，GPT-4 Turbo实现了20次回复实例99%的准确率。GPT-4 Turbo在反对和支持HTPs信息上的准确率也比中性信息高。GPT-3.5错误分类时常将反对或支持HTPs的信息标记为中性或无关信息，而GPT-4 Turbo在各个类别上的表现均有改善。

总之，LLMs可以用于HTPs相关社交媒体信息的情绪分析，GPT-4 Turbo的准确率达到了约80%与人类专家相当，但仍有因不同情感类别准确性差异而导致整体情绪误代表的风险。 

---
# Multi-Agent Design: Optimizing Agents with Better Prompts and Topologies 

**Title (ZH)**: 多agent系统设计：通过更好的提示和拓扑结构优化agent 

**Authors**: Han Zhou, Xingchen Wan, Ruoxi Sun, Hamid Palangi, Shariq Iqbal, Ivan Vulić, Anna Korhonen, Sercan Ö. Arık  

**Link**: [PDF](https://arxiv.org/pdf/2502.02533)  

**Abstract**: Large language models, employed as multiple agents that interact and collaborate with each other, have excelled at solving complex tasks. The agents are programmed with prompts that declare their functionality, along with the topologies that orchestrate interactions across agents. Designing prompts and topologies for multi-agent systems (MAS) is inherently complex. To automate the entire design process, we first conduct an in-depth analysis of the design space aiming to understand the factors behind building effective MAS. We reveal that prompts together with topologies play critical roles in enabling more effective MAS design. Based on the insights, we propose Multi-Agent System Search (MASS), a MAS optimization framework that efficiently exploits the complex MAS design space by interleaving its optimization stages, from local to global, from prompts to topologies, over three stages: 1) block-level (local) prompt optimization; 2) workflow topology optimization; 3) workflow-level (global) prompt optimization, where each stage is conditioned on the iteratively optimized prompts/topologies from former stages. We show that MASS-optimized multi-agent systems outperform a spectrum of existing alternatives by a substantial margin. Based on the MASS-found systems, we finally propose design principles behind building effective multi-agent systems. 

**Abstract (ZH)**: 大规模语言模型被用作多个相互交互和协作的代理，在解决复杂任务方面表现出色。这些代理被编程以声明其功能，并通过拓扑结构来协调跨代理的交互。设计多代理系统（Multi-Agent System, MAS）中的代理及其交互拓扑本身是复杂的过程。为了自动化整个设计过程，我们首先进行了深入分析，以理解构建有效MAS的驱动因素。我们发现，代理和拓扑在使MAS设计更有效方面发挥着关键作用。基于这些见解，我们提出了多代理系统搜索（Multi-Agent System Search, MASS）框架，该框架通过从局部到全局、从提示到拓扑的三个优化阶段交错其优化阶段，有效地利用了复杂的MAS设计空间。具体而言，这三个阶段包括：1）块级（局部）提示优化；2）工作流拓扑优化；3）工作流级（全局）提示优化，每个阶段均依赖于从前一阶段优化迭代得到的提示/拓扑。我们展示了经过MASS优化的多代理系统明显优于现有多种替代方案。基于MASS发现的系统，我们最终提出了构建有效多代理系统的若干设计原则。 

---
# Analyzing Similarity Metrics for Data Selection for Language Model Pretraining 

**Title (ZH)**: 分析数据选择中用于语言模型预训练的相似性度量方法 

**Authors**: Dylan Sam, Ayan Chakrabarti, Afshin Rostamizadeh, Srikumar Ramalingam, Gui Citovsky, Sanjiv Kumar  

**Link**: [PDF](https://arxiv.org/pdf/2502.02494)  

**Abstract**: Similarity between training examples is used to curate pretraining datasets for language models by many methods -- for diversification and to select examples similar to high-quality data. However, similarity is typically measured with off-the-shelf embedding models that are generic or trained for tasks such as retrieval. This paper introduces a framework to analyze the suitability of embedding models specifically for data curation in the language model pretraining setting. We quantify the correlation between similarity in the embedding space to similarity in pretraining loss between different training examples, and how diversifying in the embedding space affects pretraining quality. We analyze a variety of embedding models in our framework, with experiments using the Pile dataset for pretraining a 1.7B parameter decoder-only language model. We find that the embedding models we consider are all useful for pretraining data curation. Moreover, a simple approach of averaging per-token embeddings proves to be surprisingly competitive with more sophisticated embedding models -- likely because the latter are not designed specifically for pretraining data curation. Indeed, we believe our analysis and evaluation framework can serve as a foundation for the design of embedding models that specifically reason about similarity in pretraining datasets. 

**Abstract (ZH)**: 在多种方法下，根据训练样例之间的相似性来编纂语言模型的预训练数据集，以便实现多样性并选择与高质量数据相似的样例。然而，相似性通常通过通用的现成嵌入模型或为检索等任务训练的嵌入模型来衡量。本文提出了一种框架，用于分析嵌入模型在语言模型预训练数据编纂场景中的适用性。我们量化了嵌入空间中相似度与预训练损失之间差异性训练样例相似度的相关性，并研究了多样性在嵌入空间中的变化如何影响预训练质量。我们使用Pile数据集在1.7亿参数的解码器模型中进行预训练，分析了框架中的多种嵌入模型。结果显示，我们考虑的所有嵌入模型都适用于预训练数据编纂。此外，简单的平均单个词嵌入的方法证明了出乎意料的竞争性——可能是因为后者并非专门为此设计。事实上，我们认为我们的分析和评估框架可以作为设计专门针对预训练数据集相似性推理的嵌入模型的基础。 

---
# Rankify: A Comprehensive Python Toolkit for Retrieval, Re-Ranking, and Retrieval-Augmented Generation 

**Title (ZH)**: Rankify：一个全面的Python工具箱，用于检索、再排序及检索增强生成 

**Authors**: Abdelrahman Abdallah, Jamshid Mozafari, Bhawna Piryani, Mohammed Ali, Adam Jatowt  

**Link**: [PDF](https://arxiv.org/pdf/2502.02464)  

**Abstract**: Retrieval, re-ranking, and retrieval-augmented generation (RAG) are critical components of modern natural language processing (NLP) applications in information retrieval, question answering, and knowledge-based text generation. However, existing solutions are often fragmented, lacking a unified framework that easily integrates these essential processes. The absence of a standardized implementation, coupled with the complexity of retrieval and re-ranking workflows, makes it challenging for researchers to compare and evaluate different approaches in a consistent environment. While existing toolkits such as Rerankers and RankLLM provide general-purpose reranking pipelines, they often lack the flexibility required for fine-grained experimentation and benchmarking. In response to these challenges, we introduce \textbf{Rankify}, a powerful and modular open-source toolkit designed to unify retrieval, re-ranking, and RAG within a cohesive framework. Rankify supports a wide range of retrieval techniques, including dense and sparse retrievers, while incorporating state-of-the-art re-ranking models to enhance retrieval quality. Additionally, Rankify includes a collection of pre-retrieved datasets to facilitate benchmarking, available at Huggingface (this https URL). To encourage adoption and ease of integration, we provide comprehensive documentation (this http URL), an open-source implementation on GitHub(this https URL), and a PyPI package for effortless installation(this https URL). By providing a unified and lightweight framework, Rankify allows researchers and practitioners to advance retrieval and re-ranking methodologies while ensuring consistency, scalability, and ease of use. 

**Abstract (ZH)**: 检索、重新排序和检索增强生成（RAG）是现代自然语言处理（NLP）应用中信息检索、问答和知识驱动文本生成的关键组成部分。然而，现有的解决方案经常是分散的，缺乏一个易于集成这些关键过程的统一框架。由于缺乏标准化的实现，并且检索和重新排序工作流的复杂性，研究人员要在一致的环境中比较和评估不同方法变得困难。虽然已经有Rerankers和RankLLM等现成的工具包提供了通用的重新排序流水线，但它们往往缺乏细粒度实验和基准测试所需的灵活性。为应对这些挑战，我们引入了**Rankify**，一个功能强大且模块化的开源工具包，旨在将检索、重新排序和RAG统一在一个紧密联系的框架中。Rankify支持各种各样的检索技术，包括密集型和稀疏型检索器，同时结合最先进的重新排序模型以提高检索质量。此外，Rankify还包含了一系列预检索数据集，方便基准测试，可从Huggingface (这个<https://huggingface.co/spaces/rankify/rankify>) 获取。为了促进采用和集成，我们提供了全面的文档 (这个<https://github.com/rankify/rankify/wiki>)，GitHub上的开源实现 (这个<https://github.com/rankify/rankify>) 和PyPI包 (这个<https://pypi.org/project/rankify-wrapped/>)，以便轻松安装。通过提供一个统一且轻量级的框架，Rankify使研究人员和实践者能够推进检索和重新排序方法，同时确保一致性、可扩展性和易用性。 

---
# Avoiding spurious sharpness minimization broadens applicability of SAM 

**Title (ZH)**: 避免虚假锐利度最小化扩大了SAM的应用范围 

**Authors**: Sidak Pal Singh, Hossein Mobahi, Atish Agarwala, Yann Dauphin  

**Link**: [PDF](https://arxiv.org/pdf/2502.02407)  

**Abstract**: Curvature regularization techniques like Sharpness Aware Minimization (SAM) have shown great promise in improving generalization on vision tasks. However, we find that SAM performs poorly in domains like natural language processing (NLP), often degrading performance -- even with twice the compute budget. We investigate the discrepancy across domains and find that in the NLP setting, SAM is dominated by regularization of the logit statistics -- instead of improving the geometry of the function itself. We use this observation to develop an alternative algorithm we call Functional-SAM, which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the SAM perturbation also prevents spurious minimization, and when combined with Functional-SAM, it gives further improvements. Our proposed algorithms show improved performance over AdamW and SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including billion-parameter scale). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs). 

**Abstract (ZH)**: 类似于Sharpness Aware Minimization (Sharpness Aware Minimization, SAM)这样的曲率正则化技术在视觉任务中的泛化能力方面显示出巨大的潜力。然而，我们发现SAM在自然语言处理（NLP）等领域表现较差，通常会降低性能——即使在计算预算加倍的情况下也是如此。我们研究了不同领域的差异，发现NLP环境中SAM主要通过正则化逻辑概率统计来起作用，而不是通过改进函数本身的几何结构。基于这一观察，我们开发了一个新的算法，称为Functional-SAM，该算法仅通过修改神经网络实现的整体函数的统计量来正则化曲率，并避免通过逻辑概率操纵导致的错误最小化。此外，我们还提出，预处理SAM扰动也可以防止错误最小化，而当与Functional-SAM结合使用时，它可以进一步提高性能。我们提出的算法在固定的训练步数下，无论是固定长度训练还是沿用Chinchilla风格的训练设置，在各种模型规模（包括十亿参数规模）下，都显示出优于AdamW和SAM基线模型的性能。总体而言，我们的工作突出了更精确描述尖锐性的重要性，在扩大曲率正则化在大规模语言模型（LLMs）中的适用范围方面具有重要意义。 

---
# ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs 

**Title (ZH)**: ReSpark: 利用先前的数据报告作为参考生成新报告的LLM方法 

**Authors**: Yuan Tian, Chuhan Zhang, Xiaotong Wang, Sitong Pan, Weiwei Cui, Haidong Zhang, Dazhen Deng, Yingcai Wu  

**Link**: [PDF](https://arxiv.org/pdf/2502.02329)  

**Abstract**: Creating data reports is time-consuming, as it requires iterative exploration and understanding of data, followed by summarizing the insights. While large language models (LLMs) are powerful tools for data processing and text generation, they often struggle to produce complete data reports that fully meet user expectations. One significant challenge is effectively communicating the entire analysis logic to LLMs. Moreover, determining a comprehensive analysis logic can be mentally taxing for users. To address these challenges, we propose ReSpark, an LLM-based method that leverages existing data reports as references for creating new ones. Given a data table, ReSpark searches for similar-topic reports, parses them into interdependent segments corresponding to analytical objectives, and executes them with new data. It identifies inconsistencies and customizes the objectives, data transformations, and textual descriptions. ReSpark allows users to review real-time outputs, insert new objectives, and modify report content. Its effectiveness was evaluated through comparative and user studies. 

**Abstract (ZH)**: 生成数据报告耗时较长，因为这需要迭代地探索和理解数据，进而总结出洞察。虽然大型语言模型（LLMs）在数据处理和文本生成方面具有强大功能，但它们往往难以生成完全符合用户期望的完整数据报告。一个重要挑战是如何有效地向LLMs传达整个分析逻辑。此外，确定全面的分析逻辑对用户来说可能是一种认知负担。为了解决这些挑战，我们提出了ReSpark，一种基于LLM的方法，利用现有数据报告作为参考来创建新的报告。给定一个数据表格，ReSpark 搜索相关主题的报告，解析它们成为与分析目标相关的相互依存段落，并使用新的数据执行这些段落。它会识别不一致之处，并自定义目标、数据转换和文本描述。ReSpark 允许用户实时审查输出结果、插入新目标并修改报告内容。其效果通过对比性和用户研究进行了评估。 

---
# VaiBot: Shuttle Between the Instructions and Parameters 

**Title (ZH)**: VaiBot：指令与参数之间的桥梁 

**Authors**: Wangtao Sun, Haotian Xu, Huanxuan Liao, Xuanqing Yu, Zhongtao Jiang, Shizhu He, Jun Zhao, Kang Liu  

**Link**: [PDF](https://arxiv.org/pdf/2502.02315)  

**Abstract**: How to interact with LLMs through \emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. The code and data for this paper can be found at this https URL. 

**Abstract (ZH)**: 通过指令与大规模语言模型（LLM）互动的方式已被研究人员广泛研究。然而，之前的研究所处理的指令的出现与LLM在任务数据上的训练这两个过程被当作独立的过程，忽视了两者之间的内在统一性。本文提出了一种神经网络框架VaiBot，该框架结合了VAE（变分自编码器）和VIB（变异信息瓶颈），旨在统一建模、学习和推断LLM下的演绎和归纳任务。通过实验，我们证明了VaiBot在演绎能力方面与现有基线方法相当，而在归纳能力方面显著超越它们。我们还发现，VaiBot可以用一般的指令遵循数据进行扩展，并表现出优异的一次性归纳能力。最后，我们以一种协同方式整合了VaiBot的演绎和归纳过程。通过T-SNE降维，我们观察到其演绎归纳过程显著改善了训练参数的分布，使其在归纳推理任务中超越了基线方法。这篇论文的代码和数据可以在这个网址 https://github.com/example-repo 找到。 

---
# Can You Move These Over There? An LLM-based VR Mover for Supporting Object Manipulation 

**Title (ZH)**: 当然，以下是翻译内容：

《物体操作支持的基于LLM的VR搬运器：你可以把这些移到那边吗？》

这里的“LLM”指的是语言模型（Language Model），在翻译时可以根据上下文具体指代的内容来选择合适的术语或保持原文缩写。如果是指特定的技术或模型，可以进一步明确为“基于语言模型”或直接使用“LLM”。 

**Authors**: Xiangzhi Eric Wang, Zackary P. T. Sin, Ye Jia, Daniel Archer, Wynonna H. Y. Fong, Qing Li, Chen Li  

**Link**: [PDF](https://arxiv.org/pdf/2502.02201)  

**Abstract**: In our daily lives, we can naturally convey instructions for the spatial manipulation of objects using words and gestures. Transposing this form of interaction into virtual reality (VR) object manipulation can be beneficial. We propose VR Mover, an LLM-empowered solution that can understand and interpret the user's vocal instruction to support object manipulation. By simply pointing and speaking, the LLM can manipulate objects without structured input. Our user study demonstrates that VR Mover enhances user usability, overall experience and performance on multi-object manipulation, while also reducing workload and arm fatigue. Users prefer the proposed natural interface for broad movements and may complementarily switch to gizmos or virtual hands for finer adjustments. These findings are believed to contribute to design implications for future LLM-based object manipulation interfaces, highlighting the potential for more intuitive and efficient user interactions in VR environments. 

**Abstract (ZH)**: 在我们的日常生活中，我们自然能够通过语言和手势传达对物体的空间操作指令。将这种交互形式移植到虚拟现实（VR）物体操作中是有益的。我们提出了VR Mover，这是一种由语言模型（LLM）赋能的解决方案，能够理解和解释用户的语音指令以支持物体操作。用户只需指一指、说一说，LLM 就能够无需结构化输入地操作物体。我们的用户研究显示，VR Mover 能够提升用户的易用性、整体体验和多物体操作的表现，同时减少工作负担和手臂疲劳。用户更偏好提出的自然界面用于广泛的操作，并且可以在需要精细调整时切换到操纵杆或虚拟手。这些发现被认为有助于为未来基于语言模型的物体操作界面的设计提供启示，强调了在 VR 环境中实现更直观和高效用户交互的潜力。 

---
# Vulnerability Mitigation for Safety-Aligned Language Models via Debiasing 

**Title (ZH)**: 通过去偏见方法减轻安全对齐语言模型的脆弱性 

**Authors**: Thien Q. Tran, Akifumi Wachi, Rei Sato, Takumi Tanabe, Youhei Akimoto  

**Link**: [PDF](https://arxiv.org/pdf/2502.02153)  

**Abstract**: Safety alignment is an essential research topic for real-world AI applications. Despite the multifaceted nature of safety and trustworthiness in AI, current safety alignment methods often focus on a comprehensive notion of safety. By carefully assessing models from the existing safety-alignment methods, we found that, while they generally improved overall safety performance, they failed to ensure safety in specific categories. Our study first identified the difficulty of eliminating such vulnerabilities without sacrificing the model's helpfulness. We observed that, while smaller KL penalty parameters, increased training iterations, and dataset cleansing can enhance safety, they do not necessarily improve the trade-off between safety and helpfulness. We discovered that safety alignment could even induce undesired effects and result in a model that prefers generating negative tokens leading to rejective responses, regardless of the input context. To address this, we introduced a learning-free method, Token-level Safety-Debiased Inference (TSDI), to estimate and correct this bias during the generation process using randomly constructed prompts. Our experiments demonstrated that our method could enhance the model's helpfulness while maintaining safety, thus improving the trade-off Pareto-front. 

**Abstract (ZH)**: 安全对齐是实际应用中AI系统的关键研究课题。尽管AI中的安全性和可信性具有多维性，当前的安全对齐方法往往侧重于全面的安全概念。通过仔细评估现有安全对齐方法中的模型，我们发现尽管它们通常提升了整体安全性能，但未能保证特定类别中的安全性。我们的研究首先识别出在不牺牲模型帮助性的情况下消除这些漏洞的难度。我们观察到，虽然较小的KL惩罚参数、增加训练迭代次数和数据集清理可以在一定程度上提升安全性，但这些方法并不一定能改善安全性和帮助性之间的权衡。我们发现，安全对齐甚至可能导致不希望的效果，生成带有负面token的模型，从而产生拒绝性响应，无视输入上下文。为解决这一问题，我们提出了一种无需学习的方法——Token级别安全去偏差推断（TSDI），利用随机构造的提示，在生成过程中估计并矫正这种偏差。实验结果显示，我们的方法能够在保持安全性的前提下提升模型的帮助性，从而改善效率前沿。 

---
# Risk-Aware Driving Scenario Analysis with Large Language Models 

**Title (ZH)**: 使用大型语言模型进行风险意识驾驶场景分析 

**Authors**: Yuan Gao, Mattia Piccinini, Johannes Betz  

**Link**: [PDF](https://arxiv.org/pdf/2502.02145)  

**Abstract**: Large Language Models (LLMs) can capture nuanced contextual relationships, reasoning, and complex problem-solving. By leveraging their ability to process and interpret large-scale information, LLMs have shown potential to address domain-specific challenges, including those in autonomous driving systems. This paper proposes a novel framework that leverages LLMs for risk-aware analysis of generated driving scenarios. We hypothesize that LLMs can effectively evaluate whether driving scenarios generated by autonomous driving testing simulators are safety-critical. To validate this hypothesis, we conducted an empirical evaluation to assess the effectiveness of LLMs in performing this task. This framework will also provide feedback to generate the new safety-critical scenario by using adversarial method to modify existing non-critical scenarios and test their effectiveness in validating motion planning algorithms. Code and scenarios are available at: this https URL 

**Abstract (ZH)**: 大型语言模型（LLMs）能够捕捉到细腻的上下文关系、推理以及复杂的解决问题的能力。通过利用它们处理和解释大规模信息的能力，LLMs展示了应对特定领域挑战的潜力，包括自主驾驶系统中的问题。本文提出了一种新颖的框架，利用LLMs进行生成驾驶场景的风险意识分析。我们假设LLMs能够有效评估由自主驾驶测试模拟器生成的驾驶场景是否具备安全性关键性。为了验证这一假设，我们进行了实证研究，评估LLMs在此任务上的有效性。该框架还将提供反馈，通过使用对抗性方法修改现有的非关键性场景以生成新的安全性关键性场景，并测试其在验证运动规划算法方面的有效性。相关代码和场景可访问以下链接：[这里提供具体的URL链接] 

---
# Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign 

**Title (ZH)**: 面向大型语言模型的稳健且安全的代码水印技术：基于ML和Crypto的联合设计 

**Authors**: Ruisi Zhang, Neusha Javidnia, Nojan Sheybani, Farinaz Koushanfar  

**Link**: [PDF](https://arxiv.org/pdf/2502.02068)  

**Abstract**: This paper introduces RoSe, the first-of-its-kind ML/Crypto codesign watermarking framework that regulates LLM-generated code to avoid intellectual property rights violations and inappropriate misuse in software development. High-quality watermarks adhering to the detectability-fidelity-robustness tri-objective are limited due to codes' low-entropy nature. Watermark verification, however, often needs to reveal the signature and requires re-encoding new ones for code reuse, which potentially compromising the system's usability. To overcome these challenges, RoSe obtains high-quality watermarks by training the watermark insertion and extraction modules end-to-end to ensure (i) unaltered watermarked code functionality and (ii) enhanced detectability and robustness leveraging pre-trained CodeT5 as the insertion backbone to enlarge the code syntactic and variable rename transformation search space. In the deployment, RoSe uses zero-knowledge proofs for secure verification without revealing the underlying signatures. Extensive evaluations demonstrated RoSe achieves high detection accuracy while preserving the code functionality. RoSe is also robust against attacks and provides efficient secure watermark verification. 

**Abstract (ZH)**: 本文介绍了一种首创的机器学习/密码学协同设计水印框架RoSe，该框架旨在规范由大规模语言模型（LLM）生成的代码，以避免知识产权侵权和不当滥用问题。由于代码的低熵特性，高质量的水印（其检测性、忠实性和鲁棒性三方面并重）受到限制。然而，水印验证通常需要揭示签名，并且在代码复用时需要重新编码新的水印，这可能会影响系统的实用性。为克服这些挑战，RoSe 通过将水印插入和提取模块进行端到端训练，以确保（i）水印代码的功能不改变，并（ii）利用预训练的CodeT5作为插入主干网络，扩大代码语法和变量重命名转换的搜索空间，从而提高检测性和鲁棒性。在部署过程中，RoSe 使用零知识证明进行安全验证，而不泄露底层签名。广泛的实际评估表明，RoSe 在保持代码功能的同时，实现了高检测准确性。RoSe 还具有抵抗攻击的能力，并提供高效的安全水印验证。 

---
# AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement 

**Title (ZH)**: AdaptBot：将大规模语言模型与知识图谱及人性输入相结合，实现从通用到具体任务分解与知识精炼 

**Authors**: Shivam Singh, Karthik Swaminathan, Nabanita Dash, Ramandeep Singh, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna  

**Link**: [PDF](https://arxiv.org/pdf/2502.02067)  

**Abstract**: Embodied agents assisting humans are often asked to complete a new task in a new scenario. An agent preparing a particular dish in the kitchen based on a known recipe may be asked to prepare a new dish or to perform cleaning tasks in the storeroom. There may not be sufficient resources, e.g., time or labeled examples, to train the agent for these new situations. Large Language Models (LLMs) trained on considerable knowledge across many domains are able to predict a sequence of abstract actions for such new tasks and scenarios, although it may not be possible for the agent to execute this action sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks and scenarios. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation over cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM output. 

**Abstract (ZH)**: 在新的场景中执行新任务的实体代理经常被要求完成未接触过的任务。例如，基于已知食谱在厨房里准备某种菜肴的代理可能被要求准备新菜肴或对储藏室进行清洁。由于缺乏资源，如时间或标注数据，可能不足以对这些新情况进行训练。大规模语言模型（LLMs）通过对多个领域大量知识的训练，能够预测此类新任务和场景的抽象动作序列，但由于任务、代理或领域特定的约束，代理可能无法执行这些动作序列。我们的框架通过利用LLM提供的通用预测和知识图谱（KG）中编码的先验领域特定知识，使代理能够快速适应新任务和场景。同时，机器也会根据需要请求并利用人类输入来完善其现有知识。通过对烹饪和清洁任务在模拟领域进行实验评估，结果显示，LLM、KG和人类输入之间的互动能够带来显著的性能提升，相比仅使用LLM输出效果更佳。 

---
# Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments 

**Title (ZH)**: 预见与行动：将大语言模型与经典规划相结合以在家庭环境中高效执行任务 

**Authors**: Raghav Arora, Shivam Singh, Karthik Swaminathan, Ahana Datta, Snehasis Banerjee, Brojeshwar Bhowmick, Krishna Murthy Jatavallabhula, Mohan Sridharan, Madhava Krishna  

**Link**: [PDF](https://arxiv.org/pdf/2502.02066)  

**Abstract**: Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks. 

**Abstract (ZH)**: 执行诸如整理床铺或准备早餐等家务任务的辅助代理通常会计算和执行完成单一任务的操作。然而，通过预测即将执行的任务并计算一个能够同时完成这些任务的操作序列，效率可以得到提升。当前最先进的任务预测方法使用数据驱动的深度网络和大语言模型（LLMs），但这些方法通常是在高层任务的水平上进行的，并且需要大量的训练示例。我们的框架通过少量提示利用LLMs的通用知识来进行高层任务预测，并将预测的任务作为目标，在经典规划系统中计算实现这些目标的一系列细粒度动作。我们已在VirtualHome环境中将该框架的能力应用于现实场景，并展示了与不考虑即将执行的任务的系统相比，执行时间减少了31%。 

---
# Efficient Domain Adaptation of Multimodal Embeddings using Constrastive Learning 

**Title (ZH)**: 使用对比学习进行多模态嵌入的高效领域自适应 

**Authors**: Georgios Margaritis, Periklis Petridis, Dimitris J. Bertsimas  

**Link**: [PDF](https://arxiv.org/pdf/2502.02048)  

**Abstract**: Recent advancements in machine learning (ML), natural language processing (NLP), and foundational models have shown promise for real-life applications in critical, albeit compute-constrainted fields like healthcare.
In such areas, combining foundational models with supervised ML offers potential for automating tasks like diagnosis and treatment planning, but the limited availability of onsite computational resources pose significant challenges before applying these technologies effectively: Current approaches either yield subpar results when using pretrained models without task-specific adaptation, or require substantial computational resources for fine-tuning, which is often a barrier to entry in such environments.
This renders them inaccessible in applications where performance and quality standards are high, but computational resources are scarce.
To bridge the gap between best-in-class performance and accessibility, we propose a novel method for adapting foundational, multimodal embeddings to downstream tasks, without the need of expensive fine-tuning processes.
Our method leverages frozen embeddings from Large Language Models (LLMs) and Vision Models, and uses contrastive learning to train a small, task-specific nonlinear projection that can be used in the downstream task, without having to fine-tune the original foundational models.
We show that this efficient procedure leads to significant performance improvements across various downstream tasks, and perhaps more importantly with minimal computational overhead, offering a practical solution for the use of advanced, foundational ML models in resource-constrained settings. 

**Abstract (ZH)**: 近年来，机器学习（ML）、自然语言处理（NLP）以及基础模型的进展在诸如医疗保健等关键但计算资源受限的领域中展现了应用的潜力。在这些领域中，将基础模型与监督式ML相结合可以在自动化诊断和治疗规划等任务上提供可能性，但现场计算资源的有限供应在有效应用这些技术时提出了重大挑战：当前的方法要么在使用预训练模型而无需特定任务调整时结果不佳，要么需要大量的计算资源进行调优，而这通常是这些环境中进入的技术障碍。

这使得在高性能和高质量标准要求高但计算资源稀缺的应用场景中，这些技术难以使用。为缩小顶级性能与易用性之间的差距，我们提出了一种新型方法，用于适应基础的多模态嵌入，而无需昂贵的调优过程。

我们的方法利用大型语言模型（LLMs）和视觉模型中的冻结嵌入，并使用对比学习训练一个小的、特定任务的非线性投影，该投影可以直接应用于下游任务，而不需要重新调整个基础模型。我们展示了这种高效的方法在各种下游任务上带来了显著的性能提升，并且更为重要的是，这种提升伴随着最少的计算开销，为在计算资源受限的环境中使用先进的基础ML模型提供了实际解决方案。 

---
# Layer by Layer: Uncovering Hidden Representations in Language Models 

**Title (ZH)**: 逐层揭示：揭开语言模型中隐藏表示的秘密 

**Authors**: Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, Ravid Shwartz-Ziv  

**Link**: [PDF](https://arxiv.org/pdf/2502.02013)  

**Abstract**: From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks and comparisons across model architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features. These findings challenge the standard focus on final-layer embeddings and open new directions for model analysis and optimization, including strategic use of mid-layer representations for more robust and accurate AI systems. 

**Abstract (ZH)**: 从提取特征到生成文本，大型语言模型（LLMs）的输出通常依赖于其最后一层，沿袭了早期层仅捕获低级线索的常规智慧。然而，我们的分析表明，中间层可以编码更丰富的内容表示，往往在多种下游任务上提高性能。为了解释和量化这些隐藏层的属性，我们提出了一种基于信息论、几何学和输入扰动不变性的统一表示质量度量框架。该框架揭示了每层模型在信息压缩和信号保留之间的平衡，解释了为什么中间深度的嵌入可以超过最后一层的性能。通过在32项文本嵌入任务上的广泛实验，并且在模型结构（变换器、状态空间模型）和领域（语言、视觉）之间进行比较，我们证明中间层始终提供更强的特征表示。这些发现挑战了对最终层嵌入的常规关注，并为模型分析和优化开辟了新的方向，包括战略性地利用中间层表示以构建更稳健和准确的AI系统。 

---
# Fairness through Difference Awareness: Measuring Desired Group Discrimination in LLMs 

**Title (ZH)**: 通过差异意识实现公平：测量LLM中期望的群体歧视 

**Authors**: Angelina Wang, Michelle Phan, Daniel E. Ho, Sanmi Koyejo  

**Link**: [PDF](https://arxiv.org/pdf/2502.01926)  

**Abstract**: Algorithmic fairness has conventionally adopted a perspective of racial color-blindness (i.e., difference unaware treatment). We contend that in a range of important settings, group difference awareness matters. For example, differentiating between groups may be necessary in legal contexts (e.g., the U.S. compulsory draft applies to men but not women) and harm assessments (e.g., calling a girl a terrorist may be less harmful than calling a Muslim person one). In our work we first introduce an important distinction between descriptive (fact-based), normative (value-based), and correlation (association-based) benchmarks. This distinction is significant because each category requires distinct interpretation and mitigation tailored to its specific characteristics. Then, we present a benchmark suite composed of eight different scenarios for a total of 16k questions that enables us to assess difference awareness. Finally, we show results across ten models that demonstrate difference awareness is a distinct dimension of fairness where existing bias mitigation strategies may backfire. 

**Abstract (ZH)**: 算法公平性传统上采用一种种族盲目性的视角（即无差别的对待）。我们认为在一系列重要情境中，群体差异意识是必要的。例如，在法律情境中（例如，在美国的义务兵役仅适用于男性而不适用于女性）和危害评估中（例如，称呼一名女孩为恐怖分子比称呼一名穆斯林人为恐怖分子造成的危害更小），区分不同群体可能是必需的。在我们的研究中，我们首先引入了描述性（基于事实的）、规范性（基于价值观的）和关联性（基于关联的）基准之间的一个重要区分。这一区分很重要，因为每种类别都需要根据其特定特征进行不同的解释和缓解。然后，我们提供了一个由八种不同情境组成的基准套件，共计16,000个问题，以评估差异意识。最后，我们展示了涵盖十种不同模型的结果，这些结果表明差异意识是公平性的一个独立维度，而现有的偏差缓解策略在这种情况下可能会适得其反。 

---
# Training and Evaluating with Human Label Variation: An Empirical Study 

**Title (ZH)**: 人类标注变异的训练与评估：一项实证研究 

**Authors**: Kemal Kurniawan, Meladel Mistica, Timothy Baldwin, Jey Han Lau  

**Link**: [PDF](https://arxiv.org/pdf/2502.01891)  

**Abstract**: Human label variation (HLV) challenges the standard assumption that an example has a single ground truth, instead embracing the natural variation in human labelling to train and evaluate models. While various training methods and metrics for HLV have been proposed, there has been no systematic meta-evaluation of HLV evaluation metrics, contributing to the lack of clarity in the best HLV training method. We propose new evaluation metrics and training methods and empirically meta-evaluate HLV evaluation metrics. We find that training on either disaggregated annotations or soft labels often performs best across metrics, and that our proposed soft metric correlates best with human preference. 

**Abstract (ZH)**: 人类标签变异（HLV）挑战了标准假设，即一个示例只有一个正确的地面真实值，而是通过接受人类标签中自然存在的变异来训练和评估模型。虽然已经提出了各种针对HLV的训练方法和度量标准，但还没有对HLV评估指标进行系统的元评估，从而导致对于最佳HLV训练方法缺乏清晰的认识。我们提出新的评估指标和训练方法，并通过实证方式对HLV评估指标进行了元评估。我们发现，无论是基于拆分标签还是软标签进行训练，通常在各种指标中表现最佳，而且我们提出的软指标与人类偏好相关度最高。 

---
# Soup-of-Experts: Pretraining Specialist Models via Parameters Averaging 

**Title (ZH)**: 专家 soup 模型：通过参数平均预训练专门模型 

**Authors**: Pierre Ablin, Angelos Katharopoulos, Skyler Seto, David Grangier  

**Link**: [PDF](https://arxiv.org/pdf/2502.01804)  

**Abstract**: Machine learning models are routinely trained on a mixture of different data domains. Different domain weights yield very different downstream performances. We propose the Soup-of-Experts, a novel architecture that can instantiate a model at test time for any domain weights with minimal computational cost and without re-training the model. Our architecture consists of a bank of expert parameters, which are linearly combined to instantiate one model. We learn the linear combination coefficients as a function of the input domain weights. To train this architecture, we sample random domain weights, instantiate the corresponding model, and backprop through one batch of data sampled with these domain weights. We demonstrate how our approach obtains small specialized models on several language modeling tasks quickly. Soup-of-Experts are particularly appealing when one needs to ship many different specialist models quickly under a model size constraint. 

**Abstract (ZH)**: 机器学习模型通常在不同数据域的混合数据上进行训练。不同的域权重会导致下游性能的巨大差异。我们提出了一种新颖的Soup-of-Experts架构，可以在测试时以较低的计算成本为任意域权重实例化模型，而无需重新训练模型。该架构包括一组专家参数，这些参数通过线性组合来实例化一个模型。我们将线性组合系数学习为输入域权重的函数。为了训练此架构，我们随机采样域权重，实例化相应模型，并通过这些域权重采样的一个数据批次进行反向传播。我们展示了该方法如何在多种语言建模任务中快速获得小而专一的模型。当在模型大小受限的情况下需要快速部署多种不同领域的小模型时，Soup-of-Experts架构尤为令人感兴趣。 

---
# CTC-DRO: Robust Optimization for Reducing Language Disparities in Speech Recognition 

**Title (ZH)**: CTC-DRO：减少语音识别中语言差异的鲁棒优化方法 

**Authors**: Martijn Bartelds, Ananjan Nandi, Moussa Koulako Bala Doumbouya, Dan Jurafsky, Tatsunori Hashimoto, Karen Livescu  

**Link**: [PDF](https://arxiv.org/pdf/2502.01777)  

**Abstract**: Modern deep learning models often achieve high overall performance, but consistently fail on specific subgroups. Group distributionally robust optimization (group DRO) addresses this problem by minimizing the worst-group loss, but it fails when group losses misrepresent performance differences between groups. This is common in domains like speech, where the widely used connectionist temporal classification (CTC) loss scales with input length and varies with linguistic and acoustic properties, leading to spurious differences between group losses. We present CTC-DRO, which addresses the shortcomings of the group DRO objective by smoothing the group weight update to prevent overemphasis on consistently high-loss groups, while using input length-matched batching to mitigate CTC's scaling issues. We evaluate CTC-DRO on the task of multilingual automatic speech recognition (ASR) across five language sets from the ML-SUPERB 2.0 benchmark. CTC-DRO consistently outperforms group DRO and CTC-based baseline models, reducing the worst-language error by up to 65.9% and the average error by up to 47.7%. CTC-DRO can be applied to ASR with minimal computational costs, and offers the potential for reducing group disparities in other domains with similar challenges. 

**Abstract (ZH)**: 现代深度学习模型通常能够实现整体高性能，但在特定子群体中却常常表现不佳。群体分布鲁棒优化（group DRO）通过最小化最差群体的损失来解决这一问题，但当群体损失不能真实反映群体之间的性能差异时便会失效。这种情况在语音领域尤为常见，在该领域广泛使用的连接主义时序分类（CTC）损失随着输入长度的变化而变化，并且与语言和声学特性有关，导致群体损失之间出现虚假的差异。我们提出了CTC-DRO，该方法通过平滑群体权重更新来防止过分强调始终表现差的群体，同时使用输入长度匹配的批量处理来缓解CTC的标度问题。我们在ML-SUPERB 2.0基准中的五种语言集上的多语言自动语音识别（ASR）任务上评估了CTC-DRO。CTC-DRO在各个方面的性能均优于群体DRO和基于CTC的基本模型，可分别将最差语言错误率降低65.9%和平均错误率降低47.7%。CTC-DRO可以在几乎不增加计算成本的情况下应用于ASR，并且具有在其他具有类似挑战的领域中减少群体差距的潜在能力。 

---
# ACECODER: Acing Coder RL via Automated Test-Case Synthesis 

**Title (ZH)**: ACECODER：通过自动化测试用例合成提升编码强化学习性能 

**Authors**: Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, Wenhu Chen  

**Link**: [PDF](https://arxiv.org/pdf/2502.01718)  

**Abstract**: Most progress in recent coder models has been driven by supervised fine-tuning (SFT), while the potential of reinforcement learning (RL) remains largely unexplored, primarily due to the lack of reliable reward data/model in the code domain. In this paper, we address this challenge by leveraging automated large-scale test-case synthesis to enhance code model training. Specifically, we design a pipeline that generates extensive (question, test-cases) pairs from existing code data. Using these test cases, we construct preference pairs based on pass rates over sampled programs to train reward models with Bradley-Terry loss. It shows an average of 10-point improvement for Llama-3.1-8B-Ins and 5-point improvement for Qwen2.5-Coder-7B-Ins through best-of-32 sampling, making the 7B model on par with 236B DeepSeek-V2.5. Furthermore, we conduct reinforcement learning with both reward models and test-case pass rewards, leading to consistent improvements across HumanEval, MBPP, BigCodeBench, and LiveCodeBench (V4). Notably, we follow the R1-style training to start from Qwen2.5-Coder-base directly and show that our RL training can improve model on HumanEval-plus by over 25\% and MBPP-plus by 6\% for merely 80 optimization steps. We believe our results highlight the huge potential of reinforcement learning in coder models. 

**Abstract (ZH)**: 近年来，大多数编码器模型的进步主要得益于监督微调（SFT），而强化学习（RL）的应用潜力尚未得到充分探索，主要原因是代码领域缺乏可靠的奖励数据或模型。在本文中，我们通过利用自动化大规模测试案例合成来解决这一挑战，以提升编码模型的训练效果。具体而言，我们设计了一条生产线，从现有代码数据中生成大量的（问题，测试案例）对。利用这些测试案例，我们基于采样程序的通过率构建偏好对，使用Bradley-Terry损失来训练奖励模型。在最佳的32次采样中，Llama-3.1-8B-Ins的平均提升幅度为10点，Qwen2.5-Coder-7B-Ins的平均提升幅度为5点，使得7B模型与236B的DeepSeek-V2.5相比持平。此外，我们使用这两种奖励模型和测试案例通过奖励来进行强化学习，从而在HumanEval、MBPP、BigCodeBench和LiveCodeBench（V4）中保持一致的改进。特别地，我们遵循R1训练风格，直接从Qwen2.5-Coder-base开始，展示了仅80步优化后，我们的RL训练可以将HumanEval-plus模型的性能提高超过25%，MBPP-plus模型的性能提高6%。我们相信我们的结果突显了强化学习在编码器模型中的巨大潜力。 

---
# QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning 

**Title (ZH)**: QLESS：一种用于大规模语言模型微调中的数据估值与选择的量化方法 

**Authors**: Moses Ananta, Muhammad Farid Adilazuarda, Zayd Muhammad Kawakibi Zuhri, Ayu Purwarianti, Alham Fikri Aji  

**Link**: [PDF](https://arxiv.org/pdf/2502.01703)  

**Abstract**: Fine-tuning large language models (LLMs) is often constrained by the computational costs of processing massive datasets. We propose \textbf{QLESS} (Quantized Low-rank Gradient Similarity Search), which integrates gradient quantization with the LESS framework to enable memory-efficient data valuation and selection. QLESS employs a two-step compression process: first, it obtains low-dimensional gradient representations through LoRA-based random projection; then, it quantizes these gradients to low-bitwidth representations. Experiments on multiple LLM architectures (LLaMA, Mistral, Qwen) and benchmarks (MMLU, BBH, TyDiQA) show that QLESS achieves comparable data selection performance to LESS while reducing memory usage by up to 16x. Even 1-bit gradient quantization preserves data valuation quality. These findings underscore QLESS as a practical, scalable approach to identifying informative examples within strict memory constraints. 

**Abstract (ZH)**: 将下面的论文内容或标题翻译成中文，要符合学术规范：

大规模语言模型（LLM）的微调通常受到处理大规模数据集的计算成本限制。我们提出了一种名为**QLESS（量化低秩梯度相似性搜索）**的方法，该方法结合了梯度量化与LESS框架，以实现高效的数据估值和选择。QLESS 使用两步压缩过程：首先，它通过基于LoRA的随机投影获得低维度的梯度表示；然后，它将这些梯度量化为低位宽表示。在多个LLM架构（LLaMA、Mistral、Qwen）和基准测试集（MMLU、BBH、TyDiQA）上的实验结果显示，QLESS 在减少内存使用最多16倍的同时，实现了与LESS相当的数据选择性能。即使是1位宽梯度量化也能保持数据估值质量。这些发现突显了QLESS 在严格内存约束下识别有用示例的一种实用且可扩展的方法。 

---
# Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection 

**Title (ZH)**: 具有固有鉴别特征利用的多模态逆注意力网络用于虚假新闻检测 

**Authors**: Tianlin Zhang, En Yu, Yi Shao, Shuai Li, Sujuan Hou, Jiande Sun  

**Link**: [PDF](https://arxiv.org/pdf/2502.01699)  

**Abstract**: Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection. 

**Abstract (ZH)**: 多模态假新闻检测由于其对社会安全的深远影响而引起了广泛关注。虽然现有的方法在理解跨模态一致性方面做出了贡献，但它们往往未能充分利用模态特定的表示以及明确的差异特征。为了解决这些问题，我们提出了一种多模态反向注意网络（MIAN），这是一个新颖的框架，它基于新闻内容探索内在判别特征，以推进假新闻检测。具体而言，MIAN 引入了一个分层学习模块，通过局部到全局和局部到局部的交互捕捉多模态内的多样性关系，从而生成增强的一模态表示，以提高在多模态内部检测假新闻的能力。此外，一种跨模态交互模块利用共注意机制建立和建模精炼的一模态表示之间的依赖关系，促进跨模态的无缝语义整合。为明确提取不一致性特征，我们提出了一种反向注意机制，该机制有效地突出了假新闻在多模态内外引入的冲突模式和语义偏差。在基准数据集上的广泛实验表明，MIAN 显著优于现有的最佳方法，突显了其在通过增强多模态假新闻检测来提升社会安全方面的关键贡献。 

---
# Automated Extraction of Spatio-Semantic Graphs for Identifying Cognitive Impairment 

**Title (ZH)**: 自动化提取空间语义图以识别认知 impairment 

**Authors**: Si-Ioi Ng, Pranav S. Ambadi, Kimberly D. Mueller, Julie Liss, Visar Berisha  

**Link**: [PDF](https://arxiv.org/pdf/2502.01685)  

**Abstract**: Existing methods for analyzing linguistic content from picture descriptions for assessment of cognitive-linguistic impairment often overlook the participant's visual narrative path, which typically requires eye tracking to assess. Spatio-semantic graphs are a useful tool for analyzing this narrative path from transcripts alone, however they are limited by the need for manual tagging of content information units (CIUs). In this paper, we propose an automated approach for estimation of spatio-semantic graphs (via automated extraction of CIUs) from the Cookie Theft picture commonly used in cognitive-linguistic analyses. The method enables the automatic characterization of the visual semantic path during picture description. Experiments demonstrate that the automatic spatio-semantic graphs effectively differentiate between cognitively impaired and unimpaired speakers. Statistical analyses reveal that the features derived by the automated method produce comparable results to the manual method, with even greater group differences between clinical groups of interest. These results highlight the potential of the automated approach for extracting spatio-semantic features in developing clinical speech models for cognitive impairment assessment. 

**Abstract (ZH)**: 现有的方法在分析图片描述中的语言内容以评估认知语言障碍时，往往忽略了参与者视觉叙事路径的重要性，这通常需要借助眼动追踪进行评估。空间语义图是一种有用的工具，可以仅从转录中分析这部分叙事路径，但其受到手动标注内容信息单元（CIUs）的限制。在本文中，我们提出了一种自动方法，通过自动提取CIUs来估计空间语义图（以常用的“饼干偷窃”图片为例），该方法能够自动刻画在图片描述过程中视觉语义路径。实验结果表明，自动生成的空间语义图能够有效地区分认知受损和无受损的讲话者。统计分析表明，由自动方法提取的特征与手动方法提取的特征在临床感兴趣组别间的差异相当甚至更大。这些结果突显了自动方法在从临床语音模型中提取空间语义特征方面的潜在价值，能够用于认知障碍评估中的应用。

（注：这里的“饼干偷窃”应是原文中使用的“Cookie Theft picture”的中文翻译，具体的图片描述或情境内容可能由于上下文不同会有细微变化，但“饼干偷窃”这一情境在认知语言学研究中是作为固定例证使用的。） 

---
# LIBRA: Measuring Bias of Large Language Model from a Local Context 

**Title (ZH)**: LIBRA：从局部语境测量大型语言模型的偏见 

**Authors**: Bo Pang, Tingrui Qiao, Caroline Walker, Chris Cunningham, Yun Sing Koh  

**Link**: [PDF](https://arxiv.org/pdf/2502.01679)  

**Abstract**: Large Language Models (LLMs) have significantly advanced natural language processing applications, yet their widespread use raises concerns regarding inherent biases that may reduce utility or harm for particular social groups. Despite the advancement in addressing LLM bias, existing research has two major limitations. First, existing LLM bias evaluation focuses on the U.S. cultural context, making it challenging to reveal stereotypical biases of LLMs toward other cultures, leading to unfair development and use of LLMs. Second, current bias evaluation often assumes models are familiar with the target social groups. When LLMs encounter words beyond their knowledge boundaries that are unfamiliar in their training data, they produce irrelevant results in the local context due to hallucinations and overconfidence, which are not necessarily indicative of inherent bias. This research addresses these limitations with a Local Integrated Bias Recognition and Assessment Framework (LIBRA) for measuring bias using datasets sourced from local corpora without crowdsourcing. Implementing this framework, we develop a dataset comprising over 360,000 test cases in the New Zealand context. Furthermore, we propose the Enhanced Idealized CAT Score (EiCAT), integrating the iCAT score with a beyond knowledge boundary score (bbs) and a distribution divergence-based bias measurement to tackle the challenge of LLMs encountering words beyond knowledge boundaries. Our results show that the BERT family, GPT-2, and Llama-3 models seldom understand local words in different contexts. While Llama-3 exhibits larger bias, it responds better to different cultural contexts. The code and dataset are available at: this https URL. 

**Abstract (ZH)**: 大语言模型（LLMs）在自然语言处理应用方面取得了显著进展，但它们的广泛使用引发了关于内在偏差可能减少特定社会群体的效用或造成伤害的担忧。尽管在解决LLM偏差方面取得了进展，但现有研究存在两大局限性。首先，现有LLM偏差评估主要集中在美文化背景下，难以揭示LLM对其他文化的刻板偏见，导致LLM的发展和使用不公。其次，当前的偏差评估通常假设模型对目标社会群体较为熟悉。当LLM遇到超出其知识边界且在训练数据中未出现的词汇时，它们会在当地情境中生成不相关的结果，这主要是由于想象和过分自信，而这些结果不一定能够反映内在偏见。本研究通过提出一种基于本地语料库数据集的本地集成偏差识别与评估框架（LIBRA）来解决这些局限性，该框架无需借助众包方法来测量偏差。在实施这一框架时，我们开发了一个包含超过360,000个测试案例的数据集，其背景设在新西兰。此外，我们提出了增强理想CAT评分（EiCAT），该评分结合了理想CAT评分（iCAT）、知识边界外评分（bbs）以及基于分布差异的偏差测量方法，以应对LLM在知识边界以外遇到词汇的挑战。研究结果表明，BERT家族模型、GPT-2和Llama-3模型在不同情境下对本地词汇的理解能力有限。尽管Llama-3表现出更大的偏见，但它对不同文化背景的反应则更好。相关代码和数据集可在以下链接获取：this https URL。 

---
# Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective 

**Title (ZH)**: 从训练范式视角高效整合大规模语言模型与视觉感知：一项综述 

**Authors**: Xiaorui Ma, Haoran Xie, S. Joe Qin  

**Link**: [PDF](https://arxiv.org/pdf/2502.01524)  

**Abstract**: The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs. 

**Abstract (ZH)**: 视觉语言模态的融合一直是多模态学习中的重要研究方向，传统上依赖于视觉语言预训练模型。然而，随着大型语言模型（LLMs）的崛起，越来越多的研究开始融合LLMs与视觉模态。随之而来的是，将视觉模态集成到LLMs中的训练范式也发生了变化。最初，这种方法是通过预训练模态整合器，称为单阶段调优来实现。后来，这种方法逐渐分化为两类：一类是专注于性能提升的双阶段调优方法，另一类是侧重参数效率的直接适配方法。现有的综述主要集中在运用双阶段调优方法的视觉大型语言模型（VLLMs）上，导致了对训练范式及其独特的参数效率考量缺乏深刻的理解。本文将34个来自顶级会议、期刊和高引Arxiv论文的VLLMs进行分类和综述，重点关注从训练范式角度进行的参数效率。首先介绍了大型语言模型的结构与参数高效学习方法，随后讨论了视觉编码器，并给出了视觉模态整合器的全面分类。接着，本文回顾了三大训练范式及其效率考量，并总结了VLLM领域的基准测试结果。为了更深入地了解其在参数效率方面的有效性，我们比较和讨论了代表性模型的实验结果，其中直接适配范式的实验结果被重复验证。借此，本文旨在为研究人员和实践者提供有关视觉模态高效集成到LLMs中的最新进展和实用指南。 

---
