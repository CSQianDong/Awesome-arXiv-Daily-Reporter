{'arxiv_id': 'arXiv:2502.02145', 'title': 'Risk-Aware Driving Scenario Analysis with Large Language Models', 'authors': 'Yuan Gao, Mattia Piccinini, Johannes Betz', 'link': 'https://arxiv.org/abs/2502.02145', 'abstract': 'Large Language Models (LLMs) can capture nuanced contextual relationships, reasoning, and complex problem-solving. By leveraging their ability to process and interpret large-scale information, LLMs have shown potential to address domain-specific challenges, including those in autonomous driving systems. This paper proposes a novel framework that leverages LLMs for risk-aware analysis of generated driving scenarios. We hypothesize that LLMs can effectively evaluate whether driving scenarios generated by autonomous driving testing simulators are safety-critical. To validate this hypothesis, we conducted an empirical evaluation to assess the effectiveness of LLMs in performing this task. This framework will also provide feedback to generate the new safety-critical scenario by using adversarial method to modify existing non-critical scenarios and test their effectiveness in validating motion planning algorithms. Code and scenarios are available at: this https URL', 'abstract_zh': '大型语言模型（LLMs）能够捕捉到细腻的上下文关系、推理以及复杂的解决问题的能力。通过利用它们处理和解释大规模信息的能力，LLMs展示了应对特定领域挑战的潜力，包括自主驾驶系统中的问题。本文提出了一种新颖的框架，利用LLMs进行生成驾驶场景的风险意识分析。我们假设LLMs能够有效评估由自主驾驶测试模拟器生成的驾驶场景是否具备安全性关键性。为了验证这一假设，我们进行了实证研究，评估LLMs在此任务上的有效性。该框架还将提供反馈，通过使用对抗性方法修改现有的非关键性场景以生成新的安全性关键性场景，并测试其在验证运动规划算法方面的有效性。相关代码和场景可访问以下链接：[这里提供具体的URL链接]', 'title_zh': '使用大型语言模型进行风险意识驾驶场景分析'}
{'arxiv_id': 'arXiv:2502.01694', 'title': 'Metastable Dynamics of Chain-of-Thought Reasoning: Provable Benefits of Search, RL and Distillation', 'authors': 'Juno Kim, Denny Wu, Jason Lee, Taiji Suzuki', 'link': 'https://arxiv.org/abs/2502.01694', 'abstract': 'A key paradigm to improve the reasoning capabilities of large language models (LLMs) is to allocate more inference-time compute to search against a verifier or reward model. This process can then be utilized to refine the pretrained model or distill its reasoning patterns into more efficient models. In this paper, we study inference-time compute by viewing chain-of-thought (CoT) generation as a metastable Markov process: easy reasoning steps (e.g., algebraic manipulations) form densely connected clusters, while hard reasoning steps (e.g., applying a relevant theorem) create sparse, low-probability edges between clusters, leading to phase transitions at longer timescales. Under this framework, we prove that implementing a search protocol that rewards sparse edges improves CoT by decreasing the expected number of steps to reach different clusters. In contrast, we establish a limit on reasoning capability when the model is restricted to local information of the pretrained graph. We also show that the information gained by search can be utilized to obtain a better reasoning model: (1) the pretrained model can be directly finetuned to favor sparse edges via policy gradient methods, and moreover (2) a compressed metastable representation of the reasoning dynamics can be distilled into a smaller, more efficient model.', 'abstract_zh': '提高大型语言模型（LLM）推理能力的一个关键范式是，在搜索验证器或奖励模型时分配更多的推理时计算资源。这一过程可以用于细化预训练模型或将其推理模式提炼成更高效的模型。在本文中，我们通过将链式思维（CoT）生成视为一个亚稳态马尔可夫过程来研究推理时计算资源：容易的推理步骤（如代数操作）形成紧密连接的簇，而困难的推理步骤（如应用相关定理）则在这些簇之间形成稀疏的、低概率的边缘，导致在更长的时间尺度上产生相变。在这一框架下，我们证明了实施一个奖励稀疏边缘的搜索协议可以减少到达不同簇的期望步骤数量，从而提高CoT。相反，当模型仅受限于预训练图的局部信息时，我们建立了推理能力的上限。我们还展示了通过搜索获得的信息可以用于获得更好的推理模型：（1）预训练模型可以通过策略梯度方法直接微调，以偏好稀疏边缘；此外（2）可以通过压缩的亚稳态表示方式提炼推理动力学，将其提炼到一个更小、更高效的模型中。', 'title_zh': '链式推理的 metastable 动态：可证明的搜索、强化学习和蒸馏优势'}
{'arxiv_id': 'arXiv:2502.02573', 'title': 'Are Language Models Up to Sequential Optimization Problems? From Evaluation to a Hegelian-Inspired Enhancement', 'authors': 'Soheil Abbasloo', 'link': 'https://arxiv.org/abs/2502.02573', 'abstract': 'Large Language Models (LLMs) have demonstrated impressive capabilities across numerous fields, presenting an opportunity to revolutionize optimization problem-solving, a crucial, ubiquitous, and complex domain. This paper explores the proficiency of LLMs in handling Sequential Optimization Problems (SOPs). We introduce WorldGen, a dynamic framework for generating unseen SOPs with controllable complexities, to evaluate LLM performance. Our initial observations reveal that while LLMs perform well on simple SOPs, their performance significantly degrades with increased complexity. Motivated by this, we revisit philosophical hypotheses on reasoning to enhance LLM performance. Inspired by the influential framework of Hegelian Dialectics, we propose ACE, demonstrating how the performance of LLMs in SOP contexts can be significantly improved without any retraining or further fine-tuning.', 'abstract_zh': '大型语言模型（LLMs）已经在众多领域展现了令人印象深刻的性能，为优化问题求解这一关键且普遍复杂的领域带来了革命的机会。本文探讨了LLMs在处理序列优化问题（SOPs）方面的能力。我们引入了WorldGen框架，这是一种生成不可见的SOPs且具有可控复杂性的动态框架，用以评估LLMs的性能。初始观察结果显示，尽管LLMs在简单SOPs上表现良好，但随着复杂性的增加，其性能显著下降。受此启发，我们重新审视了推理方面的哲学假设，以提高LLMs的性能。受黑格尔辩证法框架的启发，我们提出了ACE方法，证明了在SOP上下文中，可以显著提高LLMs的性能，而无需重新训练或进一步微调。', 'title_zh': '语言模型能否胜任顺序优化问题？从评估到黑格尔启发式的增强'}
{'arxiv_id': 'arXiv:2502.02508', 'title': 'Satori: Reinforcement Learning with Chain-of-Action-Thought Enhances LLM Reasoning via Autoregressive Search', 'authors': 'Maohao Shen, Guangtao Zeng, Zhenting Qi, Zhang-Wei Hong, Zhenfang Chen, Wei Lu, Gregory Wornell, Subhro Das, David Cox, Chuang Gan', 'link': 'https://arxiv.org/abs/2502.02508', 'abstract': "Large language models (LLMs) have demonstrated remarkable reasoning capabilities across diverse domains. Recent studies have shown that increasing test-time computation enhances LLMs' reasoning capabilities. This typically involves extensive sampling at inference time guided by an external LLM verifier, resulting in a two-player system. Despite external guidance, the effectiveness of this system demonstrates the potential of a single LLM to tackle complex tasks. Thus, we pose a new research problem: Can we internalize the searching capabilities to fundamentally enhance the reasoning abilities of a single LLM? This work explores an orthogonal direction focusing on post-training LLMs for autoregressive searching (i.e., an extended reasoning process with self-reflection and self-exploration of new strategies). To achieve this, we propose the Chain-of-Action-Thought (COAT) reasoning and a two-stage training paradigm: 1) a small-scale format tuning stage to internalize the COAT reasoning format and 2) a large-scale self-improvement stage leveraging reinforcement learning. Our approach results in Satori, a 7B LLM trained on open-source models and data. Extensive empirical evaluations demonstrate that Satori achieves state-of-the-art performance on mathematical reasoning benchmarks while exhibits strong generalization to out-of-domain tasks. Code, data, and models will be fully open-sourced.", 'abstract_zh': '大型语言模型（LLMs）已经在多种领域中展现了卓越的推理能力。近期的研究表明，增加推理时的计算量可以进一步提升LLMs的推理能力。这通常涉及到在推理阶段由外部LLM验证器引导的大量采样，形成一个两者的系统。尽管有外部指导，该系统的有效性证明了一个单一LLM有能力处理复杂任务。因此，我们提出一个新的研究问题：我们是否可以将搜索能力内嵌到单一LLM中，从根本上提升其推理能力？本文探讨了一个与传统方法不同的方向，针对自回归搜索后训练LLMs（即包含自我反思和探索新策略的扩展推理过程）。为实现这一目标，我们提出了Action-Thought Chain (COAT) 推理，并采用两阶段训练方法：1）小型格式调优阶段，以内嵌COAT推理格式；2）大规模自我改进阶段，利用强化学习机制。我们的方法训练出了Satori，这是一个基于开源模型和数据训练的拥有7B参数的LLMs。广泛的经验评估表明，Satori在数学推理基准测试中取得了最先进的性能，并且在域外任务上表现出强大的泛化能力。我们将提供完整的代码、数据和模型。', 'title_zh': '悟性：基于链式行动思考的增强学习增强大语言模型推理能力的研究'}
{'arxiv_id': 'arXiv:2502.02444', 'title': 'Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models', 'authors': 'Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song', 'link': 'https://arxiv.org/abs/2502.02444', 'abstract': "Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.", 'abstract_zh': '价值观是影响个体和集体感知、认知和行为的核心驱动力。价值体系，如施瓦茨的基本人类价值观理论，界定了这些价值观的层次结构及相互作用，促进了跨学科对决策和社会动态的研究。近期，大型语言模型（LLMs）的发展引发了对其内在价值观难以捉摸的担忧。尽管在评价、理解和对齐LLM价值观方面做出了不断的努力，但基于心理理论的LLM价值体系仍处于探索阶段。本研究通过引入生成心理词汇方法（GPLA，Generative Psycho-Lexical Approach），填补了这一空白，GPLA是一种可扩展、适应性强且基于理论的方法，用于构建价值体系。利用GPLA，我们提出了一个基于心理理论的五因素价值体系，专门针对LLMs。为了系统验证，我们提出了三项基准任务，将心理原则与前沿的人工智能优先事项相结合。研究结果表明，所提出的值体系符合标准的心理学标准，更准确地捕捉了LLM的价值观，提高了LLM的安全性预测，并增强了LLM的对齐效果，相比之下，优于施瓦茨的标准价值观体系。', 'title_zh': '面向生成的心理语义学方法在大型语言模型中构建价值系统'}
{'arxiv_id': 'arXiv:2502.02441', 'title': 'LLMER: Crafting Interactive Extended Reality Worlds with JSON Data Generated by Large Language Models', 'authors': 'Jiangong Chen, Xiaoyi Wu, Tian Lan, Bin Li', 'link': 'https://arxiv.org/abs/2502.02441', 'abstract': "The integration of Large Language Models (LLMs) like GPT-4 with Extended Reality (XR) technologies offers the potential to build truly immersive XR environments that interact with human users through natural language, e.g., generating and animating 3D scenes from audio inputs. However, the complexity of XR environments makes it difficult to accurately extract relevant contextual data and scene/object parameters from an overwhelming volume of XR artifacts. It leads to not only increased costs with pay-per-use models, but also elevated levels of generation errors. Moreover, existing approaches focusing on coding script generation are often prone to generation errors, resulting in flawed or invalid scripts, application crashes, and ultimately a degraded user experience. To overcome these challenges, we introduce LLMER, a novel framework that creates interactive XR worlds using JSON data generated by LLMs. Unlike prior approaches focusing on coding script generation, LLMER translates natural language inputs into JSON data, significantly reducing the likelihood of application crashes and processing latency. It employs a multi-stage strategy to supply only the essential contextual information adapted to the user's request and features multiple modules designed for various XR tasks. Our preliminary user study reveals the effectiveness of the proposed system, with over 80% reduction in consumed tokens and around 60% reduction in task completion time compared to state-of-the-art approaches. The analysis of users' feedback also illuminates a series of directions for further optimization.", 'abstract_zh': '将大型语言模型（LLMs）如GPT-4与扩展现实（XR）技术的整合，为构建真正沉浸式的XR环境提供了可能性，这些环境可以通过自然语言与人类用户互动，例如，从音频输入生成和动画化3D场景。然而，XR环境的复杂性使得从大量XR数据中准确提取相关背景信息和场景/对象参数变得困难重重。这不仅会导致按使用付费模式下的成本增加，还会增加生成错误的概率。此外，现有的侧重于编码脚本生成的方法往往容易出现生成错误，导致脚本缺陷或无效、应用程序崩溃，并最终降低用户体验。为了克服这些挑战，我们提出了LLMER框架，该框架利用LLMs生成的JSON数据创建交互式的XR世界。与先前侧重于编码脚本生成的方法不同，LLMER将自然语言输入转化为JSON数据，显著降低了应用程序崩溃和处理延迟的可能性。该框架采用多阶段策略，仅提供与用户请求相适应的必要的背景信息，并设计了适用于各种XR任务的多个模块。初步用户研究显示了所提系统的有效性，与最先进的方法相比，JSON数据的使用量减少了80%以上，任务完成时间减少了约60%。用户反馈的分析还揭示了一系列进一步优化的方向。', 'title_zh': 'LLMER：使用大型语言模型生成JSON数据构建交互式扩展现实世界\n\n在这个翻译中，"LLMER" 被保留为缩写，因为它是论文的特定命名。"JSON数据" 是 "JSON data" 的准确翻译，而 "交互式扩展现实世界" 是 "interactive extended reality worlds" 的学术翻译。希望这符合你的需求。如果你有更具体的要求或需要进一步的修改，请告诉我。'}
{'arxiv_id': 'arXiv:2502.02421', 'title': 'Activation-Informed Merging of Large Language Models', 'authors': 'Amin Heyrani Nobari, Kaveh Alimohammadi, Ali ArjomandBigdeli, Akash Srivastava, Faez Ahmed, Navid Azizan', 'link': 'https://arxiv.org/abs/2502.02421', 'abstract': 'Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning~(CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40\\% increase in benchmark performance.', 'abstract_zh': '模型合并是一种将多个微调大型语言模型（LLM）的参数和嵌入相结合的方法，它提供了一种在各种任务中增强模型性能的同时保持计算效率的有前途的方法。本文介绍了激活信息引导合并（AIM），这是一种将大型语言模型激活空间中的信息集成到合并过程中的技术，以提高性能和鲁棒性。AIM 是一个灵活且互补的解决方案，适用于任何现有的合并方法。它旨在保留基础模型中的关键权重，借鉴了持续学习（CL）和模型压缩的原则。通过使用一个任务无关的校准集，AIM 有选择地在合并过程中优先考虑关键权重。我们通过实验证明，AIM 显著提高了多个基准测试中合并模型的性能。我们的研究结果表明，考虑激活空间信息可以在 LLM 的模型合并策略中提供重大改进，基准测试性能最多可提高40%。', 'title_zh': '激活导向的大语言模型融合方法'}
{'arxiv_id': 'arXiv:2502.02390', 'title': 'CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning', 'authors': 'Jianfeng Pan, Senyou Deng, Shaomang Huang', 'link': 'https://arxiv.org/abs/2502.02390', 'abstract': "Research on LLM technologies is rapidly emerging, with most of them employing a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. To validate the effectiveness of our framework, we conducted extensive experiments across a range of generative and reasoning tasks. These experiments demonstrated that our framework outperforms conventional inference processes on accuracy, coherence, and diversity. The framework's ability to iteratively expand its search space while retaining contextually relevant information results.", 'abstract_zh': '大规模语言模型（LLM）技术研究正迅速兴起，大多数研究采用“快速思考”方法进行推理。大多数LLM仅基于单个查询和模型的推理能力生成最终结果。然而，随着OpenAI-o1的出现，“慢思考”技术逐渐受到关注，因为其过程更接近人类的思维过程。受到人类不断关联和补充知识的能力启发，我们开发了新颖的连续关联思维（CoAT）框架，该框架引入了蒙特卡洛树搜索（MCTS）算法与一种动态的新关键信息整合机制“关联记忆”的创新结合。通过结合MCTS的结构化探索能力和关联记忆的自适应学习能力，CoAT显著扩大了LLM的搜索空间，使我们的框架能够探索多种推理路径，并实时动态更新其知识库。这不仅允许框架重新审视和改进早期推断，还能适应性地纳入不断变化的信息，确保最终输出既准确又全面。为了验证我们框架的有效性，我们在多种生成性和推理任务上进行了广泛的实验。实验结果表明，与传统的推理过程相比，我们的框架在准确性、连贯性和多样性方面表现更优。框架能够在不断提升其搜索空间的同时保留相关背景信息。', 'title_zh': 'CoAT：增强大规模语言模型推理能力的关联思维链框架'}
{'arxiv_id': 'arXiv:2502.02067', 'title': 'AdaptBot: Combining LLM with Knowledge Graphs and Human Input for Generic-to-Specific Task Decomposition and Knowledge Refinement', 'authors': 'Shivam Singh, Karthik Swaminathan, Nabanita Dash, Ramandeep Singh, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna', 'link': 'https://arxiv.org/abs/2502.02067', 'abstract': 'Embodied agents assisting humans are often asked to complete a new task in a new scenario. An agent preparing a particular dish in the kitchen based on a known recipe may be asked to prepare a new dish or to perform cleaning tasks in the storeroom. There may not be sufficient resources, e.g., time or labeled examples, to train the agent for these new situations. Large Language Models (LLMs) trained on considerable knowledge across many domains are able to predict a sequence of abstract actions for such new tasks and scenarios, although it may not be possible for the agent to execute this action sequence due to task-, agent-, or domain-specific constraints. Our framework addresses these challenges by leveraging the generic predictions provided by LLM and the prior domain-specific knowledge encoded in a Knowledge Graph (KG), enabling an agent to quickly adapt to new tasks and scenarios. The robot also solicits and uses human input as needed to refine its existing knowledge. Based on experimental evaluation over cooking and cleaning tasks in simulation domains, we demonstrate that the interplay between LLM, KG, and human input leads to substantial performance gains compared with just using the LLM output.', 'abstract_zh': '在新的场景中执行新任务的实体代理经常被要求完成未接触过的任务。例如，基于已知食谱在厨房里准备某种菜肴的代理可能被要求准备新菜肴或对储藏室进行清洁。由于缺乏资源，如时间或标注数据，可能不足以对这些新情况进行训练。大规模语言模型（LLMs）通过对多个领域大量知识的训练，能够预测此类新任务和场景的抽象动作序列，但由于任务、代理或领域特定的约束，代理可能无法执行这些动作序列。我们的框架通过利用LLM提供的通用预测和知识图谱（KG）中编码的先验领域特定知识，使代理能够快速适应新任务和场景。同时，机器也会根据需要请求并利用人类输入来完善其现有知识。通过对烹饪和清洁任务在模拟领域进行实验评估，结果显示，LLM、KG和人类输入之间的互动能够带来显著的性能提升，相比仅使用LLM输出效果更佳。', 'title_zh': 'AdaptBot：将大规模语言模型与知识图谱及人性输入相结合，实现从通用到具体任务分解与知识精炼'}
{'arxiv_id': 'arXiv:2502.02013', 'title': 'Layer by Layer: Uncovering Hidden Representations in Language Models', 'authors': 'Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, Ravid Shwartz-Ziv', 'link': 'https://arxiv.org/abs/2502.02013', 'abstract': "From extracting features to generating text, the outputs of large language models (LLMs) typically rely on their final layers, following the conventional wisdom that earlier layers capture only low-level cues. However, our analysis shows that intermediate layers can encode even richer representations, often improving performance on a wide range of downstream tasks. To explain and quantify these hidden-layer properties, we propose a unified framework of representation quality metrics based on information theory, geometry, and invariance to input perturbations. Our framework highlights how each model layer balances information compression and signal preservation, revealing why mid-depth embeddings can exceed the last layer's performance. Through extensive experiments on 32 text-embedding tasks and comparisons across model architectures (transformers, state-space models) and domains (language, vision), we demonstrate that intermediate layers consistently provide stronger features. These findings challenge the standard focus on final-layer embeddings and open new directions for model analysis and optimization, including strategic use of mid-layer representations for more robust and accurate AI systems.", 'abstract_zh': '从提取特征到生成文本，大型语言模型（LLMs）的输出通常依赖于其最后一层，沿袭了早期层仅捕获低级线索的常规智慧。然而，我们的分析表明，中间层可以编码更丰富的内容表示，往往在多种下游任务上提高性能。为了解释和量化这些隐藏层的属性，我们提出了一种基于信息论、几何学和输入扰动不变性的统一表示质量度量框架。该框架揭示了每层模型在信息压缩和信号保留之间的平衡，解释了为什么中间深度的嵌入可以超过最后一层的性能。通过在32项文本嵌入任务上的广泛实验，并且在模型结构（变换器、状态空间模型）和领域（语言、视觉）之间进行比较，我们证明中间层始终提供更强的特征表示。这些发现挑战了对最终层嵌入的常规关注，并为模型分析和优化开辟了新的方向，包括战略性地利用中间层表示以构建更稳健和准确的AI系统。', 'title_zh': '逐层揭示：揭开语言模型中隐藏表示的秘密'}
{'arxiv_id': 'arXiv:2502.02009', 'title': 'LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations', 'authors': 'Ziyang Ye, Triet Huynh Minh Le, M. Ali Babar', 'link': 'https://arxiv.org/abs/2502.02009', 'abstract': 'Security misconfigurations in Container Orchestrators (COs) can pose serious threats to software systems. While Static Analysis Tools (SATs) can effectively detect these security vulnerabilities, the industry currently lacks automated solutions capable of fixing these misconfigurations. The emergence of Large Language Models (LLMs), with their proven capabilities in code understanding and generation, presents an opportunity to address this limitation. This study introduces LLMSecConfig, an innovative framework that bridges this gap by combining SATs with LLMs. Our approach leverages advanced prompting techniques and Retrieval-Augmented Generation (RAG) to automatically repair security misconfigurations while preserving operational functionality. Evaluation of 1,000 real-world Kubernetes configurations achieved a 94\\% success rate while maintaining a low rate of introducing new misconfigurations.\nOur work makes a promising step towards automated container security management, reducing the manual effort required for configuration maintenance.', 'abstract_zh': '容器编排器（COs）的安全配置错误可能会对软件系统构成严重的威胁。虽然静态分析工具（SATs）能够有效检测这些安全漏洞，但目前行业缺乏能够自动修复这些配置错误的解决方案。大型语言模型（LLMs）凭借其在代码理解和生成方面的卓越能力，为解决这一局限提供了机会。本研究引入了LLMSecConfig这一创新框架，该框架通过结合SATs和LLMs来弥补这一空白。我们的方法利用先进的提示技术和检索增强生成（RAG）技术，自动修复安全配置错误，同时保持操作功能的完整性。对1,000个实际Kubernetes配置的评估结果显示，成功率达到94%，同时引入新的配置错误的比例较低。\n\n我们的工作朝着自动化的容器安全管理迈出了具有前景的一步，减少了配置维护所需的手动劳动。', 'title_zh': 'LLMSecConfig：一种基于LLM的方法，用于修复软件容器配置错误'}
{'arxiv_id': 'arXiv:2502.01991', 'title': 'Can LLMs Assist Annotators in Identifying Morality Frames? -- Case Study on Vaccination Debate on Social Media', 'authors': 'Tunazzina Islam, Dan Goldwasser', 'link': 'https://arxiv.org/abs/2502.01991', 'abstract': 'Nowadays, social media is pivotal in shaping public discourse, especially on polarizing issues like vaccination, where diverse moral perspectives influence individual opinions. In NLP, data scarcity and complexity of psycholinguistic tasks such as identifying morality frames makes relying solely on human annotators costly, time-consuming, and prone to inconsistency due to cognitive load. To address these issues, we leverage large language models (LLMs), which are adept at adapting new tasks through few-shot learning, utilizing a handful of in-context examples coupled with explanations that connect examples to task principles. Our research explores LLMs\' potential to assist human annotators in identifying morality frames within vaccination debates on social media. We employ a two-step process: generating concepts and explanations with LLMs, followed by human evaluation using a "think-aloud" tool. Our study shows that integrating LLMs into the annotation process enhances accuracy, reduces task difficulty, lowers cognitive load, suggesting a promising avenue for human-AI collaboration in complex psycholinguistic tasks.', 'abstract_zh': '当今，社交媒体在塑造公众舆论方面发挥着关键作用，尤其是在涉及疫苗等 polarization问题上，不同的道德视角影响着个人的观点。在自然语言处理（NLP）中，数据稀缺性和心理语言学任务（如识别道德框架）的复杂性使得依赖人工标注员进行标注成本高、耗时长且容易由于认知负担而导致一致性差。为解决这些问题，我们运用了大型语言模型（LLMs），这些模型通过少样本学习擅长于适应新的任务，利用少量上下文示例及其与任务原则的联系进行解释。我们的研究探讨了LLMs在协助人工标注员识别社交媒体上关于疫苗讨论中的道德框架方面的潜力。我们采用两步过程：首先使用LLMs生成概念和解释，然后使用“思考 aloud”工具进行人工评估。研究结果显示，将LLMs集成到标注过程中能够提高准确性、降低任务难度、减轻认知负担，这表明在复杂心理语言学任务中人类-AI协作的一个有前景的方向。', 'title_zh': '大型语言模型能否辅助注释者识别道德框架——社交媒体疫苗辩论案例研究'}
{'arxiv_id': 'arXiv:2502.01976', 'title': 'CITER: Collaborative Inference for Efficient Large Language Model Decoding with Token-Level Routing', 'authors': 'Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, Huaxiu Yao', 'link': 'https://arxiv.org/abs/2502.01976', 'abstract': 'Large language models have achieved remarkable success in various tasks but suffer from high computational costs during inference, limiting their deployment in resource-constrained applications. To address this issue, we propose a novel CITER (\\textbf{C}ollaborative \\textbf{I}nference with \\textbf{T}oken-l\\textbf{E}vel \\textbf{R}outing) framework that enables efficient collaboration between small and large language models (SLMs & LLMs) through a token-level routing strategy. Specifically, CITER routes non-critical tokens to an SLM for efficiency and routes critical tokens to an LLM for generalization quality. We formulate router training as a policy optimization, where the router receives rewards based on both the quality of predictions and the inference costs of generation. This allows the router to learn to predict token-level routing scores and make routing decisions based on both the current token and the future impact of its decisions. To further accelerate the reward evaluation process, we introduce a shortcut which significantly reduces the costs of the reward estimation and improving the practicality of our approach. Extensive experiments on five benchmark datasets demonstrate that CITER reduces the inference costs while preserving high-quality generation, offering a promising solution for real-time and resource-constrained applications.', 'abstract_zh': '大规模语言模型在各种任务中取得了显著的成功，但在推断过程中面临着高昂的计算成本，限制了它们在资源受限应用中的部署。为解决这一问题，我们提出了一种名为CITER（Collaborative Inference with Token-Level Routing）的新框架，该框架通过令牌水平路由策略在小型和大型语言模型（小语言模型(SLM)和大语言模型(LLM)）之间实现了高效的协作。具体而言，CITER将非关键令牌路由到SLM以提高效率，将关键令牌路由到LLM以获得泛化质量。我们将路由器训练形式化为策略优化问题，其中路由器根据预测质量以及生成过程的推断成本获得奖励。这使得路由器能够学习预测令牌级路由得分，并根据当前令牌及其决策未来影响做出路由决策。为了进一步加速奖励评估过程，我们引入了一个捷径，显著降低了奖励估计成本，从而提高了我们方法的实用性。在五个基准数据集上的广泛实验表明，CITER能够在保持高质量生成的同时减少推断成本，为实时和资源受限应用提供了一种有前景的解决方案。', 'title_zh': 'CITER：基于token级路由的协作推理方法，以实现高效的大规模语言模型解码'}
{'arxiv_id': 'arXiv:2502.01968', 'title': 'Token Cleaning: Fine-Grained Data Selection for LLM Supervised Fine-Tuning', 'authors': 'Jinlong Pang, Na Di, Zhaowei Zhu, Jiaheng Wei, Hao Cheng, Chen Qian, Yang Liu', 'link': 'https://arxiv.org/abs/2502.01968', 'abstract': 'Recent studies show that in supervised fine-tuning (SFT) of large language models (LLMs), data quality matters more than quantity. While most data cleaning methods concentrate on filtering entire samples, the quality of individual tokens within a sample can vary significantly. After pre-training, even in high-quality samples, patterns or phrases that are not task-related can be redundant or uninformative. Continuing to fine-tune on these patterns may offer limited benefit and even degrade downstream task performance. In this paper, we investigate token quality from a noisy-label perspective and propose a generic token cleaning pipeline for SFT tasks. Our method filters out uninformative tokens while preserving those carrying key task-specific information. Specifically, we first evaluate token quality by examining the influence of model updates on each token, then apply a threshold-based separation. The token influence can be measured in a single pass with a fixed reference model or iteratively with self-evolving reference models. The benefits and limitations of both methods are analyzed theoretically by error upper bounds. Extensive experiments show that our framework consistently improves performance across multiple downstream tasks.', 'abstract_zh': '近期的研究表明，在大型语言模型（LLMs）的监督微调（SFT）中，数据质量比数据量更为重要。尽管大多数数据清洗方法侧重于过滤整个样本，但样本内单个词 token 的质量却可能差异显著。即使在预训练后，高质量样本中也可能存在与任务无关的冗余或不相关信息。继续在这类内容上微调可能提供有限的益处，甚至会损害下游任务的性能。在本文中，我们从噪声标签的角度研究 token 的质量，并提出了一种通用的 token 清洗管道，适用于 SFT 任务。我们的方法会过滤掉无关的 token，同时保留那些承载关键任务特定信息的 token。具体而言，我们首先通过检查模型更新对每个 token 的影响来评估 token 的质量，然后应用基于阈值的分离方法。token 的影响可以在固定参考模型的一次通过中或在自演化参考模型中迭代地进行衡量。通过错误上界理论分析了两种方法的优势和局限性。大量实验证明，我们的框架在多个下游任务上均能持续提升性能。', 'title_zh': 'token清洗：用于LLM监督微调的细粒度数据选择'}
{'arxiv_id': 'arXiv:2502.01941', 'title': 'Can LLMs Maintain Fundamental Abilities under KV Cache Compression?', 'authors': 'Xiang Liu, Zhenheng Tang, Hong Chen, Peijie Dong, Zeyu Li, Xiuze Zhou, Bo Li, Xuming Hu, Xiaowen Chu', 'link': 'https://arxiv.org/abs/2502.01941', 'abstract': "This paper investigates an under-explored challenge in large language models (LLMs): the impact of KV cache compression methods on LLMs' fundamental capabilities. While existing methods achieve impressive compression ratios on long-context benchmarks, their effects on core model capabilities remain understudied. We present a comprehensive empirical study evaluating prominent KV cache compression methods across diverse tasks, spanning world knowledge, commonsense reasoning, arithmetic reasoning, code generation, safety, and long-context understanding and this http URL analysis reveals that KV cache compression methods exhibit task-specific performance degradation. Arithmetic reasoning tasks prove particularly sensitive to aggressive compression, with different methods showing performance drops of $17.4\\%$-$43.3\\%$. Notably, the DeepSeek R1 Distill model exhibits more robust compression tolerance compared to instruction-tuned models, showing only $9.67\\%$-$25.53\\%$ performance degradation. Based on our analysis of attention patterns and cross-task compression performance, we propose ShotKV, a novel compression approach that distinctly handles prefill and decoding phases while maintaining shot-level semantic coherence. Empirical results show that ShotKV achieves $9\\%$-$18\\%$ performance improvements on long-context generation tasks under aggressive compression ratios.", 'abstract_zh': '本文探讨了一个在大型语言模型（LLMs）中未被充分研究的挑战：KV缓存压缩方法对LLMs基本能力的影响。尽管现有方法在长上下文基准测试中实现了令人印象深刻的压缩比率，但它们对核心模型能力的影响仍鲜有研究。我们进行了一项全面的经验研究，评估了多种突出的KV缓存压缩方法在不同任务中的表现，涵盖了世界知识、常识推理、算术推理、代码生成、安全性和长上下文理解等多个领域。分析结果显示，KV缓存压缩方法表现出了任务特异性性能下降。算术推理任务对激进压缩特别敏感，不同方法的性能下降幅度在17.4%到43.3%之间。值得注意的是，DeepSeek R1 Distill模型在面对激进压缩时表现出更稳健的压缩容忍度，其性能下降幅度仅为9.67%到25.53%。基于我们对注意力模式和跨任务压缩性能的分析，我们提出了ShotKV，一种新颖的压缩方法，该方法在处理预填充和解码阶段时能保留摄影水平的语义一致性。实验结果表明，在激进的压缩比率下，ShotKV在长上下文生成任务中实现了9%到18%的性能提升。', 'title_zh': 'LLM在KV缓存压缩情况下能否维持基本能力？'}
{'arxiv_id': 'arXiv:2502.01803', 'title': 'Discovering Chunks in Neural Embeddings for Interpretability', 'authors': 'Shuchen Wu, Stephan Alaniz, Eric Schulz, Zeynep Akata', 'link': 'https://arxiv.org/abs/2502.01803', 'abstract': 'Understanding neural networks is challenging due to their high-dimensional, interacting components. Inspired by human cognition, which processes complex sensory data by chunking it into recurring entities, we propose leveraging this principle to interpret artificial neural population activities. Biological and artificial intelligence share the challenge of learning from structured, naturalistic data, and we hypothesize that the cognitive mechanism of chunking can provide insights into artificial systems. We first demonstrate this concept in recurrent neural networks (RNNs) trained on artificial sequences with imposed regularities, observing that their hidden states reflect these patterns, which can be extracted as a dictionary of chunks that influence network responses. Extending this to large language models (LLMs) like LLaMA, we identify similar recurring embedding states corresponding to concepts in the input, with perturbations to these states activating or inhibiting the associated concepts. By exploring methods to extract dictionaries of identifiable chunks across neural embeddings of varying complexity, our findings introduce a new framework for interpreting neural networks, framing their population activity as structured reflections of the data they process.', 'abstract_zh': '理解和解析神经网络具有挑战性，因为它们包含大量相互作用的高维组件。受到人类认知的启发，人类能够通过将复杂的感觉数据划分为重复出现的实体来进行处理，我们提出利用这一原理来解释人工神经群体的活动。生物学和人工智能在从结构化自然数据中学习方面面临着相同的挑战，并推测认知机制的划分为我们提供了解人工系统的新见解。我们首先在受控结构的人工序列上训练的循环神经网络（RNNs）上展示了这一概念，发现它们的隐藏状态反映了这些模式，并可以提取出影响网络响应的块字典。扩展这一方法到大型语言模型（LLMs）如LLaMA上，我们识别出了与输入概念相对应的相似重复嵌入状态，并且这些状态的扰动能激活或抑制相应的概念。通过探索从不同复杂度的神经嵌入中提取可识别块字典的方法，我们的发现为解释神经网络提供了一个新的框架，将它们的群体活动视为数据处理的结构化反映。', 'title_zh': '发现神经嵌入中的片段以实现可解释性'}
{'arxiv_id': 'arXiv:2502.01754', 'title': 'Evaluation of Large Language Models via Coupled Token Generation', 'authors': 'Nina Corvelo Benz, Stratis Tsirtsis, Eleni Straitouri, Ivi Chatzi, Ander Artola Velasco, Suhas Thejaswi, Manuel Gomez-Rodriguez', 'link': 'https://arxiv.org/abs/2502.01754', 'abstract': 'State of the art large language models rely on randomization to respond to a prompt. As an immediate consequence, a model may respond differently to the same prompt if asked multiple times. In this work, we argue that the evaluation and ranking of large language models should control for the randomization underpinning their functioning. Our starting point is the development of a causal model for coupled autoregressive generation, which allows different large language models to sample responses with the same source of randomness. Building upon our causal model, we first show that, on evaluations based on benchmark datasets, coupled autoregressive generation leads to the same conclusions as vanilla autoregressive generation but using provably fewer samples. However, we further show that, on evaluations based on (human) pairwise comparisons, coupled and vanilla autoregressive generation can surprisingly lead to different rankings when comparing more than two models, even with an infinite amount of samples. This suggests that the apparent advantage of a model over others in existing evaluation protocols may not be genuine but rather confounded by the randomness inherent to the generation process. To illustrate and complement our theoretical results, we conduct experiments with several large language models from the Llama family. We find that, across multiple knowledge areas from the popular MMLU benchmark dataset, coupled autoregressive generation requires up to 40% fewer samples to reach the same conclusions as vanilla autoregressive generation. Further, using data from the LMSYS Chatbot Arena platform, we find that the win-rates derived from pairwise comparisons by a strong large language model to prompts differ under coupled and vanilla autoregressive generation.', 'abstract_zh': '最先进的大语言模型依赖随机性来回应提示。作为直接结果，同一个提示若被模型多次回应，可能会得到不同的结果。本研究中，我们主张在评估和排名大语言模型时，应该控制其运行背后所依赖的随机性。我们的出发点是开发一种因果模型，用于耦合自回归生成，使得不同大语言模型能够使用相同的随机源采样响应。基于我们的因果模型，我们首先表明，在基于基准数据集的评估中，耦合自回归生成与传统的自回归生成得出相同的结论，但使用了可以证明更少的样本。然而，我们进一步表明，在基于（人类的）成对比较的评估中，即使使用无限数量的样本，耦合和传统的自回归生成法也可能导致对超过两个模型的排名不同。这表明，在现有评估协议中，模型相对于其他模型的表象优势可能并非真实存在，而是由生成过程内在的随机性造成的混淆。为了展示并补充我们的理论结果，我们使用了来自Llama家族的几种大语言模型进行了实验。我们发现，在流行的MMLU基准数据集的多个知识领域中，耦合自回归生成需要少至40%的样本，就能达到与传统的自回归生成相同的结果。此外，通过LMSYS Chatbot竞技场平台的数据，我们发现一个强大的大语言模型对提示的胜率在耦合和传统的自回归生成下有所不同。', 'title_zh': '通过耦合令牌生成评估大型语言模型'}
{'arxiv_id': 'arXiv:2502.01691', 'title': 'Agent-Based Uncertainty Awareness Improves Automated Radiology Report Labeling with an Open-Source Large Language Model', 'authors': 'Hadas Ben-Atya, Naama Gavrielov, Zvi Badash, Gili Focht, Ruth Cytter-Kuint, Talar Hagopian, Dan Turner, Moti Freiman', 'link': 'https://arxiv.org/abs/2502.01691', 'abstract': "Reliable extraction of structured data from radiology reports using Large Language Models (LLMs) remains challenging, especially for complex, non-English texts like Hebrew. This study introduces an agent-based uncertainty-aware approach to improve the trustworthiness of LLM predictions in medical applications. We analyzed 9,683 Hebrew radiology reports from Crohn's disease patients (from 2010 to 2023) across three medical centers. A subset of 512 reports was manually annotated for six gastrointestinal organs and 15 pathological findings, while the remaining reports were automatically annotated using HSMP-BERT. Structured data extraction was performed using Llama 3.1 (Llama 3-8b-instruct) with Bayesian Prompt Ensembles (BayesPE), which employed six semantically equivalent prompts to estimate uncertainty. An Agent-Based Decision Model integrated multiple prompt outputs into five confidence levels for calibrated uncertainty and was compared against three entropy-based models. Performance was evaluated using accuracy, F1 score, precision, recall, and Cohen's Kappa before and after filtering high-uncertainty cases. The agent-based model outperformed the baseline across all metrics, achieving an F1 score of 0.3967, recall of 0.6437, and Cohen's Kappa of 0.3006. After filtering high-uncertainty cases (greater than or equal to 0.5), the F1 score improved to 0.4787, and Kappa increased to 0.4258. Uncertainty histograms demonstrated clear separation between correct and incorrect predictions, with the agent-based model providing the most well-calibrated uncertainty estimates. By incorporating uncertainty-aware prompt ensembles and an agent-based decision model, this approach enhances the performance and reliability of LLMs in structured data extraction from radiology reports, offering a more interpretable and trustworthy solution for high-stakes medical applications.", 'abstract_zh': "使用大型语言模型（LLMs）从放射学报告中可靠地提取结构化数据仍然具有挑战性，尤其是在处理复杂且非英语文本（如希伯来文）时。本研究引入了一种基于代理的不确定性感知方法，以提高医疗应用中LLM预测的可信度。我们分析了来自三个医疗机构的9,683份克罗恩病患者的希伯来文放射学报告（时间范围为2010年至2023年）。其中512份报告被手动标注了六个胃肠道器官和15项病理发现，其余报告则使用HSMP-BERT进行了自动标注。结构化数据提取使用了Llama 3.1（Llama 3-8b-instruct）并结合了贝叶斯提示集（BayesPE），该方法使用六个语义等效提示来估计不确定性。基于代理的决策模型将多种提示输出整合到五个信心水平中，以校准不确定性，并与三种基于熵的方法进行了比较。性能评估使用了准确率、F1分数、精确率、召回率和柯克帕特里克系数（Cohen's Kappa）在筛选出高不确定性病例之前和之后进行。基于代理的模型在所有指标上都优于 baseline，F1分数达到了0.3967，召回率为0.6437，柯克帕特里克系数为0.3006。在筛选出高不确定性病例（大于或等于0.5）之后，F1分数提高到0.4787，Kappa增加到0.4258。不确定性直方图显示了正确和错误预测之间的明确分离，基于代理的模型提供了最准确的不确定性估计。通过结合不确定性感知提示集和基于代理的决策模型，本方法增强了LLMs在放射学报告中结构化数据提取的性能和可靠性，为具有高风险的医疗应用提供了更可解释和可信的解决方案。", 'title_zh': '基于代理的不确定性意识到提高了使用开源大规模语言模型的自动化放射学报告标签化效能'}
{'arxiv_id': 'arXiv:2502.01683', 'title': 'LLM-Powered Benchmark Factory: Reliable, Generic, and Efficient', 'authors': 'Peiwen Yuan, Shaoxiong Feng, Yiwei Li, Xinglin Wang, Yueqi Zhang, Jiayi Shi, Chuyi Tan, Boyuan Pan, Yao Hu, Kan Li', 'link': 'https://arxiv.org/abs/2502.01683', 'abstract': 'The rapid advancement of large language models (LLMs) has led to a surge in both model supply and application demands. To facilitate effective matching between them, reliable, generic and efficient benchmark generators are widely needed. However, human annotators are constrained by inefficiency, and current LLM benchmark generators not only lack generalizability but also struggle with limited reliability, as they lack a comprehensive evaluation framework for validation and optimization. To fill this gap, we first propose an automated and unbiased evaluation framework, structured around four dimensions and ten criteria. Under this framework, we carefully analyze the advantages and weaknesses of directly prompting LLMs as generic benchmark generators. To enhance the reliability, we introduce a series of methods to address the identified weaknesses and integrate them as BenchMaker. Experiments across multiple LLMs and tasks confirm that BenchMaker achieves superior or comparable performance to human-annotated benchmarks on all metrics, highlighting its generalizability and reliability. More importantly, it delivers highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation against MMLU-Pro), while taking only $0.005 and 0.38 minutes per sample.', 'abstract_zh': '大型语言模型（LLMs）的迅速发展导致了模型供应和应用需求的急剧增加。为了有效匹配供需双方，可靠、通用且高效的基准生成器需求广泛。然而，人工注释员受制于效率问题，当前的 LLM 基准生成器不仅缺乏普适性，而且在验证和优化方面表现出有限的可靠性。为弥补这一缺口，我们首先提出了一种自动化且无偏见的评估框架，该框架围绕四个维度和十个标准构建。在这一框架下，我们仔细分析了直接使用 LLM 进行基准生成的优缺点。为了提高可靠性，我们引入了一系列方法来解决识别出的缺点，并将这些方法整合为 BenchMaker。跨多个 LLM 和任务的实验结果表明，BenchMaker 在所有指标上的表现优于或可与人工标注的基准相媲美，突显了其普适性和可靠性。更重要的是，BenchMaker 在 12 个 LLM 上提供了高度一致的评估结果（与 MMLU-Pro 的 Pearson 相关系数为 0.967），并且每个样本只需花费 0.005 美元和 0.38 分钟。', 'title_zh': 'LLM �Drivern基准工厂：可靠、通用且高效'}
{'arxiv_id': 'arXiv:2502.01662', 'title': 'Speculative Ensemble: Fast Large Language Model Ensemble via Speculation', 'authors': 'Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Xin Geng, Xu Yang', 'link': 'https://arxiv.org/abs/2502.01662', 'abstract': 'Ensemble methods enhance Large Language Models (LLMs) by combining multiple models but suffer from high computational costs. In this paper, we introduce Speculative Ensemble, a novel framework that accelerates LLM ensembles without sacrificing performance, inspired by Speculative Decoding-where a small proposal model generates tokens sequentially, and a larger target model verifies them in parallel. Our approach builds on two key insights: (1) the verification distribution can be the ensemble distribution of both the proposal and target models, and (2) alternating each model as the proposer and verifier can further enhance efficiency. We generalize this method to ensembles with n models and theoretically prove that SE is never slower than a standard ensemble, typically achieving faster speed. Extensive experiments demonstrate speed improvements of 1.11x-2.23x over standard ensemble techniques without compromising generation quality. Our code is available at this https URL', 'abstract_zh': '集成方法通过结合多个模型来增强大型语言模型（LLMs），但会带来较高的计算成本。本文介绍了一种新的框架——推测集成（Speculative Ensemble），该框架能够在不牺牲性能的前提下加速LLM集成。这一框架借鉴了推测解码（Speculative Decoding）的思想——小型提议模型顺序生成令牌，而较大的目标模型并行验证这些令牌。我们的方法基于以下两个关键见解：（1）验证分布可以是提议模型和目标模型的集成分布；（2）轮流将每个模型作为提议者和验证者可以进一步提高效率。我们将此方法推广到具有n个模型的集成，并理论证明推测集成（SE）不会比标准集成慢，通常能够实现更快的速度。大量的实验证明，与标准集成技术相比，推测集成在不牺牲生成质量的情况下可以提高1.11倍至2.23倍的速度。我们的代码可在以下链接获取：this https URL', 'title_zh': '推测性集成：通过推测加快大型语言模型集成'}
{'arxiv_id': 'arXiv:2502.01657', 'title': 'Improving Rule-based Reasoning in LLMs via Neurosymbolic Representations', 'authors': 'Varun Dhanraj, Chris Eliasmith', 'link': 'https://arxiv.org/abs/2502.01657', 'abstract': "Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly tasks that involve precise rule following, as often found in mathematical reasoning tasks. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, allowing for problem-solving within a neurosymbolic vector space. The results are decoded and combined with the original hidden state, boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method improves efficiency, reliability, and interpretability. Our experimental results demonstrate an average of $82.86\\%$ lower cross entropy loss and $24.50$ times more problems correctly solved on a suite of mathematical reasoning problems compared to chain-of-thought prompting and supervised fine-tuning (LoRA), while at the same time not hindering the performance of the LLM on other tasks.", 'abstract_zh': '大型语言模型（LLMs）在解决推理任务时仍然面临挑战，特别是在涉及精确规则遵循的任务方面，这种任务在数学推理任务中很常见。本文提出了一种新型的神经符号方法，该方法通过将隐藏状态编码到神经符号向量中，从而在神经符号向量空间内解决推理问题。解码后，结果与原始隐藏状态结合，提升了模型在数值推理任务上的性能。通过神经符号表示卸载计算，该方法提高了效率、可靠性和可解释性。我们的实验结果表明，与链式思考提示和监督微调（LoRA）相比，该方法在一系列数学推理问题上的交叉熵损失平均减少82.86%，正确解决的问题数量提高了24.50倍，同时并未妨碍LLM在其他任务上的性能。', 'title_zh': '通过神经符号表示提高基于规则的推理在大规模语言模型中的能力'}
{'arxiv_id': 'arXiv:2502.02534', 'title': 'Adaptive Self-improvement LLM Agentic System for ML Library Development', 'authors': 'Genghan Zhang, Weixin Liang, Olivia Hsu, Kunle Olukotun', 'link': 'https://arxiv.org/abs/2502.02534', 'abstract': 'ML libraries, often written in architecture-specific programming languages (ASPLs) that target domain-specific architectures, are key to efficient ML systems. However, writing these high-performance ML libraries is challenging because it requires expert knowledge of ML algorithms and the ASPL. Large language models (LLMs), on the other hand, have shown general coding capabilities. However, challenges remain when using LLMs for generating ML libraries using ASPLs because 1) this task is complicated even for experienced human programmers and 2) there are limited code examples because of the esoteric and evolving nature of ASPLs. Therefore, LLMs need complex reasoning with limited data in order to complete this task. To address these challenges, we introduce an adaptive self-improvement agentic system. In order to evaluate the effectiveness of our system, we construct a benchmark of a typical ML library and generate ASPL code with both open and closed-source LLMs on this benchmark. Our results show improvements of up to $3.9\\times$ over a baseline single LLM.', 'abstract_zh': '以下是对论文内容或标题的翻译，符合学术规范：\n\n机器学习（ML）库通常用针对特定架构的编程语言（ASPLs，Architecture-specific Programming Languages）编写，是高效ML系统的关键。然而，编写这些高性能ML库具有挑战性，因为这需要对ML算法和ASPL有深刻的专业知识。相比之下，大规模语言模型（LLMs）已经展示了普遍的编码能力。然而，使用LLMs生成基于ASPL的ML库仍然存在挑战，原因在于：1）即使对于经验丰富的程序员，这项任务也非常复杂；2）由于ASPLs的专有性和不断演变性，可用的代码示例较少。因此，LLMs需要在有限的数据下进行复杂的推理以完成这一任务。为了应对这些挑战，我们提出了一种自适应自我改进的代理系统。为了评估该系统的有效性，我们基于一个典型的ML库构建了一个基准测试，并使用开源和闭源的LLMs在此基准测试上生成了ASPL代码。我们的结果显示，与单一LLM基线相比，改进幅度最高可达$3.9\\times$。', 'title_zh': '面向机器学习库开发的自适应自我改进代理系统'}
{'arxiv_id': 'arXiv:2502.02451', 'title': 'Beyond English: Evaluating Automated Measurement of Moral Foundations in Non-English Discourse with a Chinese Case Study', 'authors': 'Calvin Yixiang Cheng, Scott A Hale', 'link': 'https://arxiv.org/abs/2502.02451', 'abstract': 'This study explores computational approaches for measuring moral foundations (MFs) in non-English corpora. Since most resources are developed primarily for English, cross-linguistic applications of moral foundation theory remain limited. Using Chinese as a case study, this paper evaluates the effectiveness of applying English resources to machine translated text, local language lexicons, multilingual language models, and large language models (LLMs) in measuring MFs in non-English texts. The results indicate that machine translation and local lexicon approaches are insufficient for complex moral assessments, frequently resulting in a substantial loss of cultural information. In contrast, multilingual models and LLMs demonstrate reliable cross-language performance with transfer learning, with LLMs excelling in terms of data efficiency. Importantly, this study also underscores the need for human-in-the-loop validation of automated MF assessment, as the most advanced models may overlook cultural nuances in cross-language measurements. The findings highlight the potential of LLMs for cross-language MF measurements and other complex multilingual deductive coding tasks.', 'abstract_zh': '本文探讨了在非英语语料中测量道德基础（MFs）的计算方法。由于大多数资源主要针对英语开发，因此道德基础理论在跨语言应用中的使用仍受到限制。以汉语为例，本文评估了将英语资源应用于机器翻译文本、本地语言词典、多语言语言模型以及大型语言模型（LLMs）在测量非英语文本中道德基础的有效性。研究结果表明，机器翻译和本地词典方法在复杂的道德评估中不足，往往导致大量文化信息的丢失。相比之下，多语言模型和LLMs在迁移学习中表现出可靠的语言间性能，而LLMs在数据效率方面尤为突出。此外，研究还强调了在自动化道德基础评估中需要人工校验的重要性，即使最先进的模型也可能在跨语言测量中忽略文化细微差别。研究结果突显了大型语言模型在跨语言道德基础测量以及复杂多语言演绎编码任务中的潜在应用价值。', 'title_zh': '超越英语：在非英语语境中评估道德基础自动化测量方法——以一项中文案例研究为例'}
{'arxiv_id': 'arXiv:2502.02384', 'title': 'STAIR: Improving Safety Alignment with Introspective Reasoning', 'authors': 'Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng Dong, Jun Zhu', 'link': 'https://arxiv.org/abs/2502.02384', 'abstract': 'Ensuring the safety and harmlessness of Large Language Models (LLMs) has become equally critical as their performance in applications. However, existing safety alignment methods typically suffer from safety-performance trade-offs and the susceptibility to jailbreak attacks, primarily due to their reliance on direct refusals for malicious queries. In this paper, we propose STAIR, a novel framework that integrates SafeTy Alignment with Itrospective Reasoning. We enable LLMs to identify safety risks through step-by-step analysis by self-improving chain-of-thought (CoT) reasoning with safety awareness. STAIR first equips the model with a structured reasoning capability and then advances safety alignment via iterative preference optimization on step-level reasoning data generated using our newly proposed Safety-Informed Monte Carlo Tree Search (SI-MCTS). We further train a process reward model on this data to guide test-time searches for improved responses. Extensive experiments show that STAIR effectively mitigates harmful outputs while better preserving helpfulness, compared to instinctive alignment strategies. With test-time scaling, STAIR achieves a safety performance comparable to Claude-3.5 against popular jailbreak attacks. Relevant resources in this work are available at this https URL.', 'abstract_zh': '确保大型语言模型（LLMs）的安全性和无害性已成为与它们在应用中的性能同样重要的问题。然而，现有的安全对齐方法通常会遭受安全性能权衡和对抗囚笼攻击的脆弱性，主要是因为它们依赖于直接拒绝恶意查询。本文提出了一种新型框架STAIR，该框架将安全对齐与内省推理相结合。通过自我提升的逐步分析和带有安全意识的推理链（CoT）来使LLMs能够识别安全风险。STAIR首先增强模型的结构化推理能力，然后通过迭代偏好优化逐步推理数据，这些数据是通过我们新提出的Safety-Informed Monte Carlo Tree Search（SI-MCTS）生成的。我们进一步在这些数据上训练一个过程奖励模型，以指导测试时的搜索，从而改进响应质量。广泛的实验表明，与本能的安全对齐策略相比，STAIR能更有效地减少有害输出，同时更好地保留有用性。在测试时扩展后，STAIR在对抗流行的囚笼攻击方面达到了与Claude-3.5相似的安全性能。本工作中相关的资源可在如下链接获取：[提供的链接]。', 'title_zh': 'STAIR：通过反省推理提高安全性对齐'}
{'arxiv_id': 'arXiv:2502.02362', 'title': 'Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs', 'authors': 'Sagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, Dilek Hakkani Tur', 'link': 'https://arxiv.org/abs/2502.02362', 'abstract': 'Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.', 'abstract_zh': 'chain-of-thought (CoT) 嵌入增强大型语言模型（LLMs）的数学推理能力，通过实现详细的逐步解决方案。然而，由于LLMs的冗长特性，生成的推理链可能变得很长，这使得验证推理步骤变得更加困难，并且难以追踪由于步骤之间依赖关系而可能导致的更远处的问题。重要的是，数学推理允许每个步骤都从之前几步的一个小集合的假设推导出来。在这篇论文中，我们提出了一种框架，用于识别每个步骤的假设，以改进推理的评估。我们将传统的线性推理链重构为前提增强推理链（PARC），通过引入前提链接，形成了一个有向无环图，其中节点是步骤，边是前提链接。通过使用我们构建的基于PARC的数据集PERL（前提和错误识别在LLMs中），我们展示了LLMs可以可靠地在复杂的推理链中识别前提。特别是，即使是开源的LLMs在前提识别中的召回率达到了90%。我们还展示了PARC有助于更可靠地识别推理链中的错误。在有前提的PARC中进行逐步验证时，错误识别的准确性提高了6%到16%。我们的发现突出了以前提为中心的表示方法在解决复杂问题方面的效用，并为提高基于LLMs的推理评估的可靠性打开了新的途径。', 'title_zh': '增强前提的推理链在LLM进行数学推理中的错误识别中效果提升'}
{'arxiv_id': 'arXiv:2502.02339', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'authors': 'Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao', 'link': 'https://arxiv.org/abs/2502.02339', 'abstract': "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining substantial data and computational efficiency.", 'abstract_zh': '多模态大型语言模型（MLLMs）展现了令人印象深刻的能力，但在复杂的视觉推理方面仍然面临挑战。尽管近期的努力试图通过引入类似于OpenAI o1的结构化思考方式，如显式搜索结构或教师引导的蒸馏来增强MLLMs的推理能力，但它们往往难以在性能和效率之间取得平衡。一个关键限制是，这些模型对大量数据和搜索空间的依赖性很强，导致了低效的隐含洞察提取和数据利用。为了解决这一问题，我们提出了AStar，这是一种通过蒙特卡洛树搜索（MCTS）实现自动结构化思考的多模态推理范式。AStar 利用MCTS驱动的分层结构，从有限的数据中自动推导出高级的认知推理模式。基于这些显式的模式，我们设计了一个统一的推理框架，该框架可以无缝地整合模型的内部推理能力与外部推理准则，从而通过最少的树迭代实现高效的推理。这一新的范式在性能和效率之间取得了令人 impressive 的平衡。广泛实验表明，AStar 的有效性得到了验证：它在使用7B参数骨干网络的情况下，在MathVerse基准上达到了54.0%的准确率，超越了GPT-4o（50.2%），同时保持了显著的数据和计算效率。', 'title_zh': '使用MCTS自动结构化思维增强多模态推理'}
{'arxiv_id': 'arXiv:2502.02289', 'title': 'Evalita-LLM: Benchmarking Large Language Models on Italian', 'authors': 'Bernardo Magnini, Roberto Zanoli, Michele Resta, Martin Cimmino, Paolo Albano, Marco Madeddu, Viviana Patti', 'link': 'https://arxiv.org/abs/2502.02289', 'abstract': "We describe Evalita-LLM, a new benchmark designed to evaluate Large Language Models (LLMs) on Italian tasks. The distinguishing and innovative features of Evalita-LLM are the following: (i) all tasks are native Italian, avoiding issues of translating from Italian and potential cultural biases; (ii) in addition to well established multiple-choice tasks, the benchmark includes generative tasks, enabling more natural interaction with LLMs; (iii) all tasks are evaluated against multiple prompts, this way mitigating the model sensitivity to specific prompts and allowing a fairer and objective evaluation. We propose an iterative methodology, where candidate tasks and candidate prompts are validated against a set of LLMs used for development. We report experimental results from the benchmark's development phase, and provide performance statistics for several state-of-the-art LLMs.", 'abstract_zh': '我们描述了Evalita-LLM，这是一个新的基准，用于在意大利语任务上评估大型语言模型（LLMs）。Evalita-LLM 的主要创新特征如下：（i）所有任务均为地道的意大利语，避免了从意大利语翻译带来的问题和潜在的文化偏见；（ii）除了常用的多项选择任务外，该基准还包含生成任务，从而使与LLMs的交互更加自然；（iii）所有任务都采用多种提示进行评估，从而减轻了模型对特定提示的敏感性，并提供了更为公平和客观的评估。我们提出了一个迭代的方法论，其中候选任务和候选提示通过一组用于开发的LLMs进行验证。我们报告了基准测试开发阶段的实验结果，并提供了几种最先进的LLMs的性能统计数据。', 'title_zh': 'Evalita-LLM：评估大型语言模型在意大利语上的性能'}
{'arxiv_id': 'arXiv:2502.02095', 'title': 'LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information', 'authors': 'Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, Shanghang Zhang', 'link': 'https://arxiv.org/abs/2502.02095', 'abstract': 'Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.', 'abstract_zh': '长文本生成在学术论文写作和仓库级别代码生成中至关重要。尽管如此，现有的包括GPT-4o在内的模型仍然表现出不尽如人意的表现。现有利用偏好学习和结果监督的方法往往无法为扩展上下文提供详细的反馈，这一不足可能导致生成的内容不能充分满足查询要求，从而出现长度偏离和质量下降等问题。在本文中，我们通过引入过程监督来增强长文本生成。我们采用了蒙特卡洛树搜索来收集逐步偏好对，并利用全局记忆池来保持一致性。为了应对候选选择不理想的问题，我们整合外部评论以精炼和提高偏好对的质量。最后，我们使用收集的逐步偏好对应用逐步骤DPO。实验结果表明，我们的方法在长文本生成基准上提高了长度和质量，在不同模型基础的一般基准上性能几乎无损。', 'title_zh': '长段落生成：通过批评增强的逐步信息生成策略提升大型语言模型的长文本生成能力'}
{'arxiv_id': 'arXiv:2502.02074', 'title': 'Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models', 'authors': 'Prasanta Bhattacharya, Hong Zhang, Yiming Cao, Wei Gao, Brandon Siyuan Loh, Joseph J.P. Simons, Liang Ze Wong', 'link': 'https://arxiv.org/abs/2502.02074', 'abstract': 'Stance detection has emerged as a popular task in natural language processing research, enabled largely by the abundance of target-specific social media data. While there has been considerable research on the development of stance detection models, datasets, and application, we highlight important gaps pertaining to (i) a lack of theoretical conceptualization of stance, and (ii) the treatment of stance at an individual- or user-level, as opposed to message-level. In this paper, we first review the interdisciplinary origins of stance as an individual-level construct to highlight relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models. Further, we argue that recent pre-trained and large language models (LLMs) might offer a way to flexibly infer such user-level attributes and/or incorporate them in modelling stance. To better illustrate this, we briefly review and synthesize the emerging corpus of studies on using LLMs for inferring stance, and specifically on incorporating user attributes in such tasks. We conclude by proposing a four-point agenda for pursuing stance detection research that is theoretically informed, inclusive, and practically impactful.', 'abstract_zh': '立场检测已成为自然语言处理研究中的一个热门任务，这在很大程度上得益于特定目标社交媒体数据的丰富性。尽管在立场检测模型、数据集和应用方面已有相当多的研究，但我们仍指出存在两个重要缺口：（i）缺乏对立场的理论概念化，以及（ii）通常将立场视为个体级别或用户级别的属性，而非消息级别的属性。在本文中，我们首先回顾立场作为个体级别建构的多学科起源，以突出在立场检测模型中可能需要纳入的相关属性（例如，心理特征）。此外，我们认为最近的预训练和大型语言模型（LLMs）可能提供了一种灵活推断此类用户级别属性或在模型中纳入这些属性的方法。为了更好地说明这一点，我们简要回顾并综合了使用LLMs推断立场的研究，特别是关注嵌入用户属性的研究领域。最后，我们提出了一项四项议程，旨在推动理论基础、包容性和实际影响兼备的立场检测研究。', 'title_zh': '重思立场检测：基于理论的语言模型在用户级推断研究议程'}
{'arxiv_id': 'arXiv:2502.02046', 'title': 'Contextual Memory Reweaving in Large Language Models Using Layered Latent State Reconstruction', 'authors': 'Frederick Dillon, Gregor Halvorsen, Simon Tattershall, Magnus Rowntree, Gareth Vanderpool', 'link': 'https://arxiv.org/abs/2502.02046', 'abstract': 'Memory retention challenges in deep neural architectures have ongoing limitations in the ability to process and recall extended contextual information. Token dependencies degrade as sequence length increases, leading to a decline in coherence and factual consistency across longer outputs. A structured approach is introduced to mitigate this issue through the reweaving of latent states captured at different processing layers, reinforcing token representations over extended sequences. The proposed Contextual Memory Reweaving framework incorporates a Layered Latent State Reconstruction mechanism to systematically integrate past contextual embeddings without introducing external memory modules. Experimental results demonstrate improvements in recall accuracy across a range of sequence lengths, with notable gains in the retention of rarely occurring tokens and numerical reasoning consistency. Further analysis of computational efficiency indicates that the additional processing overhead remains within acceptable thresholds, enabling scalability across different model sizes. Evaluations in long-form text generation and ambiguous query resolution highlight the capacity of memory reweaving to enhance continuity and reduce inconsistencies over extended outputs. Attention weight distributions reveal more structured allocation patterns, suggesting that reweaved latent states contribute to improved contextual awareness. The findings establish a framework for refining memory retention mechanisms in language models, addressing long-standing challenges in handling complex, multi-step reasoning tasks.', 'abstract_zh': '深度神经架构中的记忆保持挑战限制了其处理和回忆长时间上下文信息的能力。随着序列长度的增加，标记之间的依赖性减弱，导致较长输出的一致性和事实准确性下降。本文提出了一种结构化方法，通过重新编织不同处理层捕获的潜在状态来缓解这一问题，强化扩展序列中的标记表示。提出的上下文记忆重新编织框架结合了一种分层潜在状态重构机制，系统地整合过去上下文嵌入，而不引入外部内存模块。实验结果表明，在不同序列长度下回忆准确度均有提高，特别是在稀有标记和数值推理一致性方面的显著提升。进一步的计算效率分析表明，额外的处理开销保持在可接受的范围内，从而使得该框架可以在不同模型规模下扩展。在长文本生成和模糊查询解析中的评估表明，记忆重新编织能够增强连续性并降低长时间输出中的不一致性。注意力权重分布揭示了更结构化的分配模式，表明重新编织的潜在状态有助于提高上下文意识。这些发现为改进语言模型中的记忆保持机制奠定了框架，解决了长期存在的处理复杂多步推理任务的挑战。', 'title_zh': '在大型语言模型中使用分层潜在状态重构实现上下文记忆重新编织'}
{'arxiv_id': 'arXiv:2502.02007', 'title': 'Reasoning Bias of Next Token Prediction Training', 'authors': 'Pengxiao Lin, Zhongwang Zhang, Zhi-Qin John Xu', 'link': 'https://arxiv.org/abs/2502.02007', 'abstract': "Since the inception of Large Language Models (LLMs), the quest to efficiently train them for superior reasoning capabilities has been a pivotal challenge. The dominant training paradigm for LLMs is based on next token prediction (NTP). Alternative methodologies, called Critical Token Prediction (CTP), focused exclusively on specific critical tokens (such as the answer in Q\\&A dataset), aiming to reduce the overfitting of extraneous information and noise. Contrary to initial assumptions, our research reveals that despite NTP's exposure to noise during training, it surpasses CTP in reasoning ability. We attribute this counterintuitive outcome to the regularizing influence of noise on the training dynamics. Our empirical analysis shows that NTP-trained models exhibit enhanced generalization and robustness across various benchmark reasoning datasets, demonstrating greater resilience to perturbations and achieving flatter loss minima. These findings illuminate that NTP is instrumental in fostering reasoning abilities during pretraining, whereas CTP is more effective for finetuning, thereby enriching our comprehension of optimal training strategies in LLM development.", 'abstract_zh': '自大型语言模型（LLMs）问世以来，如何高效训练以提升其推理能力成为了一个关键挑战。LLMs的主要训练范式基于下一个令牌预测（NTP）。而替代性方法，称为关键令牌预测（CTP），则专注于特定的关键令牌（如问答（Q&A）数据集中答案），旨在减少无关信息和噪声的过拟合。与最初的假设相反，我们的研究表明，尽管NTP在训练过程中会暴露于噪声，但它的推理能力仍然优于CTP。我们归因于这种反直觉结果的原因是噪声对训练动力学的正则化影响。我们的实证分析表明，NTP训练的模型在各种基准推理数据集上表现出更强的泛化能力和稳健性，更能抵御干扰并达到更平坦的损失极小值。这些研究成果揭示了NTP在预训练期间有助于促进推理能力，而CTP则在微调阶段更为有效，从而丰富了我们对LLM开发中最佳训练策略的理解。', 'title_zh': '下一词预测训练中的推理偏差'}
{'arxiv_id': 'arXiv:2502.01901', 'title': 'Conceptual Metaphor Theory as a Prompting Paradigm for Large Language Models', 'authors': 'Oliver Kramer', 'link': 'https://arxiv.org/abs/2502.01901', 'abstract': "We introduce Conceptual Metaphor Theory (CMT) as a framework for enhancing large language models (LLMs) through cognitive prompting in complex reasoning tasks. CMT leverages metaphorical mappings to structure abstract reasoning, improving models' ability to process and explain intricate concepts. By incorporating CMT-based prompts, we guide LLMs toward more structured and human-like reasoning patterns. To evaluate this approach, we compare four native models (Llama3.2, Phi3, Gemma2, and Mistral) against their CMT-augmented counterparts on benchmark tasks spanning domain-specific reasoning, creative insight, and metaphor interpretation. Responses were automatically evaluated using the Llama3.3 70B model. Experimental results indicate that CMT prompting significantly enhances reasoning accuracy, clarity, and metaphorical coherence, outperforming baseline models across all evaluated tasks.", 'abstract_zh': '我们将概念隐喻理论（CMT）引入作为通过认知提示增强大型语言模型（LLMs）的框架，以提高其在复杂推理任务中的能力。CMT利用隐喻映射来结构化抽象推理，从而提高模型处理和解释复杂概念的能力。通过引入基于CMT的提示，我们引导LLMs向更加结构化和人类化的推理模式发展。为了评估这种方法，我们在涵盖领域特定推理、创造性洞察力和隐喻解释的基准任务中，将四个原生模型（Llama3.2、Phi3、Gemma2和Mistral）与其增强的CMT版本进行了对比。响应由Llama3.3 70B模型自动评估。实验结果表明，CMT提示显著提高了推理的准确性和清晰度以及隐喻的一致性，在所有评估任务中均优于基线模型。', 'title_zh': '概念隐喻理论作为大型语言模型的促动范式'}
{'arxiv_id': 'arXiv:2502.01812', 'title': 'SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models', 'authors': 'Diyana Muhammed, Gollam Rabby, Sören Auer', 'link': 'https://arxiv.org/abs/2502.01812', 'abstract': "Detecting hallucinations in Large Language Models (LLMs) remains a critical challenge for their reliable deployment in real-world applications. To address this, we introduce SelfCheckAgent, a novel framework integrating three different agents: the Symbolic Agent, the Specialized Detection Agent, and the Contextual Consistency Agent. These agents provide a robust multi-dimensional approach to hallucination detection. Notable results include the Contextual Consistency Agent leveraging Llama 3.1 with Chain-of-Thought (CoT) to achieve outstanding performance on the WikiBio dataset, with NonFactual hallucination detection scoring 93.64%, Factual 70.26%, and Ranking 78.48% respectively. On the AIME dataset, GPT-4o with CoT excels in NonFactual detection with 94.89% but reveals trade-offs in Factual with 30.58% and Ranking with 30.68%, underscoring the complexity of hallucination detection in the complex mathematical domains. The framework also incorporates a triangulation strategy, which increases the strengths of the SelfCheckAgent, yielding significant improvements in real-world hallucination identification. The comparative analysis demonstrates SelfCheckAgent's applicability across diverse domains, positioning it as a crucial advancement for trustworthy LLMs. These findings highlight the potentiality of consistency-driven methodologies in detecting hallucinations in LLMs.", 'abstract_zh': '在现实世界应用中可靠部署大型语言模型（LLMs）的关键挑战之一是检测幻觉。为此，我们引入了SelfCheckAgent，这是一种新颖的框架，集成了三个不同的智能体：符号智能体、专业化检测智能体以及上下文一致性智能体。这些智能体提供了一种稳健的多维方法来检测幻觉。值得注意的是，上下文一致性智能体利用Llama 3.1和思维链（CoT）在WikiBio数据集上取得了显著性能，非事实幻觉检测得分为93.64%，事实幻觉得分为70.26%，排名得分为78.48%。在AIME数据集上，GPT-4o配以思维链在非事实幻觉检测方面表现出色，得分为94.89%，但在事实幻觉和排名方面分别仅得30.58%和30.68%，这表明在复杂数学领域中幻觉检测的复杂性。该框架还采用了三角测量策略，这增强了SelfCheckAgent的优势，显著改善了实际幻觉识别的性能。通过比较分析可以看出，SelfCheckAgent适用于多种领域，将其确立为增强可信LLMs的关键进步。这些发现突显了以一致性驱动方法在检测LLMs中幻觉方面的潜力。', 'title_zh': 'SelfCheckAgent：生成型大规模语言模型中的零资源幻觉检测'}
{'arxiv_id': 'arXiv:2502.02407', 'title': 'Avoiding spurious sharpness minimization broadens applicability of SAM', 'authors': 'Sidak Pal Singh, Hossein Mobahi, Atish Agarwala, Yann Dauphin', 'link': 'https://arxiv.org/abs/2502.02407', 'abstract': 'Curvature regularization techniques like Sharpness Aware Minimization (SAM) have shown great promise in improving generalization on vision tasks. However, we find that SAM performs poorly in domains like natural language processing (NLP), often degrading performance -- even with twice the compute budget. We investigate the discrepancy across domains and find that in the NLP setting, SAM is dominated by regularization of the logit statistics -- instead of improving the geometry of the function itself. We use this observation to develop an alternative algorithm we call Functional-SAM, which regularizes curvature only through modification of the statistics of the overall function implemented by the neural network, and avoids spurious minimization through logit manipulation. Furthermore, we argue that preconditioning the SAM perturbation also prevents spurious minimization, and when combined with Functional-SAM, it gives further improvements. Our proposed algorithms show improved performance over AdamW and SAM baselines when trained for an equal number of steps, in both fixed-length and Chinchilla-style training settings, at various model scales (including billion-parameter scale). On the whole, our work highlights the importance of more precise characterizations of sharpness in broadening the applicability of curvature regularization to large language models (LLMs).', 'abstract_zh': '类似于Sharpness Aware Minimization (Sharpness Aware Minimization, SAM)这样的曲率正则化技术在视觉任务中的泛化能力方面显示出巨大的潜力。然而，我们发现SAM在自然语言处理（NLP）等领域表现较差，通常会降低性能——即使在计算预算加倍的情况下也是如此。我们研究了不同领域的差异，发现NLP环境中SAM主要通过正则化逻辑概率统计来起作用，而不是通过改进函数本身的几何结构。基于这一观察，我们开发了一个新的算法，称为Functional-SAM，该算法仅通过修改神经网络实现的整体函数的统计量来正则化曲率，并避免通过逻辑概率操纵导致的错误最小化。此外，我们还提出，预处理SAM扰动也可以防止错误最小化，而当与Functional-SAM结合使用时，它可以进一步提高性能。我们提出的算法在固定的训练步数下，无论是固定长度训练还是沿用Chinchilla风格的训练设置，在各种模型规模（包括十亿参数规模）下，都显示出优于AdamW和SAM基线模型的性能。总体而言，我们的工作突出了更精确描述尖锐性的重要性，在扩大曲率正则化在大规模语言模型（LLMs）中的适用范围方面具有重要意义。', 'title_zh': '避免虚假锐利度最小化扩大了SAM的应用范围'}
{'arxiv_id': 'arXiv:2502.02329', 'title': 'ReSpark: Leveraging Previous Data Reports as References to Generate New Reports with LLMs', 'authors': 'Yuan Tian, Chuhan Zhang, Xiaotong Wang, Sitong Pan, Weiwei Cui, Haidong Zhang, Dazhen Deng, Yingcai Wu', 'link': 'https://arxiv.org/abs/2502.02329', 'abstract': 'Creating data reports is time-consuming, as it requires iterative exploration and understanding of data, followed by summarizing the insights. While large language models (LLMs) are powerful tools for data processing and text generation, they often struggle to produce complete data reports that fully meet user expectations. One significant challenge is effectively communicating the entire analysis logic to LLMs. Moreover, determining a comprehensive analysis logic can be mentally taxing for users. To address these challenges, we propose ReSpark, an LLM-based method that leverages existing data reports as references for creating new ones. Given a data table, ReSpark searches for similar-topic reports, parses them into interdependent segments corresponding to analytical objectives, and executes them with new data. It identifies inconsistencies and customizes the objectives, data transformations, and textual descriptions. ReSpark allows users to review real-time outputs, insert new objectives, and modify report content. Its effectiveness was evaluated through comparative and user studies.', 'abstract_zh': '生成数据报告耗时较长，因为这需要迭代地探索和理解数据，进而总结出洞察。虽然大型语言模型（LLMs）在数据处理和文本生成方面具有强大功能，但它们往往难以生成完全符合用户期望的完整数据报告。一个重要挑战是如何有效地向LLMs传达整个分析逻辑。此外，确定全面的分析逻辑对用户来说可能是一种认知负担。为了解决这些挑战，我们提出了ReSpark，一种基于LLM的方法，利用现有数据报告作为参考来创建新的报告。给定一个数据表格，ReSpark 搜索相关主题的报告，解析它们成为与分析目标相关的相互依存段落，并使用新的数据执行这些段落。它会识别不一致之处，并自定义目标、数据转换和文本描述。ReSpark 允许用户实时审查输出结果、插入新目标并修改报告内容。其效果通过对比性和用户研究进行了评估。', 'title_zh': 'ReSpark: 利用先前的数据报告作为参考生成新报告的LLM方法'}
{'arxiv_id': 'arXiv:2502.02315', 'title': 'VaiBot: Shuttle Between the Instructions and Parameters', 'authors': 'Wangtao Sun, Haotian Xu, Huanxuan Liao, Xuanqing Yu, Zhongtao Jiang, Shizhu He, Jun Zhao, Kang Liu', 'link': 'https://arxiv.org/abs/2502.02315', 'abstract': 'How to interact with LLMs through \\emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. The code and data for this paper can be found at this https URL.', 'abstract_zh': '通过指令与大规模语言模型（LLM）互动的方式已被研究人员广泛研究。然而，之前的研究所处理的指令的出现与LLM在任务数据上的训练这两个过程被当作独立的过程，忽视了两者之间的内在统一性。本文提出了一种神经网络框架VaiBot，该框架结合了VAE（变分自编码器）和VIB（变异信息瓶颈），旨在统一建模、学习和推断LLM下的演绎和归纳任务。通过实验，我们证明了VaiBot在演绎能力方面与现有基线方法相当，而在归纳能力方面显著超越它们。我们还发现，VaiBot可以用一般的指令遵循数据进行扩展，并表现出优异的一次性归纳能力。最后，我们以一种协同方式整合了VaiBot的演绎和归纳过程。通过T-SNE降维，我们观察到其演绎归纳过程显著改善了训练参数的分布，使其在归纳推理任务中超越了基线方法。这篇论文的代码和数据可以在这个网址 https://github.com/example-repo 找到。', 'title_zh': 'VaiBot：指令与参数之间的桥梁'}
{'arxiv_id': 'arXiv:2502.02201', 'title': 'Can You Move These Over There? An LLM-based VR Mover for Supporting Object Manipulation', 'authors': 'Xiangzhi Eric Wang, Zackary P. T. Sin, Ye Jia, Daniel Archer, Wynonna H. Y. Fong, Qing Li, Chen Li', 'link': 'https://arxiv.org/abs/2502.02201', 'abstract': "In our daily lives, we can naturally convey instructions for the spatial manipulation of objects using words and gestures. Transposing this form of interaction into virtual reality (VR) object manipulation can be beneficial. We propose VR Mover, an LLM-empowered solution that can understand and interpret the user's vocal instruction to support object manipulation. By simply pointing and speaking, the LLM can manipulate objects without structured input. Our user study demonstrates that VR Mover enhances user usability, overall experience and performance on multi-object manipulation, while also reducing workload and arm fatigue. Users prefer the proposed natural interface for broad movements and may complementarily switch to gizmos or virtual hands for finer adjustments. These findings are believed to contribute to design implications for future LLM-based object manipulation interfaces, highlighting the potential for more intuitive and efficient user interactions in VR environments.", 'abstract_zh': '在我们的日常生活中，我们自然能够通过语言和手势传达对物体的空间操作指令。将这种交互形式移植到虚拟现实（VR）物体操作中是有益的。我们提出了VR Mover，这是一种由语言模型（LLM）赋能的解决方案，能够理解和解释用户的语音指令以支持物体操作。用户只需指一指、说一说，LLM 就能够无需结构化输入地操作物体。我们的用户研究显示，VR Mover 能够提升用户的易用性、整体体验和多物体操作的表现，同时减少工作负担和手臂疲劳。用户更偏好提出的自然界面用于广泛的操作，并且可以在需要精细调整时切换到操纵杆或虚拟手。这些发现被认为有助于为未来基于语言模型的物体操作界面的设计提供启示，强调了在 VR 环境中实现更直观和高效用户交互的潜力。', 'title_zh': '当然，以下是翻译内容：\n\n《物体操作支持的基于LLM的VR搬运器：你可以把这些移到那边吗？》\n\n这里的“LLM”指的是语言模型（Language Model），在翻译时可以根据上下文具体指代的内容来选择合适的术语或保持原文缩写。如果是指特定的技术或模型，可以进一步明确为“基于语言模型”或直接使用“LLM”。'}
{'arxiv_id': 'arXiv:2502.02066', 'title': 'Anticipate & Act : Integrating LLMs and Classical Planning for Efficient Task Execution in Household Environments', 'authors': 'Raghav Arora, Shivam Singh, Karthik Swaminathan, Ahana Datta, Snehasis Banerjee, Brojeshwar Bhowmick, Krishna Murthy Jatavallabhula, Mohan Sridharan, Madhava Krishna', 'link': 'https://arxiv.org/abs/2502.02066', 'abstract': "Assistive agents performing household tasks such as making the bed or cooking breakfast often compute and execute actions that accomplish one task at a time. However, efficiency can be improved by anticipating upcoming tasks and computing an action sequence that jointly achieves these tasks. State-of-the-art methods for task anticipation use data-driven deep networks and Large Language Models (LLMs), but they do so at the level of high-level tasks and/or require many training examples. Our framework leverages the generic knowledge of LLMs through a small number of prompts to perform high-level task anticipation, using the anticipated tasks as goals in a classical planning system to compute a sequence of finer-granularity actions that jointly achieve these goals. We ground and evaluate our framework's abilities in realistic scenarios in the VirtualHome environment and demonstrate a 31% reduction in execution time compared with a system that does not consider upcoming tasks.", 'abstract_zh': '执行诸如整理床铺或准备早餐等家务任务的辅助代理通常会计算和执行完成单一任务的操作。然而，通过预测即将执行的任务并计算一个能够同时完成这些任务的操作序列，效率可以得到提升。当前最先进的任务预测方法使用数据驱动的深度网络和大语言模型（LLMs），但这些方法通常是在高层任务的水平上进行的，并且需要大量的训练示例。我们的框架通过少量提示利用LLMs的通用知识来进行高层任务预测，并将预测的任务作为目标，在经典规划系统中计算实现这些目标的一系列细粒度动作。我们已在VirtualHome环境中将该框架的能力应用于现实场景，并展示了与不考虑即将执行的任务的系统相比，执行时间减少了31%。', 'title_zh': '预见与行动：将大语言模型与经典规划相结合以在家庭环境中高效执行任务'}
{'arxiv_id': 'arXiv:2502.02048', 'title': 'Efficient Domain Adaptation of Multimodal Embeddings using Constrastive Learning', 'authors': 'Georgios Margaritis, Periklis Petridis, Dimitris J. Bertsimas', 'link': 'https://arxiv.org/abs/2502.02048', 'abstract': 'Recent advancements in machine learning (ML), natural language processing (NLP), and foundational models have shown promise for real-life applications in critical, albeit compute-constrainted fields like healthcare.\nIn such areas, combining foundational models with supervised ML offers potential for automating tasks like diagnosis and treatment planning, but the limited availability of onsite computational resources pose significant challenges before applying these technologies effectively: Current approaches either yield subpar results when using pretrained models without task-specific adaptation, or require substantial computational resources for fine-tuning, which is often a barrier to entry in such environments.\nThis renders them inaccessible in applications where performance and quality standards are high, but computational resources are scarce.\nTo bridge the gap between best-in-class performance and accessibility, we propose a novel method for adapting foundational, multimodal embeddings to downstream tasks, without the need of expensive fine-tuning processes.\nOur method leverages frozen embeddings from Large Language Models (LLMs) and Vision Models, and uses contrastive learning to train a small, task-specific nonlinear projection that can be used in the downstream task, without having to fine-tune the original foundational models.\nWe show that this efficient procedure leads to significant performance improvements across various downstream tasks, and perhaps more importantly with minimal computational overhead, offering a practical solution for the use of advanced, foundational ML models in resource-constrained settings.', 'abstract_zh': '近年来，机器学习（ML）、自然语言处理（NLP）以及基础模型的进展在诸如医疗保健等关键但计算资源受限的领域中展现了应用的潜力。在这些领域中，将基础模型与监督式ML相结合可以在自动化诊断和治疗规划等任务上提供可能性，但现场计算资源的有限供应在有效应用这些技术时提出了重大挑战：当前的方法要么在使用预训练模型而无需特定任务调整时结果不佳，要么需要大量的计算资源进行调优，而这通常是这些环境中进入的技术障碍。\n\n这使得在高性能和高质量标准要求高但计算资源稀缺的应用场景中，这些技术难以使用。为缩小顶级性能与易用性之间的差距，我们提出了一种新型方法，用于适应基础的多模态嵌入，而无需昂贵的调优过程。\n\n我们的方法利用大型语言模型（LLMs）和视觉模型中的冻结嵌入，并使用对比学习训练一个小的、特定任务的非线性投影，该投影可以直接应用于下游任务，而不需要重新调整个基础模型。我们展示了这种高效的方法在各种下游任务上带来了显著的性能提升，并且更为重要的是，这种提升伴随着最少的计算开销，为在计算资源受限的环境中使用先进的基础ML模型提供了实际解决方案。', 'title_zh': '使用对比学习进行多模态嵌入的高效领域自适应'}
{'arxiv_id': 'arXiv:2502.02061', 'title': 'Large Language Models for Recommendation with Deliberative User Preference Alignment', 'authors': 'Yi Fang, Wenjie Wang, Yang Zhang, Fengbin Zhu, Qifan Wang, Fuli Feng, Xiangnan He', 'link': 'https://arxiv.org/abs/2502.02061', 'abstract': 'While recent advancements in aligning Large Language Models (LLMs) with recommendation tasks have shown great potential and promising performance overall, these aligned recommendation LLMs still face challenges in complex scenarios. This is primarily due to the current alignment approach focusing on optimizing LLMs to generate user feedback directly, without incorporating deliberation. To overcome this limitation and develop more reliable LLMs for recommendations, we propose a new Deliberative Recommendation task, which incorporates explicit reasoning about user preferences as an additional alignment goal. We then introduce the Deliberative User Preference Alignment framework, designed to enhance reasoning capabilities by utilizing verbalized user feedback in a step-wise manner to tackle this task. The framework employs collaborative step-wise experts and tailored training strategies for each expert. Experimental results across three real-world datasets demonstrate the rationality of the deliberative task formulation and the superior performance of the proposed framework in improving both prediction accuracy and reasoning quality.', 'abstract_zh': '虽然近期在将大型语言模型（LLMs）与推荐任务对齐方面取得了显著进展并展现了良好的整体性能，但这些对齐的推荐LLMs在复杂场景下仍面临挑战。这主要是因为当前的对齐方法侧重于优化LLMs以直接生成用户反馈，而忽略了推理过程。为了克服这一局限，开发更为可靠的推荐LLMs，我们提出了一项新的“推理推荐”任务，该任务将用户偏好显式推理作为额外的对齐目标。随后，我们引入了“推理用户偏好对齐”框架，旨在通过逐步利用口头反馈来增强推理能力，从而应对这一任务。该框架采用协作的逐步专家，并为每个专家设计了定制化的训练策略。实验结果表明，通过三个真实数据集的测试，推理任务的表述是合乎逻辑的，所提出的框架在提高预测准确性和推理质量方面具有优越表现。', 'title_zh': '大规模语言模型在迭代用户偏好对齐中的推荐应用'}
