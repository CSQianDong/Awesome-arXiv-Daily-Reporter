{'arxiv_id': 'arXiv:2502.02406', 'title': 'LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models', 'authors': 'Tzu-Tao Chang, Shivaram Venkataraman', 'link': 'https://arxiv.org/abs/2502.02406', 'abstract': 'Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique enabling support for longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 5.58$\\times$ end-to-end speedup compared to existing approaches.', 'abstract_zh': '跨注意力机制在多模态大型语言模型（MLLMs）中广泛用于将视觉信息整合到语言骨干中。然而，在处理大量视觉输入的应用场景，如视频理解中，跨注意力层中处理大量视觉标记会导致高内存需求，并且通常需要在多块GPU上进行分布式计算。现有的分布式注意力机制面临着显著的通信开销，使得跨注意力层成为MLLMs高效训练和推理的瓶颈。为了解决这一问题，我们提出了一种名为LV-XAttn的分布式、精确跨注意力机制，其具有极小的通信开销。我们观察到，在涉及大量视觉输入的应用中，查询块的大小通常远小于键值块的大小。因此，在LV-XAttn中，我们留在每个GPU上本地存储较大的键值块，并通过GPU之间交换较小的查询块来实现通信。我们还引入了一种高效的激活重计算技术，以支持更长的视觉上下文。我们从理论上分析了LV-XAttn的通信效益，并展示了它在广泛模型范围内的加速效果。通过使用mPLUG-Owl3和OpenFlamingo模型进行评估，我们发现LV-XAttn相比现有方法可以获得高达5.58倍的端到端加速效果。', 'title_zh': 'LV-XAttn: 分布式跨注意力机制处理多模态大型语言模型中的长视觉输入'}
{'arxiv_id': 'arXiv:2502.02088', 'title': 'IPO: Iterative Preference Optimization for Text-to-Video Generation', 'authors': 'Xiaomeng Yang, Zhiyu Tan, Xuecheng Nie, Hao Li', 'link': 'https://arxiv.org/abs/2502.02088', 'abstract': 'Video foundation models have achieved significant advancement with the help of network upgrade as well as model scale-up. However, they are still hard to meet requirements of applications due to unsatisfied generation quality. To solve this problem, we propose to align video foundation models with human preferences from the perspective of post-training in this paper. Consequently, we introduce an Iterative Preference Optimization strategy to enhance generated video quality by incorporating human feedback. Specifically, IPO exploits a critic model to justify video generations for pairwise ranking as in Direct Preference Optimization or point-wise scoring as in Kahneman-Tversky Optimization. Given this, IPO optimizes video foundation models with guidance of signals from preference feedback, which helps improve generated video quality in subject consistency, motion smoothness and aesthetic quality, etc. In addition, IPO incorporates the critic model with the multi-modality large language model, which enables it to automatically assign preference labels without need of retraining or relabeling. In this way, IPO can efficiently perform multi-round preference optimization in an iterative manner, without the need of tediously manual labeling. Comprehensive experiments demonstrate that the proposed IPO can effectively improve the video generation quality of a pretrained model and help a model with only 2B parameters surpass the one with 5B parameters. Besides, IPO achieves new state-of-the-art performance on VBench benchmark. We will release our source codes, models as well as dataset to advance future research and applications.', 'abstract_zh': '视频基础模型在网络升级和模型规模扩大帮助下取得了显著的进步，但它们仍然难以满足应用要求，因为生成质量未达到满意标准。为了解决这个问题，本文从后训练的角度出发，提出了将视频基础模型与人类偏好相匹配的策略。为此，我们引入了一种迭代偏好优化（IPO）策略，通过整合人类反馈来提高生成视频的质量。具体而言，IPO 使用一个批判模型进行评价，评估视频生成的成对排名或点评分，类似于直接偏好优化（Direct Preference Optimization）或开曼-特韦斯基优化（Kahneman-Tversky Optimization）。以此为基础，IPO 利用偏好反馈信号指导视频基础模型的优化，从而提高生成视频在主题一致性、运动流畅性和审美质量等方面的质量。此外，IPO 将批判模型与多模态大型语言模型相结合，使其能够自动分配偏好标签，无需重新训练或重新打标签。这样，IPO 可以高效地以迭代方式执行多轮偏好优化，无需繁琐的手动标注工作。全面的实验表明，提出的 IPO 能够有效提高预训练模型的视频生成质量，并帮助一个只有 2B 参数的模型超越拥有 5B 参数的模型。此外，IPO 在 VBench 基准测试中实现了新的最佳性能。我们将发布我们的源代码、模型和数据集，以促进未来的研究和应用。', 'title_zh': 'IPO：迭代偏好优化的文本到视频生成方法'}
{'arxiv_id': 'arXiv:2502.01969', 'title': 'Mitigating Object Hallucinations in Large Vision-Language Models via Attention Calibration', 'authors': 'Younan Zhu, Linwei Tao, Minjing Dong, Chang Xu', 'link': 'https://arxiv.org/abs/2502.01969', 'abstract': 'Large Vision-Language Models (LVLMs) exhibit impressive multimodal reasoning capabilities but remain highly susceptible to object hallucination, where models generate responses that are not factually aligned with the visual content. Recent works attribute this issue to an inherent bias of LVLMs where vision token attention map has a fixed correlation with spatial position, and propose to mitigate this issue by reordering visual tokens. However, we find that different LVLMs exhibit different correlations between attention and spatial position, which makes the existing solution difficult to generalize to other LVLMs. To address this issue, we first introduce a training-free solution, Uniform Attention Calibration (UAC), that estimates the bias from single meaningless input image and applies a calibration matrix to rectify attention imbalances. To further alleviate the bias, we relax the assumption of single meaningless input in UAC and introduce a fine-tuning solution, Dynamic Attention Calibration (DAC), that enforces the consistent outputs wherever the object locates in the image via a plug-and-plays module. Comprehensive experiments across multiple benchmarks demonstrate that UAC and DAC significantly reduce object hallucination while improving general multimodal alignment. Our methods achieve state-of-the-art performance across diverse LVLM architectures on various metrics.', 'abstract_zh': '大型多模态语言-视觉模型（Large Vision-Language Models, LVLMs）表现出令人印象深刻的多模态推理能力，但在对象幻觉方面仍非常脆弱，即模型生成的内容与视觉内容不符合事实。最近的研究将这一问题归因于LVLM固有的偏见，即视觉令牌注意力图与空间位置之间存在固定的相关性，并提出通过重新排序视觉令牌来解决这一问题。然而，我们发现不同LVLM之间的注意力与空间位置之间的相关性不同，这使得现有的解决方案难以泛化到其他LVLM中。为解决这一问题，我们首先提出了一种无需训练的解决方案——均匀注意力校准（Uniform Attention Calibration, UAC），该方法通过单个无意义输入图像估计偏差，并应用校准矩阵来纠正注意力不平衡。为进一步降低偏差，我们放松UAC中关于单个无意义输入的假设，并引入了一种微调解决方案——动态注意力校准（Dynamic Attention Calibration, DAC），该方法通过插件模块确保图像中对象无论位于何处都产生一致的输出。跨多个基准的全面实验表明，UAC和DAC显著减少了对象幻觉并提高了整体多模态对齐效果。我们的方法在多种LVLM架构上实现了多种指标下的最新性能。', 'title_zh': '通过注意力校准缓解大型视觉-语言模型中的对象幻视问题'}
{'arxiv_id': 'arXiv:2502.01785', 'title': 'AquaticCLIP: A Vision-Language Foundation Model for Underwater Scene Analysis', 'authors': 'Basit Alawode, Iyyakutti Iyappan Ganapathi, Sajid Javed, Naoufel Werghi, Mohammed Bennamoun, Arif Mahmood', 'link': 'https://arxiv.org/abs/2502.01785', 'abstract': 'The preservation of aquatic biodiversity is critical in mitigating the effects of climate change. Aquatic scene understanding plays a pivotal role in aiding marine scientists in their decision-making processes. In this paper, we introduce AquaticCLIP, a novel contrastive language-image pre-training model tailored for aquatic scene understanding. AquaticCLIP presents a new unsupervised learning framework that aligns images and texts in aquatic environments, enabling tasks such as segmentation, classification, detection, and object counting. By leveraging our large-scale underwater image-text paired dataset without the need for ground-truth annotations, our model enriches existing vision-language models in the aquatic domain. For this purpose, we construct a 2 million underwater image-text paired dataset using heterogeneous resources, including YouTube, Netflix, NatGeo, etc. To fine-tune AquaticCLIP, we propose a prompt-guided vision encoder that progressively aggregates patch features via learnable prompts, while a vision-guided mechanism enhances the language encoder by incorporating visual context. The model is optimized through a contrastive pretraining loss to align visual and textual modalities. AquaticCLIP achieves notable performance improvements in zero-shot settings across multiple underwater computer vision tasks, outperforming existing methods in both robustness and interpretability. Our model sets a new benchmark for vision-language applications in underwater environments. The code and dataset for AquaticCLIP are publicly available on GitHub at xxx.', 'abstract_zh': '水生生物多样性的保护在缓解气候变化方面至关重要。水下场景理解在帮助海洋科学家进行决策过程中扮演着关键角色。本文介绍了一种名为AquaticCLIP的新型对比语言-图像预训练模型，专门用于水下场景理解。AquaticCLIP提供了一种新的无监督学习框架，可以将水下环境中的图像与文本对齐，从而实现分割、分类、检测和物体计数等任务。通过利用大规模的无注释水下图像-文本配对数据集，我们的模型丰富了现有水下领域的视觉-语言模型。为了构建这一数据集，我们充分利用了YouTube、Netflix、国家地理（NatGeo）等多源资源，构造了一个包含200万对水下图像和文本的数据集。为了微调AquaticCLIP，我们提出了一种提示引导的视觉编码器，该编码器通过可学习的提示逐步聚合局部特征，同时视觉引导机制通过引入视觉上下文增强语言编码器。模型通过对比预训练损失进行优化，以对齐视觉和文本模态。在多个水下计算机视觉任务的零样本设置中，AquaticCLIP取得了显著的性能改进，并在鲁棒性和可解释性方面优于现有方法。我们的模型为水下环境中视觉-语言应用建立了新的基准。AquaticCLIP的代码和数据集已在GitHub（xxx）上公开可供下载。', 'title_zh': 'AquaticCLIP：一种用于水下场景分析的多模态基础模型'}
{'arxiv_id': 'arXiv:2502.01158', 'title': 'MIND: Modality-Informed Knowledge Distillation Framework for Multimodal Clinical Prediction Tasks', 'authors': 'Alejandro Guerra-Manzanares, Farah E. Shamout', 'link': 'https://arxiv.org/abs/2502.01158', 'abstract': 'Multimodal fusion leverages information across modalities to learn better feature representations with the goal of improving performance in fusion-based tasks. However, multimodal datasets, especially in medical settings, are typically smaller than their unimodal counterparts, which can impede the performance of multimodal models. Additionally, the increase in the number of modalities is often associated with an overall increase in the size of the multimodal network, which may be undesirable in medical use cases. Utilizing smaller unimodal encoders may lead to sub-optimal performance, particularly when dealing with high-dimensional clinical data. In this paper, we propose the Modality-INformed knowledge Distillation (MIND) framework, a multimodal model compression approach based on knowledge distillation that transfers knowledge from ensembles of pre-trained deep neural networks of varying sizes into a smaller multimodal student. The teacher models consist of unimodal networks, allowing the student to learn from diverse representations. MIND employs multi-head joint fusion models, as opposed to single-head models, enabling the use of unimodal encoders in the case of unimodal samples without requiring imputation or masking of absent modalities. As a result, MIND generates an optimized multimodal model, enhancing both multimodal and unimodal representations. It can also be leveraged to balance multimodal learning during training. We evaluate MIND on binary and multilabel clinical prediction tasks using time series data and chest X-ray images. Additionally, we assess the generalizability of the MIND framework on three non-medical multimodal multiclass datasets. Experimental results demonstrate that MIND enhances the performance of the smaller multimodal network across all five tasks, as well as various fusion methods and multimodal architectures, compared to state-of-the-art baselines.', 'abstract_zh': '多模态融合通过跨模态信息的学习，旨在提高融合任务中的性能。然而，多模态数据集，特别是在医疗环境中，通常比单模态数据集小，这可能会妨碍多模态模型的性能。此外，模态数量的增加通常会导致多模态网络整体增大，这在医疗应用场景中可能是不理想的。使用较小的单模态编码器可能在处理高维临床数据时导致次优性能。本文提出了一种基于知识蒸馏的多模态模型压缩方法——Modality-INformed知识蒸馏（MIND）框架，该方法将不同大小的预训练深度神经网络集成的知识转移到一个较小的多模态学生网络中。教师模型由单模态网络组成，使得学生可以从多样化的表示中学习。MIND采用了多头联合融合模型，而非单头模型，使得在单模态样本情况下可以使用单模态编码器，而无需对缺失的模态进行插补或掩码处理。因此，MIND生成了优化后的多模态模型，增强了多模态和单模态的表示，同时也可以在训练过程中平衡多模态学习。我们通过时间序列数据和胸部X光图像，评估了MIND在二分类和多标签临床预测任务中的性能，并评估了MIND框架在三个非医疗多模态多类别数据集上的通用性。实验结果表明，相较于最先进的基线方法，MIND在所有五个任务和各种融合方法及多模态架构中均提高了较小多模态网络的性能。', 'title_zh': 'MIND：模态导向的知识蒸馏框架，用于多模态临床预测任务'}
{'arxiv_id': 'arXiv:2410.14170', 'title': 'Personalized Image Generation with Large Multimodal Models', 'authors': 'Yiyan Xu, Wenjie Wang, Yang Zhang, Biao Tang, Peng Yan, Fuli Feng, Xiangnan He', 'link': 'https://arxiv.org/abs/2410.14170', 'abstract': "Personalized content filtering, such as recommender systems, has become a critical infrastructure to alleviate information overload. However, these systems merely filter existing content and are constrained by its limited diversity, making it difficult to meet users' varied content needs. To address this limitation, personalized content generation has emerged as a promising direction with broad applications. Nevertheless, most existing research focuses on personalized text generation, with relatively little attention given to personalized image generation. The limited work in personalized image generation faces challenges in accurately capturing users' visual preferences and needs from noisy user-interacted images and complex multimodal instructions. Worse still, there is a lack of supervised data for training personalized image generation models.\nTo overcome the challenges, we propose a Personalized Image Generation Framework named Pigeon, which adopts exceptional large multimodal models with three dedicated modules to capture users' visual preferences and needs from noisy user history and multimodal instructions. To alleviate the data scarcity, we introduce a two-stage preference alignment scheme, comprising masked preference reconstruction and pairwise preference alignment, to align Pigeon with the personalized image generation task. We apply Pigeon to personalized sticker and movie poster generation, where extensive quantitative results and human evaluation highlight its superiority over various generative baselines.", 'abstract_zh': '个性化内容过滤，如推荐系统，已成为缓解信息过载的关键基础设施。然而，这些系统仅仅过滤现有的内容，并受到内容多样性有限的限制，难以满足用户多样化的信息需求。为解决这一局限性，个性化内容生成已经作为一种有广阔应用前景的发展方向出现。尽管如此，现有的大多数研究集中在个性化文本生成上，对于个性化图像生成的关注相对较少。在个性化图像生成方面有限的研究工作面临从含噪声的用户交互图像和复杂的多模态指令中准确捕捉用户视觉偏好的挑战。更糟糕的是，缺乏用于训练个性化图像生成模型的监督数据。\n\n为克服这些挑战，我们提出了一种名为Pigeon的个性化图像生成框架，该框架采用特殊的大型多模态模型，并配备了三个专用模块，从含噪声的用户历史和多模态指令中捕捉用户的视觉偏好和需求。为缓解数据稀缺性，我们引入了一种两阶段偏好对齐方案，包括掩码偏好重构和成对偏好对齐，以使Pigeon与个性化图像生成任务相匹配。我们在个性化贴纸和电影海报生成中应用了Pigeon，广泛的定量结果和人工评估表明，Pigeon在各种生成基准中表现出优越性。', 'title_zh': '基于大型多模态模型的个性化图像生成'}
{'arxiv_id': 'arXiv:2502.02458', 'title': 'SAISA: Towards Multimodal Large Language Models with Both Training and Inference Efficiency', 'authors': 'Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun', 'link': 'https://arxiv.org/abs/2502.02458', 'abstract': 'Multimodal Large Language Models (MLLMs) mainly fall into two architectures, each involving a trade-off between training and inference efficiency: embedding space alignment (e.g., LLaVA-1.5) is inefficient during inference, while cross-attention space alignment (e.g., Flamingo) is inefficient in training. In this paper, we compare these two architectures and identify the key factors for building efficient MLLMs. A primary difference between them lies in how attention is applied to visual tokens, particularly in their interactions with each other. To investigate whether attention among visual tokens is necessary, we propose a new self-attention mechanism, NAAViT (\\textbf{N}o \\textbf{A}ttention \\textbf{A}mong \\textbf{Vi}sual \\textbf{T}okens), which eliminates this type of attention. Our pilot experiment on LLaVA-1.5 shows that attention among visual tokens is highly redundant. Based on these insights, we introduce SAISA (\\textbf{S}elf-\\textbf{A}ttention \\textbf{I}nput \\textbf{S}pace \\textbf{A}lignment), a novel architecture that enhance both training and inference efficiency. SAISA directly aligns visual features with the input spaces of NAAViT self-attention blocks, reducing computational overhead in both self-attention blocks and feed-forward networks (FFNs). Using the same configuration as LLaVA-1.5, SAISA reduces inference FLOPs by 66\\% and training budget by 26\\%, while achieving superior performance in terms of accuracy. Comprehensive ablation studies further validate the effectiveness of SAISA across various LLMs and visual encoders. The code and model will be publicly available at this https URL.', 'abstract_zh': '多模态大型语言模型（MLLMs）主要划分为两种架构，每种架构在训练和推理效率之间存在权衡：嵌入空间对齐（例如，LLaVA-1.5）在推理过程中效率较低，而跨注意力空间对齐（例如，Flamingo）在训练过程中效率较低。在本文中，我们比较了这两种架构，并确定了构建高效MLLMs的关键因素。它们之间的一个主要区别在于视觉标记之间如何应用注意力，特别是在它们之间的交互方式上。为了探讨视觉标记之间注意力是否必要，我们提出了一种新的自注意力机制NAAViT（No Attention Among Visual Tokens，视觉标记之间无注意力），该机制消除了这种类型的注意力。在LLaVA-1.5的小规模实验中，我们发现视觉标记之间的注意力是高度冗余的。基于这些洞见，我们引入了SAISA（Self-Attention Input Space Alignment，自注意力输入空间对齐）架构，该架构能够同时提高训练和推理效率。SAISA直接将视觉特征与NAAViT自注意力块的输入空间对齐，从而减少自注意力块和前馈网络（FFNs）中的计算开销。使用与LLaVA-1.5相同的配置，SAISA将推理FLOPs减少了66%，训练预算减少了26%，同时在准确性方面取得了更好的表现。综合消融实验进一步验证了SAISA在各种大型语言模型和视觉编码器上均具有有效性。代码和模型将在以下链接公开：https://github.com/alibaba/Qwen。', 'title_zh': 'SAISA：面向高效训练与推理的多模态大型语言模型'}
{'arxiv_id': 'arXiv:2502.02339', 'title': 'Boosting Multimodal Reasoning with MCTS-Automated Structured Thinking', 'authors': 'Jinyang Wu, Mingkuan Feng, Shuai Zhang, Ruihan Jin, Feihu Che, Zengqi Wen, Jianhua Tao', 'link': 'https://arxiv.org/abs/2502.02339', 'abstract': "Multimodal large language models (MLLMs) exhibit impressive capabilities but still face challenges in complex visual reasoning. While recent efforts attempt to enhance MLLMs' reasoning by incorporating OpenAI o1-like structured thinking through explicit search structures or teacher-guided distillation, they often struggle to balance performance and efficiency. A critical limitation is their heavy reliance on extensive data and search spaces, resulting in low-efficiency implicit insight extraction and data utilization. To address this, we propose AStar, an Automated Structured thinking paradigm for multimodal reasoning via Monte Carlo Tree Search (MCTS). AStar automatically derives high-level cognitive reasoning patterns from limited data using MCTS-powered hierarchical structures. Building on these explicit patterns, we design a unified reasoning framework that seamlessly integrates models' internal reasoning capabilities and external reasoning guidelines, enabling efficient inference with minimal tree iterations. This novel paradigm strikes a compelling balance between performance and efficiency. Extensive experiments demonstrate AStar's effectiveness, achieving superior accuracy (54.0$\\%$) on the MathVerse benchmark with a 7B backbone, surpassing GPT-4o (50.2$\\%$) while maintaining substantial data and computational efficiency.", 'abstract_zh': '多模态大型语言模型（MLLMs）展现了令人印象深刻的能力，但在复杂的视觉推理方面仍然面临挑战。尽管近期的努力试图通过引入类似于OpenAI o1的结构化思考方式，如显式搜索结构或教师引导的蒸馏来增强MLLMs的推理能力，但它们往往难以在性能和效率之间取得平衡。一个关键限制是，这些模型对大量数据和搜索空间的依赖性很强，导致了低效的隐含洞察提取和数据利用。为了解决这一问题，我们提出了AStar，这是一种通过蒙特卡洛树搜索（MCTS）实现自动结构化思考的多模态推理范式。AStar 利用MCTS驱动的分层结构，从有限的数据中自动推导出高级的认知推理模式。基于这些显式的模式，我们设计了一个统一的推理框架，该框架可以无缝地整合模型的内部推理能力与外部推理准则，从而通过最少的树迭代实现高效的推理。这一新的范式在性能和效率之间取得了令人 impressive 的平衡。广泛实验表明，AStar 的有效性得到了验证：它在使用7B参数骨干网络的情况下，在MathVerse基准上达到了54.0%的准确率，超越了GPT-4o（50.2%），同时保持了显著的数据和计算效率。', 'title_zh': '使用MCTS自动结构化思维增强多模态推理'}
{'arxiv_id': 'arXiv:2502.02048', 'title': 'Efficient Domain Adaptation of Multimodal Embeddings using Constrastive Learning', 'authors': 'Georgios Margaritis, Periklis Petridis, Dimitris J. Bertsimas', 'link': 'https://arxiv.org/abs/2502.02048', 'abstract': 'Recent advancements in machine learning (ML), natural language processing (NLP), and foundational models have shown promise for real-life applications in critical, albeit compute-constrainted fields like healthcare.\nIn such areas, combining foundational models with supervised ML offers potential for automating tasks like diagnosis and treatment planning, but the limited availability of onsite computational resources pose significant challenges before applying these technologies effectively: Current approaches either yield subpar results when using pretrained models without task-specific adaptation, or require substantial computational resources for fine-tuning, which is often a barrier to entry in such environments.\nThis renders them inaccessible in applications where performance and quality standards are high, but computational resources are scarce.\nTo bridge the gap between best-in-class performance and accessibility, we propose a novel method for adapting foundational, multimodal embeddings to downstream tasks, without the need of expensive fine-tuning processes.\nOur method leverages frozen embeddings from Large Language Models (LLMs) and Vision Models, and uses contrastive learning to train a small, task-specific nonlinear projection that can be used in the downstream task, without having to fine-tune the original foundational models.\nWe show that this efficient procedure leads to significant performance improvements across various downstream tasks, and perhaps more importantly with minimal computational overhead, offering a practical solution for the use of advanced, foundational ML models in resource-constrained settings.', 'abstract_zh': '近年来，机器学习（ML）、自然语言处理（NLP）以及基础模型的进展在诸如医疗保健等关键但计算资源受限的领域中展现了应用的潜力。在这些领域中，将基础模型与监督式ML相结合可以在自动化诊断和治疗规划等任务上提供可能性，但现场计算资源的有限供应在有效应用这些技术时提出了重大挑战：当前的方法要么在使用预训练模型而无需特定任务调整时结果不佳，要么需要大量的计算资源进行调优，而这通常是这些环境中进入的技术障碍。\n\n这使得在高性能和高质量标准要求高但计算资源稀缺的应用场景中，这些技术难以使用。为缩小顶级性能与易用性之间的差距，我们提出了一种新型方法，用于适应基础的多模态嵌入，而无需昂贵的调优过程。\n\n我们的方法利用大型语言模型（LLMs）和视觉模型中的冻结嵌入，并使用对比学习训练一个小的、特定任务的非线性投影，该投影可以直接应用于下游任务，而不需要重新调整个基础模型。我们展示了这种高效的方法在各种下游任务上带来了显著的性能提升，并且更为重要的是，这种提升伴随着最少的计算开销，为在计算资源受限的环境中使用先进的基础ML模型提供了实际解决方案。', 'title_zh': '使用对比学习进行多模态嵌入的高效领域自适应'}
{'arxiv_id': 'arXiv:2502.01699', 'title': 'Multimodal Inverse Attention Network with Intrinsic Discriminant Feature Exploitation for Fake News Detection', 'authors': 'Tianlin Zhang, En Yu, Yi Shao, Shuai Li, Sujuan Hou, Jiande Sun', 'link': 'https://arxiv.org/abs/2502.01699', 'abstract': 'Multimodal fake news detection has garnered significant attention due to its profound implications for social security. While existing approaches have contributed to understanding cross-modal consistency, they often fail to leverage modal-specific representations and explicit discrepant features. To address these limitations, we propose a Multimodal Inverse Attention Network (MIAN), a novel framework that explores intrinsic discriminative features based on news content to advance fake news detection. Specifically, MIAN introduces a hierarchical learning module that captures diverse intra-modal relationships through local-to-global and local-to-local interactions, thereby generating enhanced unimodal representations to improve the identification of fake news at the intra-modal level. Additionally, a cross-modal interaction module employs a co-attention mechanism to establish and model dependencies between the refined unimodal representations, facilitating seamless semantic integration across modalities. To explicitly extract inconsistency features, we propose an inverse attention mechanism that effectively highlights the conflicting patterns and semantic deviations introduced by fake news in both intra- and inter-modality. Extensive experiments on benchmark datasets demonstrate that MIAN significantly outperforms state-of-the-art methods, underscoring its pivotal contribution to advancing social security through enhanced multimodal fake news detection.', 'abstract_zh': '多模态假新闻检测因其对社会安全的深远影响而引起了广泛关注。尽管现有方法在理解跨模态一致性方面做出了贡献，但它们往往未能充分利用特定模态的表示和显式的不一致特征。为了解决这些问题，我们提出了一种多模态逆注意力网络（MIAN），这是一种新颖的框架，基于新闻内容探索内在的辨别特征，以促进假新闻检测。具体而言，MIAN 引入了一种分层学习模块，通过局部到全局和局部到局部的交互来捕捉多种内模态关系，从而生成增强的一模态表示，以在内模态级别提高假新闻的识别能力。此外，跨模态交互模块采用共注意力机制来建立并建模细化的一模态表示之间的依赖关系，促进不同模态之间的无缝语义集成。为明确提取不一致特征，我们提出了一种逆注意力机制，该机制有效地突出了假新闻引入的内在和跨模态中的矛盾模式和语义偏差。在基准数据集上的广泛实验表明，MIAN 显著优于现有最先进的方法，突显了其在通过增强多模态假新闻检测来提升社会安全方面的重要贡献。', 'title_zh': '具有内在鉴别特征利用的多模态逆注意力网络用于假新闻检测'}
{'arxiv_id': 'arXiv:2502.01524', 'title': 'Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective', 'authors': 'Xiaorui Ma, Haoran Xie, S. Joe Qin', 'link': 'https://arxiv.org/abs/2502.01524', 'abstract': 'The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs.', 'abstract_zh': '视觉语言模态的融合一直是多模态学习中的重要研究方向，传统上依赖于视觉语言预训练模型。然而，随着大型语言模型（LLMs）的崛起，越来越多的研究开始融合LLMs与视觉模态。随之而来的是，将视觉模态集成到LLMs中的训练范式也发生了变化。最初，这种方法是通过预训练模态整合器，称为单阶段调优来实现。后来，这种方法逐渐分化为两类：一类是专注于性能提升的双阶段调优方法，另一类是侧重参数效率的直接适配方法。现有的综述主要集中在运用双阶段调优方法的视觉大型语言模型（VLLMs）上，导致了对训练范式及其独特的参数效率考量缺乏深刻的理解。本文将34个来自顶级会议、期刊和高引Arxiv论文的VLLMs进行分类和综述，重点关注从训练范式角度进行的参数效率。首先介绍了大型语言模型的结构与参数高效学习方法，随后讨论了视觉编码器，并给出了视觉模态整合器的全面分类。接着，本文回顾了三大训练范式及其效率考量，并总结了VLLM领域的基准测试结果。为了更深入地了解其在参数效率方面的有效性，我们比较和讨论了代表性模型的实验结果，其中直接适配范式的实验结果被重复验证。借此，本文旨在为研究人员和实践者提供有关视觉模态高效集成到LLMs中的最新进展和实用指南。', 'title_zh': '从训练范式视角高效整合大规模语言模型与视觉感知：一项综述'}
